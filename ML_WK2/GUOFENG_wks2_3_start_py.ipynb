{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "GUOFENG_wks2_3_start.py.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "o-1eXKEK3vEj",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import optimizers"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m           \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudart_dll_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a50351283a23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCSVLogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Perform pre-load sanity checks in order to produce a more actionable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# than we get from an error during SWIG import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mself_check\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\u001b[0m in \u001b[0;36mpreload_check\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m               \u001b[1;34m\"environment variable. Download and install CUDA %s from \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m               \u001b[1;34m\"this URL: https://developer.nvidia.com/cuda-90-download-archive\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m               % (build_info.cudart_dll_name, build_info.cuda_version_number))\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m       if hasattr(build_info, \"cudnn_dll_name\") and hasattr(\n",
      "\u001b[1;31mImportError\u001b[0m: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive"
     ],
     "ename": "ImportError",
     "evalue": "Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FeES-y7_38YR",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def implt(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Set up 'ggplot' style\n",
    "plt.style.use('ggplot')  # if want to use the default style, set 'classic'\n",
    "plt.rcParams['ytick.right'] = True\n",
    "plt.rcParams['ytick.labelright'] = True\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.labelleft'] = False\n",
    "plt.rcParams['font.family'] = 'Arial'\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eZ0qQoCnsYhX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data = cifar10.load_data() #type tuple"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mOnXi-IXGUhm",
    "colab_type": "code",
    "outputId": "1fa76cbe-5c62-4c82-ad6f-d61395620f60",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    }
   },
   "source": [
    "(train_data, train_label) = data[0]\n",
    "(test_data, test_label) = data[1]\n",
    "\n",
    "# Convert the data into 'float32'\n",
    "# Rescale the values from 0~255 to 0~1\n",
    "train_data = train_data.astype('float32')/255 #Training data\n",
    "test_data = test_data.astype('float32')/255 #Test data\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# Retrieve the row size of each image\n",
    "# Retrieve the column size of each image\n",
    "imgrows = train_data.shape[1] \n",
    "imgclms = train_data.shape[2] \n",
    "channel = train_data.shape[3]\n",
    "\n",
    "# For deep learning training and testing, the data must be in the \n",
    "# form of (no_sample, row, clm, channel)\n",
    "\n",
    "# Perform one hot encoding on the labels\n",
    "# Retrieve the number of classes in this problem\n",
    "train_label = to_categorical(train_label) \n",
    "test_label = to_categorical(test_label) \n",
    "num_classes = test_label.shape[1]\n",
    "print(num_classes)\n",
    "\n",
    "# ............................................................................."
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "10\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IYTEGf6vBB9B",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "outputId": "9e301554-1694-422f-b44d-b174e447fa56"
   },
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "\n",
    "optmz = optimizers.RMSprop(lr=0.0001)\n",
    "modelname = 'wks2_3'\n",
    "\n",
    "\n",
    "# define the deep learning model\n",
    "def createModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32,(3,3), \n",
    "    input_shape=(imgrows,imgclms,channel), \n",
    "    padding='same', \n",
    "    activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "    model.add(Conv2D(48,(3,3),padding='same',activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "    model.add(Conv2D(64,(3,3),padding='same',activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(256,activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes,activation='softmax')) \n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "    optimizer=optmz, \n",
    "    metrics=['accuracy'])\n",
    "  \n",
    "    return model\n",
    "\n",
    "    # Setup the models\n",
    "\n",
    "\n",
    "model = createModel()  # This is meant for training\n",
    "modelGo = createModel()  # This is used for final testing\n",
    "\n",
    "model.summary()\n"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 48)        13872     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 48)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 64)          27712     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 307,450\n",
      "Trainable params: 307,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6_QUCFrvCV4s",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Create checkpoint for the training\n",
    "# This checkpoint performs model saving when\n",
    "# an epoch gives highest testing accuracy\n",
    "filepath = modelname + \".hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "# Log the epoch detail into csv\n",
    "csv_logger = CSVLogger(modelname + '.csv')\n",
    "callbacks_list = [checkpoint, csv_logger]\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ut7nvS9wCeO1",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "716f4f73-f97c-4c8d-e7b2-026c56568349"
   },
   "source": [
    "\n",
    "# Fit the model\n",
    "# This is where the training starts\n",
    "model.fit(train_data,\n",
    "          train_label,\n",
    "          validation_data=(test_data, test_label),\n",
    "          epochs=100,\n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          callbacks=callbacks_list)\n"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 9s 172us/sample - loss: 1.9499 - acc: 0.2983 - val_loss: 1.8067 - val_acc: 0.3950\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.6691 - acc: 0.4035 - val_loss: 1.6384 - val_acc: 0.4568\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.5529 - acc: 0.4411 - val_loss: 1.5525 - val_acc: 0.4757\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.4794 - acc: 0.4691 - val_loss: 1.4809 - val_acc: 0.5091\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.4240 - acc: 0.4915 - val_loss: 1.4299 - val_acc: 0.5285\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.3803 - acc: 0.5082 - val_loss: 1.4455 - val_acc: 0.5179\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3461 - acc: 0.5191 - val_loss: 1.3653 - val_acc: 0.5530\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.3118 - acc: 0.5340 - val_loss: 1.3272 - val_acc: 0.5652\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.2793 - acc: 0.5458 - val_loss: 1.3486 - val_acc: 0.5486\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 8s 164us/sample - loss: 1.2486 - acc: 0.5583 - val_loss: 1.2775 - val_acc: 0.5745\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.2236 - acc: 0.5681 - val_loss: 1.2619 - val_acc: 0.5854\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 8s 161us/sample - loss: 1.1994 - acc: 0.5780 - val_loss: 1.2324 - val_acc: 0.5923\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.1748 - acc: 0.5878 - val_loss: 1.2345 - val_acc: 0.5916\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.1527 - acc: 0.5946 - val_loss: 1.2219 - val_acc: 0.5944\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.1326 - acc: 0.6015 - val_loss: 1.1704 - val_acc: 0.6155\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.1110 - acc: 0.6104 - val_loss: 1.1552 - val_acc: 0.6143\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.0949 - acc: 0.6146 - val_loss: 1.1715 - val_acc: 0.6143\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 8s 160us/sample - loss: 1.0769 - acc: 0.6246 - val_loss: 1.1118 - val_acc: 0.6349\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.0578 - acc: 0.6317 - val_loss: 1.1070 - val_acc: 0.6378\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.0398 - acc: 0.6372 - val_loss: 1.0914 - val_acc: 0.6403\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.0239 - acc: 0.6418 - val_loss: 1.1080 - val_acc: 0.6347\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 1.0112 - acc: 0.6437 - val_loss: 1.0763 - val_acc: 0.6437\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 0.9933 - acc: 0.6548 - val_loss: 1.0559 - val_acc: 0.6530\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 0.9813 - acc: 0.6574 - val_loss: 1.0844 - val_acc: 0.6410\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 0.9677 - acc: 0.6620 - val_loss: 1.0489 - val_acc: 0.6562\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 0.9525 - acc: 0.6657 - val_loss: 1.0227 - val_acc: 0.6541\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 0.9408 - acc: 0.6715 - val_loss: 1.0327 - val_acc: 0.6636\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 0.9286 - acc: 0.6767 - val_loss: 0.9846 - val_acc: 0.6768\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 0.9148 - acc: 0.6794 - val_loss: 0.9865 - val_acc: 0.6741\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 0.9051 - acc: 0.6842 - val_loss: 0.9750 - val_acc: 0.6833\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.8951 - acc: 0.6866 - val_loss: 0.9537 - val_acc: 0.6803\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.8821 - acc: 0.6923 - val_loss: 0.9970 - val_acc: 0.6616\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 0.8715 - acc: 0.6951 - val_loss: 0.9567 - val_acc: 0.6807\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 0.8623 - acc: 0.6998 - val_loss: 0.9360 - val_acc: 0.6918\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.8518 - acc: 0.7027 - val_loss: 0.9290 - val_acc: 0.6934\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.8418 - acc: 0.7059 - val_loss: 0.9158 - val_acc: 0.6952\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.8330 - acc: 0.7078 - val_loss: 0.9022 - val_acc: 0.7010\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.8230 - acc: 0.7128 - val_loss: 0.9047 - val_acc: 0.7051\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 0.8123 - acc: 0.7149 - val_loss: 0.8920 - val_acc: 0.7070\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.8039 - acc: 0.7199 - val_loss: 0.8789 - val_acc: 0.7080\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.7933 - acc: 0.7216 - val_loss: 0.9437 - val_acc: 0.6823\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.7881 - acc: 0.7231 - val_loss: 0.8831 - val_acc: 0.7045\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.7811 - acc: 0.7290 - val_loss: 0.8904 - val_acc: 0.6939\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.7738 - acc: 0.7309 - val_loss: 0.8865 - val_acc: 0.7089\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.7636 - acc: 0.7346 - val_loss: 0.8614 - val_acc: 0.7155\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.7538 - acc: 0.7363 - val_loss: 0.8460 - val_acc: 0.7172\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.7468 - acc: 0.7413 - val_loss: 0.8626 - val_acc: 0.7127\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.7375 - acc: 0.7438 - val_loss: 0.8729 - val_acc: 0.7080\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 8s 159us/sample - loss: 0.7311 - acc: 0.7462 - val_loss: 0.8323 - val_acc: 0.7258\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 0.7238 - acc: 0.7487 - val_loss: 0.8297 - val_acc: 0.7227\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 0.7171 - acc: 0.7512 - val_loss: 0.8290 - val_acc: 0.7246\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.7096 - acc: 0.7531 - val_loss: 0.8181 - val_acc: 0.7211\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.7023 - acc: 0.7533 - val_loss: 0.8450 - val_acc: 0.7130\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.6976 - acc: 0.7577 - val_loss: 0.8444 - val_acc: 0.7131\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.6879 - acc: 0.7603 - val_loss: 0.8053 - val_acc: 0.7292\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.6826 - acc: 0.7634 - val_loss: 0.8098 - val_acc: 0.7252\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.6755 - acc: 0.7635 - val_loss: 0.8065 - val_acc: 0.7294\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.6683 - acc: 0.7683 - val_loss: 0.8014 - val_acc: 0.7267\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.6620 - acc: 0.7707 - val_loss: 0.8038 - val_acc: 0.7265\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.6571 - acc: 0.7722 - val_loss: 0.7808 - val_acc: 0.7322\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.6470 - acc: 0.7750 - val_loss: 0.7967 - val_acc: 0.7287\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.6401 - acc: 0.7778 - val_loss: 0.8104 - val_acc: 0.7238\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.6364 - acc: 0.7799 - val_loss: 0.7756 - val_acc: 0.7389\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.6316 - acc: 0.7808 - val_loss: 0.7651 - val_acc: 0.7376\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.6273 - acc: 0.7816 - val_loss: 0.7662 - val_acc: 0.7395\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.6221 - acc: 0.7822 - val_loss: 0.7759 - val_acc: 0.7378\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.6140 - acc: 0.7878 - val_loss: 0.7590 - val_acc: 0.7392\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.6082 - acc: 0.7888 - val_loss: 0.7639 - val_acc: 0.7384\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.6067 - acc: 0.7900 - val_loss: 0.7509 - val_acc: 0.7405\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 8s 156us/sample - loss: 0.5987 - acc: 0.7921 - val_loss: 0.7471 - val_acc: 0.7448\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.5912 - acc: 0.7937 - val_loss: 0.7441 - val_acc: 0.7465\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.5908 - acc: 0.7952 - val_loss: 0.7505 - val_acc: 0.7437\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 0.5815 - acc: 0.7981 - val_loss: 0.7823 - val_acc: 0.7308\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 0.5785 - acc: 0.7996 - val_loss: 0.7503 - val_acc: 0.7439\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.5723 - acc: 0.8005 - val_loss: 0.7667 - val_acc: 0.7377\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 0.5692 - acc: 0.8026 - val_loss: 0.7484 - val_acc: 0.7413\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 0.5613 - acc: 0.8039 - val_loss: 0.7567 - val_acc: 0.7433\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.5544 - acc: 0.8091 - val_loss: 0.7286 - val_acc: 0.7492\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 0.5516 - acc: 0.8091 - val_loss: 0.7280 - val_acc: 0.7518\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 8s 155us/sample - loss: 0.5475 - acc: 0.8091 - val_loss: 0.7312 - val_acc: 0.7509\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.5439 - acc: 0.8112 - val_loss: 0.7437 - val_acc: 0.7469\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 0.5399 - acc: 0.8142 - val_loss: 0.7462 - val_acc: 0.7488\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 8s 152us/sample - loss: 0.5340 - acc: 0.8169 - val_loss: 0.7274 - val_acc: 0.7510\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.5328 - acc: 0.8128 - val_loss: 0.7213 - val_acc: 0.7550\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 0.5256 - acc: 0.8169 - val_loss: 0.7353 - val_acc: 0.7499\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.5175 - acc: 0.8206 - val_loss: 0.7171 - val_acc: 0.7550\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.5141 - acc: 0.8209 - val_loss: 0.7197 - val_acc: 0.7550\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 8s 152us/sample - loss: 0.5109 - acc: 0.8232 - val_loss: 0.7100 - val_acc: 0.7613\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 0.5061 - acc: 0.8218 - val_loss: 0.7056 - val_acc: 0.7598\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.5018 - acc: 0.8253 - val_loss: 0.7293 - val_acc: 0.7434\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.4954 - acc: 0.8260 - val_loss: 0.7042 - val_acc: 0.7583\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 8s 151us/sample - loss: 0.4933 - acc: 0.8299 - val_loss: 0.7078 - val_acc: 0.7555\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 8s 151us/sample - loss: 0.4913 - acc: 0.8296 - val_loss: 0.6978 - val_acc: 0.7637\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 8s 151us/sample - loss: 0.4875 - acc: 0.8294 - val_loss: 0.6906 - val_acc: 0.7630\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 8s 151us/sample - loss: 0.4805 - acc: 0.8326 - val_loss: 0.7077 - val_acc: 0.7571\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 8s 151us/sample - loss: 0.4802 - acc: 0.8312 - val_loss: 0.7128 - val_acc: 0.7536\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 8s 152us/sample - loss: 0.4713 - acc: 0.8356 - val_loss: 0.6928 - val_acc: 0.7643\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 8s 151us/sample - loss: 0.4705 - acc: 0.8360 - val_loss: 0.7011 - val_acc: 0.7630\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 8s 153us/sample - loss: 0.4650 - acc: 0.8398 - val_loss: 0.6865 - val_acc: 0.7671\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 8s 152us/sample - loss: 0.4614 - acc: 0.8387 - val_loss: 0.6965 - val_acc: 0.7631\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc17cd31518>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v7lp4nhnCisk",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "outputId": "0ba18752-e59a-4cfc-80d6-bd4e0e2da2aa"
   },
   "source": [
    "# Now the training is complete, we get\n",
    "# another object to load the weights\n",
    "# compile it, so that we can do\n",
    "# final evaluation on it\n",
    "modelGo.load_weights(filepath)\n",
    "modelGo.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# .......................................................................\n",
    "\n",
    "\n",
    "# Make classification on the test dataset\n",
    "predicts = modelGo.predict(test_data)\n",
    "predout = np.argmax(predicts,axis=1) \n",
    "testout = np.argmax(test_label, axis=1)\n",
    "                    \n",
    "labelname = ['airplane',\n",
    "             'automobile',\n",
    "             'bird',\n",
    "             'cat',\n",
    "             'deer',\n",
    "             'dog',\n",
    "             'frog',\n",
    "             'horse',\n",
    "             'ship',\n",
    "             'truck']\n",
    "\n",
    "# Prepare the classification output\n",
    "# for the classification report\n",
    "testScores = metrics.accuracy_score(testout,predout) \n",
    "confusion = metrics.confusion_matrix(testout,predout)\n",
    "\n",
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100)) \n",
    "print(metrics.classification_report(testout,predout,target_names=labelname,digits=4)) \n",
    "print(confusion)\n",
    "# ..................................................................\n",
    "\n"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Best accuracy (on testing dataset): 76.71%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane     0.8038    0.8070    0.8054      1000\n",
      "  automobile     0.8937    0.8320    0.8617      1000\n",
      "        bird     0.6762    0.6830    0.6796      1000\n",
      "         cat     0.6083    0.5870    0.5975      1000\n",
      "        deer     0.6879    0.7670    0.7253      1000\n",
      "         dog     0.6727    0.6680    0.6703      1000\n",
      "        frog     0.8822    0.7790    0.8274      1000\n",
      "       horse     0.7988    0.8260    0.8122      1000\n",
      "        ship     0.8933    0.8460    0.8690      1000\n",
      "       truck     0.7835    0.8760    0.8272      1000\n",
      "\n",
      "    accuracy                         0.7671     10000\n",
      "   macro avg     0.7701    0.7671    0.7676     10000\n",
      "weighted avg     0.7701    0.7671    0.7676     10000\n",
      "\n",
      "[[807  12  44  18  19   5   5   7  40  43]\n",
      " [ 12 832   6   9   4   3   4   2  19 109]\n",
      " [ 59   2 683  46  82  58  29  23   6  12]\n",
      " [ 15   7  65 587  74 150  32  45   9  16]\n",
      " [  9   3  70  42 767  30  18  51   5   5]\n",
      " [ 14   2  36 156  49 668   9  54   2  10]\n",
      " [  3   5  55  60  55  24 779   8   6   5]\n",
      " [ 10   0  26  22  55  43   2 826   1  15]\n",
      " [ 56  22  16  15   7   5   3   3 846  27]\n",
      " [ 19  46   9  10   3   7   2  15  13 876]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7EOUp0hHZc3Q",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "outputId": "4ccff983-f53b-4d3d-9f8f-cfbd4c0d06b9"
   },
   "source": [
    "# Plot the training output\n",
    "\n",
    "import pandas as pd \n",
    "records = pd.read_csv(modelname +'.csv') \n",
    "plt.figure() \n",
    "plt.subplot(211) \n",
    "plt.plot(records['val_loss']) \n",
    "plt.yticks([0.00,0.60,0.70,0.80]) \n",
    "plt.title('Loss value',fontsize=12) \n",
    "ax = plt.gca() \n",
    "ax.set_xticklabels([]) \n",
    "plt.subplot(212) \n",
    "plt.plot(records['val_acc']) \n",
    "plt.yticks([0.5,0.6,0.7,0.8]) \n",
    "plt.title('Accuracy',fontsize=12) \n",
    "plt.show()"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  }
 ]
}