{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Found 2423 images belonging to 3 classes.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from keras import Input, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "\n",
    "TRAIN_DIR = r\"D:\\NUS_TERM2_CA2\\Train\"\n",
    "# TRAIN_DIR = r\"C:\\Users\\guofe\\Downloads\\CA2_Data\"\n",
    "\n",
    "HEIGHT = 300\n",
    "WIDTH = 300\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "datagen =  ImageDataGenerator(\n",
    "      preprocessing_function=preprocess_input,\n",
    "      rotation_range=90,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True\n",
    "    )\n",
    "\n",
    "train_generator = datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                                    target_size=(HEIGHT, WIDTH),\n",
    "                                                    batch_size=BATCH_SIZE)\n",
    "\n",
    "class_list = [\"food\", \"landmark\", \"people\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 300, 300, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 300, 300, 8)       104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 300, 300, 8)       32        \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 300, 300, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 150, 150, 16)      528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 150, 150, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 150, 150, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_64 (MaxPooling (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 75, 75, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 75, 75, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_65 (MaxPooling (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 37, 37, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 37, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_66 (MaxPooling (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 18, 18, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_67 (MaxPooling (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 9, 9, 256)         131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_68 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 4, 4, 512)         524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_69 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_69 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 2, 2, 1024)        2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 2, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,954,427\n",
      "Trainable params: 2,950,347\n",
      "Non-trainable params: 4,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "inputs = Input((HEIGHT, WIDTH, 3))\n",
    "y = Conv2D(8, kernel_size=2, padding='same')(inputs)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(16, kernel_size=2, padding='same')(y)\n",
    "# y = Conv2D(16, kernel_size=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(32, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(64, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(128, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(256, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(512, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(1024, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Flatten()(y)\n",
    "# y = Dense(128, activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Dense(128, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Dense(128, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "y = Dense(len(class_list), activation='softmax')(y) \n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) //accuracy did not change much with 40 epoch\n",
    "\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 1.2822 - acc: 0.3850\n",
      "\n",
      "Epoch 00001: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 2/40\n",
      "125/125 [==============================] - 19s 155ms/step - loss: 1.0262 - acc: 0.4776\n",
      "\n",
      "Epoch 00002: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 3/40\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.9860 - acc: 0.5006\n",
      "\n",
      "Epoch 00003: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 4/40\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.9669 - acc: 0.5270\n",
      "\n",
      "Epoch 00004: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 5/40\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.9230 - acc: 0.5250\n",
      "\n",
      "Epoch 00005: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 6/40\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.9382 - acc: 0.5543\n",
      "\n",
      "Epoch 00006: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 7/40\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.9122 - acc: 0.5610\n",
      "\n",
      "Epoch 00007: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 8/40\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.8936 - acc: 0.5570\n",
      "\n",
      "Epoch 00008: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 9/40\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.8970 - acc: 0.5834\n",
      "\n",
      "Epoch 00009: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 10/40\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.9041 - acc: 0.5530\n",
      "\n",
      "Epoch 00010: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 11/40\n",
      "125/125 [==============================] - 20s 163ms/step - loss: 0.8746 - acc: 0.5897\n",
      "\n",
      "Epoch 00011: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 12/40\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.8662 - acc: 0.5850\n",
      "\n",
      "Epoch 00012: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 13/40\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.8782 - acc: 0.5929\n",
      "\n",
      "Epoch 00013: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 14/40\n",
      "125/125 [==============================] - 21s 164ms/step - loss: 0.8119 - acc: 0.6450\n",
      "\n",
      "Epoch 00014: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 15/40\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.8402 - acc: 0.6450\n",
      "\n",
      "Epoch 00015: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 16/40\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.8434 - acc: 0.6310\n",
      "\n",
      "Epoch 00016: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 17/40\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.7874 - acc: 0.6546\n",
      "\n",
      "Epoch 00017: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 18/40\n",
      "125/125 [==============================] - 20s 159ms/step - loss: 0.8153 - acc: 0.6584\n",
      "\n",
      "Epoch 00018: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 19/40\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.7961 - acc: 0.6500\n",
      "\n",
      "Epoch 00019: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 20/40\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.7779 - acc: 0.6820\n",
      "\n",
      "Epoch 00020: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 21/40\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.7596 - acc: 0.6886\n",
      "\n",
      "Epoch 00021: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 22/40\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.7643 - acc: 0.6860\n",
      "\n",
      "Epoch 00022: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 23/40\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.7218 - acc: 0.6959\n",
      "\n",
      "Epoch 00023: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 24/40\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.7332 - acc: 0.6910\n",
      "\n",
      "Epoch 00024: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 25/40\n",
      "125/125 [==============================] - 20s 160ms/step - loss: 0.7661 - acc: 0.6809\n",
      "\n",
      "Epoch 00025: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 26/40\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.7445 - acc: 0.6960\n",
      "\n",
      "Epoch 00026: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 27/40\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.6953 - acc: 0.7100\n",
      "\n",
      "Epoch 00027: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 28/40\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.7100 - acc: 0.6983\n",
      "\n",
      "Epoch 00028: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 29/40\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.7303 - acc: 0.6910\n",
      "\n",
      "Epoch 00029: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 30/40\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.7342 - acc: 0.6840\n",
      "\n",
      "Epoch 00030: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 31/40\n",
      "125/125 [==============================] - 20s 163ms/step - loss: 0.6827 - acc: 0.7380\n",
      "\n",
      "Epoch 00031: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 32/40\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.6699 - acc: 0.7206\n",
      "\n",
      "Epoch 00032: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 33/40\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.7052 - acc: 0.7110\n",
      "\n",
      "Epoch 00033: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 34/40\n",
      "125/125 [==============================] - 21s 164ms/step - loss: 0.6772 - acc: 0.7270\n",
      "\n",
      "Epoch 00034: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 35/40\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.7103 - acc: 0.7280\n",
      "\n",
      "Epoch 00035: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 36/40\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.6554 - acc: 0.7409\n",
      "\n",
      "Epoch 00036: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 37/40\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.6684 - acc: 0.7320\n",
      "\n",
      "Epoch 00037: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 38/40\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.6520 - acc: 0.7340\n",
      "\n",
      "Epoch 00038: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 39/40\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.6228 - acc: 0.7397\n",
      "\n",
      "Epoch 00039: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 40/40\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.6531 - acc: 0.7386\n",
      "\n",
      "Epoch 00040: saving model to ./checkpoints/ca2_model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from PIL import Image\n",
    "\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 8\n",
    "num_train_images = 1000\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ca2\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 300, 300, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_89 (Conv2D)           (None, 300, 300, 8)       104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 300, 300, 8)       32        \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 300, 300, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_87 (MaxPooling (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_90 (Conv2D)           (None, 150, 150, 16)      528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 150, 150, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 150, 150, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_88 (MaxPooling (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 75, 75, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_69 (Batc (None, 75, 75, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_89 (MaxPooling (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 37, 37, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 37, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_90 (MaxPooling (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_93 (Conv2D)           (None, 18, 18, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_91 (Activation)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_91 (MaxPooling (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_94 (Conv2D)           (None, 9, 9, 256)         131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_92 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_92 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 4, 4, 512)         524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_93 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_93 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_122 (Dropout)        (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 2, 2, 1024)        2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 2, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_94 (Activation)   (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_94 (MaxPooling (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_123 (Dropout)        (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,954,427\n",
      "Trainable params: 2,950,347\n",
      "Non-trainable params: 4,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "inputs = Input((HEIGHT, WIDTH, 3))\n",
    "y = Conv2D(8, kernel_size=2, padding='same')(inputs)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(16, kernel_size=2, padding='same')(y)\n",
    "# y = Conv2D(16, kernel_size=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(32, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(64, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(128, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(256, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(512, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(1024, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Flatten()(y)\n",
    "# y = Dense(128, activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Dense(128, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Dense(128, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "y = Dense(len(class_list), activation='softmax')(y) \n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) //accuracy did not change much with 40 epoch\n",
    "\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 1.2508 - acc: 0.3861\n",
      "\n",
      "Epoch 00001: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 20s 157ms/step - loss: 1.0293 - acc: 0.4520\n",
      "\n",
      "Epoch 00002: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.9629 - acc: 0.5256\n",
      "\n",
      "Epoch 00003: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.9598 - acc: 0.5240\n",
      "\n",
      "Epoch 00004: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.9109 - acc: 0.5390\n",
      "\n",
      "Epoch 00005: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 21s 164ms/step - loss: 0.9383 - acc: 0.5480\n",
      "\n",
      "Epoch 00006: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 21s 164ms/step - loss: 0.8929 - acc: 0.5680\n",
      "\n",
      "Epoch 00007: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.9014 - acc: 0.5794\n",
      "\n",
      "Epoch 00008: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 21s 164ms/step - loss: 0.8405 - acc: 0.5980\n",
      "\n",
      "Epoch 00009: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.8870 - acc: 0.6087\n",
      "\n",
      "Epoch 00010: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.8655 - acc: 0.6210\n",
      "\n",
      "Epoch 00011: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.8827 - acc: 0.6027\n",
      "\n",
      "Epoch 00012: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.8312 - acc: 0.6237\n",
      "\n",
      "Epoch 00013: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.8858 - acc: 0.5920\n",
      "\n",
      "Epoch 00014: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.8765 - acc: 0.5990\n",
      "\n",
      "Epoch 00015: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.7944 - acc: 0.6404\n",
      "\n",
      "Epoch 00016: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 22s 177ms/step - loss: 0.8232 - acc: 0.6360\n",
      "\n",
      "Epoch 00017: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 20s 161ms/step - loss: 0.8229 - acc: 0.6411\n",
      "\n",
      "Epoch 00018: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.7891 - acc: 0.6610\n",
      "\n",
      "Epoch 00019: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.7816 - acc: 0.6670\n",
      "\n",
      "Epoch 00020: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.8036 - acc: 0.6630\n",
      "\n",
      "Epoch 00021: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.8064 - acc: 0.6463\n",
      "\n",
      "Epoch 00022: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.7504 - acc: 0.6890\n",
      "\n",
      "Epoch 00023: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.7633 - acc: 0.6840\n",
      "\n",
      "Epoch 00024: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.7302 - acc: 0.7030\n",
      "\n",
      "Epoch 00025: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.7773 - acc: 0.6850\n",
      "\n",
      "Epoch 00026: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 22s 174ms/step - loss: 0.7597 - acc: 0.6786\n",
      "\n",
      "Epoch 00027: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.7417 - acc: 0.7000\n",
      "\n",
      "Epoch 00028: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.7335 - acc: 0.7129\n",
      "\n",
      "Epoch 00029: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.7087 - acc: 0.7260\n",
      "\n",
      "Epoch 00030: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.7241 - acc: 0.7177\n",
      "\n",
      "Epoch 00031: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 21s 172ms/step - loss: 0.7197 - acc: 0.7060\n",
      "\n",
      "Epoch 00032: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.7101 - acc: 0.7197\n",
      "\n",
      "Epoch 00033: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.7189 - acc: 0.7200\n",
      "\n",
      "Epoch 00034: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.7028 - acc: 0.7220\n",
      "\n",
      "Epoch 00035: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.6759 - acc: 0.7294\n",
      "\n",
      "Epoch 00036: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.6819 - acc: 0.7400\n",
      "\n",
      "Epoch 00037: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.7019 - acc: 0.7134\n",
      "\n",
      "Epoch 00038: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.7204 - acc: 0.7070\n",
      "\n",
      "Epoch 00039: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.6910 - acc: 0.7190\n",
      "\n",
      "Epoch 00040: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.6580 - acc: 0.7440\n",
      "\n",
      "Epoch 00041: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.6993 - acc: 0.7074\n",
      "\n",
      "Epoch 00042: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.6684 - acc: 0.7280\n",
      "\n",
      "Epoch 00043: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.6639 - acc: 0.7346\n",
      "\n",
      "Epoch 00044: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.6783 - acc: 0.7370\n",
      "\n",
      "Epoch 00045: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 22s 178ms/step - loss: 0.6522 - acc: 0.7370\n",
      "\n",
      "Epoch 00046: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.6325 - acc: 0.7586\n",
      "\n",
      "Epoch 00047: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 21s 172ms/step - loss: 0.6451 - acc: 0.7320\n",
      "\n",
      "Epoch 00048: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 23s 180ms/step - loss: 0.6848 - acc: 0.7400\n",
      "\n",
      "Epoch 00049: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.6566 - acc: 0.7410\n",
      "\n",
      "Epoch 00050: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 21s 172ms/step - loss: 0.6477 - acc: 0.7410\n",
      "\n",
      "Epoch 00051: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 22s 178ms/step - loss: 0.6074 - acc: 0.7710\n",
      "\n",
      "Epoch 00052: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.6220 - acc: 0.7659\n",
      "\n",
      "Epoch 00053: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.6508 - acc: 0.7390\n",
      "\n",
      "Epoch 00054: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.6144 - acc: 0.7640\n",
      "\n",
      "Epoch 00055: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 21s 164ms/step - loss: 0.6148 - acc: 0.7680\n",
      "\n",
      "Epoch 00056: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 22s 179ms/step - loss: 0.6298 - acc: 0.7490\n",
      "\n",
      "Epoch 00057: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.5543 - acc: 0.7930\n",
      "\n",
      "Epoch 00058: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.6366 - acc: 0.7570\n",
      "\n",
      "Epoch 00059: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.6079 - acc: 0.7600\n",
      "\n",
      "Epoch 00060: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.6092 - acc: 0.7451\n",
      "\n",
      "Epoch 00061: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.6076 - acc: 0.7560\n",
      "\n",
      "Epoch 00062: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.6133 - acc: 0.7600\n",
      "\n",
      "Epoch 00063: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5704 - acc: 0.7839\n",
      "\n",
      "Epoch 00064: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 20s 163ms/step - loss: 0.5807 - acc: 0.7790\n",
      "\n",
      "Epoch 00065: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.6271 - acc: 0.7480\n",
      "\n",
      "Epoch 00066: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.6279 - acc: 0.7570\n",
      "\n",
      "Epoch 00067: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.5716 - acc: 0.7830\n",
      "\n",
      "Epoch 00068: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5817 - acc: 0.7689\n",
      "\n",
      "Epoch 00069: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.6020 - acc: 0.7670\n",
      "\n",
      "Epoch 00070: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.6288 - acc: 0.7630\n",
      "\n",
      "Epoch 00071: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.5840 - acc: 0.7524\n",
      "\n",
      "Epoch 00072: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.5549 - acc: 0.7910\n",
      "\n",
      "Epoch 00073: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.6154 - acc: 0.75804s - loss: 0.\n",
      "\n",
      "Epoch 00074: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.5772 - acc: 0.7800\n",
      "\n",
      "Epoch 00075: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.5202 - acc: 0.8100\n",
      "\n",
      "Epoch 00076: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.5870 - acc: 0.7767\n",
      "\n",
      "Epoch 00077: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.5455 - acc: 0.7880\n",
      "\n",
      "Epoch 00078: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.5364 - acc: 0.8020\n",
      "\n",
      "Epoch 00079: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.5868 - acc: 0.7769\n",
      "\n",
      "Epoch 00080: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 21s 172ms/step - loss: 0.5402 - acc: 0.7999\n",
      "\n",
      "Epoch 00081: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.5351 - acc: 0.8080\n",
      "\n",
      "Epoch 00082: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.6059 - acc: 0.7830\n",
      "\n",
      "Epoch 00083: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.5358 - acc: 0.7737\n",
      "\n",
      "Epoch 00084: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5438 - acc: 0.7960\n",
      "\n",
      "Epoch 00085: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.5469 - acc: 0.7887\n",
      "\n",
      "Epoch 00086: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 22s 174ms/step - loss: 0.5564 - acc: 0.7880\n",
      "\n",
      "Epoch 00087: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 20s 163ms/step - loss: 0.5670 - acc: 0.7710\n",
      "\n",
      "Epoch 00088: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.5667 - acc: 0.7850\n",
      "\n",
      "Epoch 00089: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.5465 - acc: 0.7936\n",
      "\n",
      "Epoch 00090: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.5565 - acc: 0.7870\n",
      "\n",
      "Epoch 00091: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5627 - acc: 0.7860\n",
      "\n",
      "Epoch 00092: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 22s 174ms/step - loss: 0.5620 - acc: 0.7929\n",
      "\n",
      "Epoch 00093: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.5524 - acc: 0.7940\n",
      "\n",
      "Epoch 00094: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 20s 157ms/step - loss: 0.5414 - acc: 0.7890\n",
      "\n",
      "Epoch 00095: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.5431 - acc: 0.7950\n",
      "\n",
      "Epoch 00096: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.5598 - acc: 0.7849\n",
      "\n",
      "Epoch 00097: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.5436 - acc: 0.7887\n",
      "\n",
      "Epoch 00098: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5241 - acc: 0.8050\n",
      "\n",
      "Epoch 00099: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.5381 - acc: 0.7880\n",
      "\n",
      "Epoch 00100: saving model to ./checkpoints/ca2_model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from PIL import Image\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "num_train_images = 1000\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ca2\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 300, 300, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 300, 300, 8)       104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 300, 300, 8)       32        \n",
      "_________________________________________________________________\n",
      "activation_95 (Activation)   (None, 300, 300, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_95 (MaxPooling (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 150, 150, 16)      528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 150, 150, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_96 (Activation)   (None, 150, 150, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_96 (MaxPooling (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_99 (Conv2D)           (None, 75, 75, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 75, 75, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_97 (Activation)   (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_97 (MaxPooling (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_100 (Conv2D)          (None, 37, 37, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 37, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_98 (Activation)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_98 (MaxPooling (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_129 (Dropout)        (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_101 (Conv2D)          (None, 18, 18, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_79 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_99 (Activation)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_99 (MaxPooling (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_130 (Dropout)        (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 9, 9, 256)         131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_100 (Activation)  (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_100 (MaxPoolin (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_131 (Dropout)        (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 4, 4, 512)         524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_101 (Activation)  (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_101 (MaxPoolin (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_132 (Dropout)        (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, 2, 2, 1024)        2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_82 (Batc (None, 2, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_102 (Activation)  (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_102 (MaxPoolin (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_133 (Dropout)        (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 3,595,323\n",
      "Trainable params: 3,591,243\n",
      "Non-trainable params: 4,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "inputs = Input((HEIGHT, WIDTH, 3))\n",
    "y = Conv2D(8, kernel_size=2, padding='same')(inputs)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(16, kernel_size=2, padding='same')(y)\n",
    "# y = Conv2D(16, kernel_size=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(32, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(64, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(128, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(256, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(512, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(1024, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Flatten()(y)\n",
    "# y = Dense(128, activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Dense(512, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Dense(512, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "y = Dense(len(class_list), activation='softmax')(y) \n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) //accuracy did not change much with 40 epoch\n",
    "\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5886 - acc: 0.7787\n",
      "\n",
      "Epoch 00001: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.6141 - acc: 0.7650\n",
      "\n",
      "Epoch 00002: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5718 - acc: 0.7570\n",
      "\n",
      "Epoch 00003: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.6306 - acc: 0.7440\n",
      "\n",
      "Epoch 00004: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.6089 - acc: 0.7459\n",
      "\n",
      "Epoch 00005: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.6293 - acc: 0.7490\n",
      "\n",
      "Epoch 00006: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.5833 - acc: 0.7690\n",
      "\n",
      "Epoch 00007: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.5568 - acc: 0.7800\n",
      "\n",
      "Epoch 00008: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.5663 - acc: 0.7920\n",
      "\n",
      "Epoch 00009: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.6050 - acc: 0.7574\n",
      "\n",
      "Epoch 00010: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.5468 - acc: 0.7777\n",
      "\n",
      "Epoch 00011: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.5991 - acc: 0.7740\n",
      "\n",
      "Epoch 00012: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.5871 - acc: 0.7724\n",
      "\n",
      "Epoch 00013: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.5688 - acc: 0.7740\n",
      "\n",
      "Epoch 00014: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 21s 172ms/step - loss: 0.5836 - acc: 0.7649\n",
      "\n",
      "Epoch 00015: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.5671 - acc: 0.7840\n",
      "\n",
      "Epoch 00016: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.5505 - acc: 0.7950\n",
      "\n",
      "Epoch 00017: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.5666 - acc: 0.7780\n",
      "\n",
      "Epoch 00018: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 23s 182ms/step - loss: 0.5545 - acc: 0.7869\n",
      "\n",
      "Epoch 00019: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.5406 - acc: 0.7910\n",
      "\n",
      "Epoch 00020: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.5688 - acc: 0.7799\n",
      "\n",
      "Epoch 00021: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 21s 172ms/step - loss: 0.5358 - acc: 0.8020\n",
      "\n",
      "Epoch 00022: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.5603 - acc: 0.7860\n",
      "\n",
      "Epoch 00023: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.5447 - acc: 0.7910\n",
      "\n",
      "Epoch 00024: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 21s 172ms/step - loss: 0.5843 - acc: 0.7690\n",
      "\n",
      "Epoch 00025: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.5246 - acc: 0.7896\n",
      "\n",
      "Epoch 00026: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 22s 174ms/step - loss: 0.5286 - acc: 0.7959\n",
      "\n",
      "Epoch 00027: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 21s 164ms/step - loss: 0.5060 - acc: 0.8020\n",
      "\n",
      "Epoch 00028: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.5838 - acc: 0.7680\n",
      "\n",
      "Epoch 00029: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.5699 - acc: 0.7640\n",
      "\n",
      "Epoch 00030: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 22s 178ms/step - loss: 0.5536 - acc: 0.7816\n",
      "\n",
      "Epoch 00031: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.5623 - acc: 0.7689\n",
      "\n",
      "Epoch 00032: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.5318 - acc: 0.7940\n",
      "\n",
      "Epoch 00033: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.5324 - acc: 0.7800\n",
      "\n",
      "Epoch 00034: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5276 - acc: 0.7989\n",
      "\n",
      "Epoch 00035: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.5428 - acc: 0.7870\n",
      "\n",
      "Epoch 00036: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.5223 - acc: 0.8000\n",
      "\n",
      "Epoch 00037: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.5576 - acc: 0.7860\n",
      "\n",
      "Epoch 00038: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.5211 - acc: 0.8030\n",
      "\n",
      "Epoch 00039: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.5232 - acc: 0.7959\n",
      "\n",
      "Epoch 00040: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.5506 - acc: 0.7840\n",
      "\n",
      "Epoch 00041: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 22s 173ms/step - loss: 0.4992 - acc: 0.8180\n",
      "\n",
      "Epoch 00042: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.5402 - acc: 0.7826\n",
      "\n",
      "Epoch 00043: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.5357 - acc: 0.7790\n",
      "\n",
      "Epoch 00044: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.5186 - acc: 0.8150\n",
      "\n",
      "Epoch 00045: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.4900 - acc: 0.8099\n",
      "\n",
      "Epoch 00046: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.4904 - acc: 0.8097\n",
      "\n",
      "Epoch 00047: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.5396 - acc: 0.7940\n",
      "\n",
      "Epoch 00048: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.4550 - acc: 0.8280\n",
      "\n",
      "Epoch 00049: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.5379 - acc: 0.7917\n",
      "\n",
      "Epoch 00050: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 21s 169ms/step - loss: 0.5040 - acc: 0.8110\n",
      "\n",
      "Epoch 00051: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.5391 - acc: 0.7930\n",
      "\n",
      "Epoch 00052: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.4968 - acc: 0.7999\n",
      "\n",
      "Epoch 00053: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.5128 - acc: 0.8077\n",
      "\n",
      "Epoch 00054: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.4998 - acc: 0.8160\n",
      "\n",
      "Epoch 00055: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.4941 - acc: 0.8220\n",
      "\n",
      "Epoch 00056: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.4977 - acc: 0.8087\n",
      "\n",
      "Epoch 00057: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 20s 162ms/step - loss: 0.5093 - acc: 0.8020\n",
      "\n",
      "Epoch 00058: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 22s 174ms/step - loss: 0.5009 - acc: 0.8020\n",
      "\n",
      "Epoch 00059: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.4906 - acc: 0.8250\n",
      "\n",
      "Epoch 00060: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.4617 - acc: 0.8219\n",
      "\n",
      "Epoch 00061: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5017 - acc: 0.7840\n",
      "\n",
      "Epoch 00062: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.5031 - acc: 0.8010\n",
      "\n",
      "Epoch 00063: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.4998 - acc: 0.8060\n",
      "\n",
      "Epoch 00064: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.5121 - acc: 0.7960\n",
      "\n",
      "Epoch 00065: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.4873 - acc: 0.8167\n",
      "\n",
      "Epoch 00066: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 20s 163ms/step - loss: 0.5152 - acc: 0.8030\n",
      "\n",
      "Epoch 00067: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.4776 - acc: 0.8150\n",
      "\n",
      "Epoch 00068: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.4803 - acc: 0.8150\n",
      "\n",
      "Epoch 00069: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.5036 - acc: 0.7957\n",
      "\n",
      "Epoch 00070: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.4745 - acc: 0.8200\n",
      "\n",
      "Epoch 00071: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.5082 - acc: 0.8140\n",
      "\n",
      "Epoch 00072: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.4992 - acc: 0.8290\n",
      "\n",
      "Epoch 00073: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.4772 - acc: 0.8170\n",
      "\n",
      "Epoch 00074: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.4323 - acc: 0.8397\n",
      "\n",
      "Epoch 00075: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.5061 - acc: 0.7970\n",
      "\n",
      "Epoch 00076: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.4844 - acc: 0.8246\n",
      "\n",
      "Epoch 00077: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.4721 - acc: 0.8150\n",
      "\n",
      "Epoch 00078: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.5170 - acc: 0.8167\n",
      "\n",
      "Epoch 00079: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.4694 - acc: 0.8170\n",
      "\n",
      "Epoch 00080: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.4688 - acc: 0.8230\n",
      "\n",
      "Epoch 00081: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.4669 - acc: 0.8240\n",
      "\n",
      "Epoch 00082: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.4579 - acc: 0.8266\n",
      "\n",
      "Epoch 00083: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.5084 - acc: 0.8119\n",
      "\n",
      "Epoch 00084: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 21s 171ms/step - loss: 0.4624 - acc: 0.8300\n",
      "\n",
      "Epoch 00085: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.4719 - acc: 0.8170\n",
      "\n",
      "Epoch 00086: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.4649 - acc: 0.8300\n",
      "\n",
      "Epoch 00087: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 22s 177ms/step - loss: 0.4903 - acc: 0.8120\n",
      "\n",
      "Epoch 00088: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.4298 - acc: 0.8190\n",
      "\n",
      "Epoch 00089: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.4492 - acc: 0.8340\n",
      "\n",
      "Epoch 00090: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 22s 178ms/step - loss: 0.4377 - acc: 0.8310\n",
      "\n",
      "Epoch 00091: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.4541 - acc: 0.8309\n",
      "\n",
      "Epoch 00092: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.4533 - acc: 0.8150\n",
      "\n",
      "Epoch 00093: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 21s 164ms/step - loss: 0.4685 - acc: 0.8190\n",
      "\n",
      "Epoch 00094: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.4496 - acc: 0.8259\n",
      "\n",
      "Epoch 00095: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 20s 164ms/step - loss: 0.4513 - acc: 0.8360\n",
      "\n",
      "Epoch 00096: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.4465 - acc: 0.8260\n",
      "\n",
      "Epoch 00097: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 22s 172ms/step - loss: 0.4695 - acc: 0.8280\n",
      "\n",
      "Epoch 00098: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.4495 - acc: 0.8289\n",
      "\n",
      "Epoch 00099: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.3874 - acc: 0.8530\n",
      "\n",
      "Epoch 00100: saving model to ./checkpoints/ca2_model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from PIL import Image\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "num_train_images = 1000\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ca2\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0919 19:18:06.725368 10600 deprecation_wrapper.py:119] From c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300, 300, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 300, 300, 8)       104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 300, 300, 8)       32        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 300, 300, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 150, 150, 16)      528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 150, 150, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 150, 150, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 75, 75, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 75, 75, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 37, 37, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 37, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 18, 18, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 9, 9, 256)         131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 4, 4, 512)         524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 2, 2, 1024)        2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 2, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 3,595,323\n",
      "Trainable params: 3,591,243\n",
      "Non-trainable params: 4,080\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "inputs = Input((HEIGHT, WIDTH, 3))\n",
    "y = Conv2D(8, kernel_size=2, padding='same')(inputs)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(16, kernel_size=2, padding='same')(y)\n",
    "# y = Conv2D(16, kernel_size=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(32, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(64, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(128, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(256, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(512, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(1024, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "\n",
    "y = Flatten()(y)\n",
    "# y = Dense(128, activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Dense(512, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Dense(512, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "y = Dense(len(class_list), activation='softmax')(y) \n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) //accuracy did not change much with 40 epoch\n",
    "\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 184ms/step - loss: 0.9688 - acc: 0.5579\n",
      "\n",
      "Epoch 00001: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 2/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.9211 - acc: 0.5423\n",
      "\n",
      "Epoch 00002: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 3/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.9073 - acc: 0.5605\n",
      "\n",
      "Epoch 00003: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 4/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.8759 - acc: 0.6230\n",
      "\n",
      "Epoch 00004: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 5/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 9s 153ms/step - loss: 0.9014 - acc: 0.5726\n",
      "\n",
      "Epoch 00005: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 6/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.9133 - acc: 0.5585\n",
      "\n",
      "Epoch 00006: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 7/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.8562 - acc: 0.6120\n",
      "\n",
      "Epoch 00007: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 8/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 157ms/step - loss: 0.8843 - acc: 0.6008\n",
      "\n",
      "Epoch 00008: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 9/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.8910 - acc: 0.5948\n",
      "\n",
      "Epoch 00009: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 10/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.8406 - acc: 0.6290\n",
      "\n",
      "Epoch 00010: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 11/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 155ms/step - loss: 0.8486 - acc: 0.6190\n",
      "\n",
      "Epoch 00011: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 12/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.7766 - acc: 0.69563s - loss: 0.7412\n",
      "\n",
      "Epoch 00012: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 13/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.9001 - acc: 0.6008\n",
      "\n",
      "Epoch 00013: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 14/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.8508 - acc: 0.6391\n",
      "\n",
      "Epoch 00014: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 15/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.8131 - acc: 0.6371\n",
      "\n",
      "Epoch 00015: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 16/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 9s 152ms/step - loss: 0.7717 - acc: 0.6815\n",
      "\n",
      "Epoch 00016: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 17/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.8258 - acc: 0.6310\n",
      "\n",
      "Epoch 00017: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 18/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 178ms/step - loss: 0.8293 - acc: 0.6348\n",
      "\n",
      "Epoch 00018: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 19/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.8118 - acc: 0.6452\n",
      "\n",
      "Epoch 00019: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 20/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.7451 - acc: 0.6835\n",
      "\n",
      "Epoch 00020: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 21/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 176ms/step - loss: 0.7827 - acc: 0.6633\n",
      "\n",
      "Epoch 00021: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 22/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.7585 - acc: 0.6633\n",
      "\n",
      "Epoch 00022: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 23/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.8274 - acc: 0.6391\n",
      "\n",
      "Epoch 00023: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 24/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 9s 153ms/step - loss: 0.7847 - acc: 0.6489\n",
      "\n",
      "Epoch 00024: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 25/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.7463 - acc: 0.7097\n",
      "\n",
      "Epoch 00025: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 26/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.7009 - acc: 0.6996\n",
      "\n",
      "Epoch 00026: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 27/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 169ms/step - loss: 0.7990 - acc: 0.6331\n",
      "\n",
      "Epoch 00027: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 28/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6812 - acc: 0.7097\n",
      "\n",
      "Epoch 00028: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 29/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 183ms/step - loss: 0.7177 - acc: 0.7088\n",
      "\n",
      "Epoch 00029: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 30/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.7047 - acc: 0.7198\n",
      "\n",
      "Epoch 00030: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 31/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.7309 - acc: 0.7157\n",
      "\n",
      "Epoch 00031: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 32/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.7140 - acc: 0.7157\n",
      "\n",
      "Epoch 00032: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 33/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.7437 - acc: 0.7097\n",
      "\n",
      "Epoch 00033: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 34/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6650 - acc: 0.7287\n",
      "\n",
      "Epoch 00034: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 35/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.6510 - acc: 0.7419\n",
      "\n",
      "Epoch 00035: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 36/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.7131 - acc: 0.7218\n",
      "\n",
      "Epoch 00036: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 37/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.7297 - acc: 0.7036\n",
      "\n",
      "Epoch 00037: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 38/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.7614 - acc: 0.6835\n",
      "\n",
      "Epoch 00038: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 39/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.6879 - acc: 0.7339\n",
      "\n",
      "Epoch 00039: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 40/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 160ms/step - loss: 0.7280 - acc: 0.7077\n",
      "\n",
      "Epoch 00040: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 41/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6798 - acc: 0.7258\n",
      "\n",
      "Epoch 00041: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 42/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7101 - acc: 0.7336\n",
      "\n",
      "Epoch 00042: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 43/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.6641 - acc: 0.7198\n",
      "\n",
      "Epoch 00043: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 44/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.7024 - acc: 0.7278\n",
      "\n",
      "Epoch 00044: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 45/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 154ms/step - loss: 0.6486 - acc: 0.7278\n",
      "\n",
      "Epoch 00045: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 46/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.7429 - acc: 0.7077\n",
      "\n",
      "Epoch 00046: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 47/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.6543 - acc: 0.7451\n",
      "\n",
      "Epoch 00047: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 48/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.6573 - acc: 0.7359\n",
      "\n",
      "Epoch 00048: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 49/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.7046 - acc: 0.6875\n",
      "\n",
      "Epoch 00049: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 50/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.6766 - acc: 0.7339\n",
      "\n",
      "Epoch 00050: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 51/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.6789 - acc: 0.7097\n",
      "\n",
      "Epoch 00051: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 52/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 155ms/step - loss: 0.6721 - acc: 0.7177\n",
      "\n",
      "Epoch 00052: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 53/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.6019 - acc: 0.7661\n",
      "\n",
      "Epoch 00053: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 54/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.7172 - acc: 0.7008\n",
      "\n",
      "Epoch 00054: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 55/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 154ms/step - loss: 0.5910 - acc: 0.7500\n",
      "\n",
      "Epoch 00055: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 56/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.7214 - acc: 0.7316\n",
      "\n",
      "Epoch 00056: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 57/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.6108 - acc: 0.7440\n",
      "\n",
      "Epoch 00057: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 58/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7080 - acc: 0.7218\n",
      "\n",
      "Epoch 00058: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 59/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 181ms/step - loss: 0.6502 - acc: 0.7520\n",
      "\n",
      "Epoch 00059: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 60/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 9s 152ms/step - loss: 0.6553 - acc: 0.7379\n",
      "\n",
      "Epoch 00060: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 61/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6348 - acc: 0.7339\n",
      "\n",
      "Epoch 00061: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 62/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.6143 - acc: 0.7621\n",
      "\n",
      "Epoch 00062: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 63/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.6665 - acc: 0.7215\n",
      "\n",
      "Epoch 00063: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 64/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.7051 - acc: 0.7177\n",
      "\n",
      "Epoch 00064: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 65/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 157ms/step - loss: 0.6255 - acc: 0.7560\n",
      "\n",
      "Epoch 00065: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 66/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.6281 - acc: 0.7480\n",
      "\n",
      "Epoch 00066: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 67/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.6493 - acc: 0.7198\n",
      "\n",
      "Epoch 00067: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 68/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.6395 - acc: 0.7681\n",
      "\n",
      "Epoch 00068: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 69/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.5884 - acc: 0.7863\n",
      "\n",
      "Epoch 00069: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 70/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.5822 - acc: 0.7601\n",
      "\n",
      "Epoch 00070: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 71/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.6606 - acc: 0.73599s \n",
      "\n",
      "Epoch 00071: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 72/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.6303 - acc: 0.7310\n",
      "\n",
      "Epoch 00072: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 73/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.5561 - acc: 0.7702\n",
      "\n",
      "Epoch 00073: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 74/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5912 - acc: 0.7702\n",
      "\n",
      "Epoch 00074: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 75/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 182ms/step - loss: 0.5964 - acc: 0.7581\n",
      "\n",
      "Epoch 00075: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 76/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.6773 - acc: 0.7137\n",
      "\n",
      "Epoch 00076: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 77/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.6209 - acc: 0.7601\n",
      "\n",
      "Epoch 00077: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 78/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.6661 - acc: 0.7399\n",
      "\n",
      "Epoch 00078: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 79/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 180ms/step - loss: 0.6411 - acc: 0.7359\n",
      "\n",
      "Epoch 00079: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 80/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.5726 - acc: 0.7817\n",
      "\n",
      "Epoch 00080: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 81/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.5677 - acc: 0.7742\n",
      "\n",
      "Epoch 00081: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 82/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 176ms/step - loss: 0.5727 - acc: 0.7903\n",
      "\n",
      "Epoch 00082: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 83/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.5904 - acc: 0.7661\n",
      "\n",
      "Epoch 00083: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 84/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.5690 - acc: 0.7883\n",
      "\n",
      "Epoch 00084: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 85/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5847 - acc: 0.7863\n",
      "\n",
      "Epoch 00085: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 86/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.5708 - acc: 0.7621\n",
      "\n",
      "Epoch 00086: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 87/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 159ms/step - loss: 0.5298 - acc: 0.8085\n",
      "\n",
      "Epoch 00087: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 88/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 12s 197ms/step - loss: 0.6427 - acc: 0.7661\n",
      "\n",
      "Epoch 00088: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 89/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 154ms/step - loss: 0.5100 - acc: 0.8024\n",
      "\n",
      "Epoch 00089: saving model to ./checkpoints/ca2_model_weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.5651 - acc: 0.7681\n",
      "\n",
      "Epoch 00090: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 91/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.5707 - acc: 0.7863\n",
      "\n",
      "Epoch 00091: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 92/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6025 - acc: 0.7552\n",
      "\n",
      "Epoch 00092: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 93/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 176ms/step - loss: 0.4984 - acc: 0.79035s -\n",
      "\n",
      "Epoch 00093: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 94/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 159ms/step - loss: 0.5395 - acc: 0.7880\n",
      "\n",
      "Epoch 00094: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 95/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.5627 - acc: 0.7540\n",
      "\n",
      "Epoch 00095: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 96/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.5613 - acc: 0.7863\n",
      "\n",
      "Epoch 00096: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 97/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.6044 - acc: 0.7540\n",
      "\n",
      "Epoch 00097: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 98/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.6534 - acc: 0.7540\n",
      "\n",
      "Epoch 00098: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 99/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 169ms/step - loss: 0.5286 - acc: 0.8001\n",
      "\n",
      "Epoch 00099: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 100/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.5650 - acc: 0.7903\n",
      "\n",
      "Epoch 00100: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 101/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.4802 - acc: 0.8327\n",
      "\n",
      "Epoch 00101: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 102/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 154ms/step - loss: 0.6023 - acc: 0.7641\n",
      "\n",
      "Epoch 00102: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 103/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.4770 - acc: 0.8125\n",
      "\n",
      "Epoch 00103: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 104/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.5079 - acc: 0.8145\n",
      "\n",
      "Epoch 00104: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 105/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.5875 - acc: 0.7656\n",
      "\n",
      "Epoch 00105: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 106/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.5409 - acc: 0.7923\n",
      "\n",
      "Epoch 00106: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 107/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.5586 - acc: 0.7863\n",
      "\n",
      "Epoch 00107: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 108/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.6853 - acc: 0.7298\n",
      "\n",
      "Epoch 00108: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 109/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 157ms/step - loss: 0.5630 - acc: 0.7667\n",
      "\n",
      "Epoch 00109: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 110/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.5393 - acc: 0.7863\n",
      "\n",
      "Epoch 00110: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 111/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.5366 - acc: 0.8024\n",
      "\n",
      "Epoch 00111: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 112/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.5610 - acc: 0.7883\n",
      "\n",
      "Epoch 00112: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 113/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.5013 - acc: 0.7964\n",
      "\n",
      "Epoch 00113: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 114/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.5286 - acc: 0.7964\n",
      "\n",
      "Epoch 00114: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 115/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.5684 - acc: 0.7719\n",
      "\n",
      "Epoch 00115: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 116/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5699 - acc: 0.7823\n",
      "\n",
      "Epoch 00116: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 117/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.5919 - acc: 0.7742\n",
      "\n",
      "Epoch 00117: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 118/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.5675 - acc: 0.7661\n",
      "\n",
      "Epoch 00118: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 119/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.4914 - acc: 0.8044\n",
      "\n",
      "Epoch 00119: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 120/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5333 - acc: 0.8004\n",
      "\n",
      "Epoch 00120: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 121/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5109 - acc: 0.8065\n",
      "\n",
      "Epoch 00121: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 122/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5681 - acc: 0.7863\n",
      "\n",
      "Epoch 00122: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 123/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 182ms/step - loss: 0.5862 - acc: 0.7762\n",
      "\n",
      "Epoch 00123: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 124/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 155ms/step - loss: 0.5574 - acc: 0.7923\n",
      "\n",
      "Epoch 00124: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 125/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5398 - acc: 0.8024\n",
      "\n",
      "Epoch 00125: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 126/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.5655 - acc: 0.7679\n",
      "\n",
      "Epoch 00126: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 127/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.5599 - acc: 0.7843\n",
      "\n",
      "Epoch 00127: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 128/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.6234 - acc: 0.7641\n",
      "\n",
      "Epoch 00128: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 129/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5510 - acc: 0.7903\n",
      "\n",
      "Epoch 00129: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 130/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 159ms/step - loss: 0.5815 - acc: 0.7702\n",
      "\n",
      "Epoch 00130: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 131/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 160ms/step - loss: 0.5822 - acc: 0.7823\n",
      "\n",
      "Epoch 00131: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 132/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 181ms/step - loss: 0.5569 - acc: 0.7984\n",
      "\n",
      "Epoch 00132: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 133/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 154ms/step - loss: 0.4948 - acc: 0.7984\n",
      "\n",
      "Epoch 00133: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 134/200\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 10s 161ms/step - loss: 0.5325 - acc: 0.7923\n",
      "\n",
      "Epoch 00134: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 135/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.5426 - acc: 0.7802\n",
      "\n",
      "Epoch 00135: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 136/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.5434 - acc: 0.7863\n",
      "\n",
      "Epoch 00136: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 137/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.5313 - acc: 0.81396s - loss\n",
      "\n",
      "Epoch 00137: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 138/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.5271 - acc: 0.7984\n",
      "\n",
      "Epoch 00138: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 139/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.4899 - acc: 0.8004\n",
      "\n",
      "Epoch 00139: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 140/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.4834 - acc: 0.8318\n",
      "\n",
      "Epoch 00140: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 141/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5763 - acc: 0.7802\n",
      "\n",
      "Epoch 00141: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 142/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 176ms/step - loss: 0.5617 - acc: 0.7722\n",
      "\n",
      "Epoch 00142: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 143/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.4881 - acc: 0.8102\n",
      "\n",
      "Epoch 00143: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 144/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5134 - acc: 0.8024\n",
      "\n",
      "Epoch 00144: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 145/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.5186 - acc: 0.8226\n",
      "\n",
      "Epoch 00145: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 146/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.5879 - acc: 0.7681\n",
      "\n",
      "Epoch 00146: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 147/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.4877 - acc: 0.8185\n",
      "\n",
      "Epoch 00147: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 148/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.5385 - acc: 0.7903\n",
      "\n",
      "Epoch 00148: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 149/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.5576 - acc: 0.7903\n",
      "\n",
      "Epoch 00149: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 150/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.5830 - acc: 0.7621\n",
      "\n",
      "Epoch 00150: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 151/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.5321 - acc: 0.7984\n",
      "\n",
      "Epoch 00151: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 152/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.5390 - acc: 0.7964\n",
      "\n",
      "Epoch 00152: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 153/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.5531 - acc: 0.7762\n",
      "\n",
      "Epoch 00153: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 154/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.5039 - acc: 0.8105\n",
      "\n",
      "Epoch 00154: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 155/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.5516 - acc: 0.8105\n",
      "\n",
      "Epoch 00155: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 156/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.5939 - acc: 0.7581\n",
      "\n",
      "Epoch 00156: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 157/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.4974 - acc: 0.7872\n",
      "\n",
      "Epoch 00157: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 158/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.5080 - acc: 0.8004\n",
      "\n",
      "Epoch 00158: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 159/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.4919 - acc: 0.8226\n",
      "\n",
      "Epoch 00159: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 160/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.5520 - acc: 0.7843\n",
      "\n",
      "Epoch 00160: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 161/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5588 - acc: 0.7843\n",
      "\n",
      "Epoch 00161: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 162/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 180ms/step - loss: 0.4947 - acc: 0.8065\n",
      "\n",
      "Epoch 00162: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 163/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.5781 - acc: 0.7808\n",
      "\n",
      "Epoch 00163: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 164/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.5462 - acc: 0.7984\n",
      "\n",
      "Epoch 00164: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 165/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 160ms/step - loss: 0.5414 - acc: 0.7923\n",
      "\n",
      "Epoch 00165: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 166/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.5469 - acc: 0.7863\n",
      "\n",
      "Epoch 00166: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 167/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.5571 - acc: 0.7681\n",
      "\n",
      "Epoch 00167: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 168/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5103 - acc: 0.8044\n",
      "\n",
      "Epoch 00168: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 169/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.6173 - acc: 0.7379\n",
      "\n",
      "Epoch 00169: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 170/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.5269 - acc: 0.7797\n",
      "\n",
      "Epoch 00170: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 171/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.4944 - acc: 0.8226\n",
      "\n",
      "Epoch 00171: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 172/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 181ms/step - loss: 0.5596 - acc: 0.7681\n",
      "\n",
      "Epoch 00172: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 173/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.5462 - acc: 0.8064\n",
      "\n",
      "Epoch 00173: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 174/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.5583 - acc: 0.8004\n",
      "\n",
      "Epoch 00174: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 175/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.5075 - acc: 0.8024\n",
      "\n",
      "Epoch 00175: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 176/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 12s 188ms/step - loss: 0.5310 - acc: 0.7863\n",
      "\n",
      "Epoch 00176: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 177/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 9s 153ms/step - loss: 0.5325 - acc: 0.7964\n",
      "\n",
      "Epoch 00177: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 178/200\n",
      "Learning rate:  5e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 10s 168ms/step - loss: 0.5912 - acc: 0.7601\n",
      "\n",
      "Epoch 00178: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 179/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.5201 - acc: 0.7918\n",
      "\n",
      "Epoch 00179: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 180/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.5106 - acc: 0.8024\n",
      "\n",
      "Epoch 00180: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 181/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.5176 - acc: 0.8065\n",
      "\n",
      "Epoch 00181: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 182/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.5930 - acc: 0.7702\n",
      "\n",
      "Epoch 00182: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 183/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.5897 - acc: 0.7641\n",
      "\n",
      "Epoch 00183: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 184/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.4942 - acc: 0.8105\n",
      "\n",
      "Epoch 00184: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 185/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.5435 - acc: 0.7823\n",
      "\n",
      "Epoch 00185: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 186/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.5498 - acc: 0.7964\n",
      "\n",
      "Epoch 00186: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 187/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.4759 - acc: 0.8266\n",
      "\n",
      "Epoch 00187: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 188/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.5385 - acc: 0.7843\n",
      "\n",
      "Epoch 00188: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 189/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5585 - acc: 0.7782\n",
      "\n",
      "Epoch 00189: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 190/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.5801 - acc: 0.7800\n",
      "\n",
      "Epoch 00190: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 191/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.5966 - acc: 0.7581\n",
      "\n",
      "Epoch 00191: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 192/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.5283 - acc: 0.7964\n",
      "\n",
      "Epoch 00192: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 193/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.5094 - acc: 0.7944\n",
      "\n",
      "Epoch 00193: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 194/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 160ms/step - loss: 0.6168 - acc: 0.7601\n",
      "\n",
      "Epoch 00194: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 195/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.5582 - acc: 0.7883\n",
      "\n",
      "Epoch 00195: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 196/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.5206 - acc: 0.7998\n",
      "\n",
      "Epoch 00196: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 197/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.5423 - acc: 0.8004\n",
      "\n",
      "Epoch 00197: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 198/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5565 - acc: 0.7540\n",
      "\n",
      "Epoch 00198: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 199/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.5230 - acc: 0.7923\n",
      "\n",
      "Epoch 00199: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 200/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.5794 - acc: 0.7840\n",
      "\n",
      "Epoch 00200: saving model to ./checkpoints/ca2_model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from PIL import Image\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "num_train_images = 1000\n",
    "\n",
    "def lrSchedule(epoch):\n",
    "    lr = 1e-2\n",
    "\n",
    "    if epoch > 160:\n",
    "        lr *= 0.5e-3\n",
    "\n",
    "    elif epoch > 140:\n",
    "        lr *= 1e-3\n",
    "\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "\n",
    "    print('Learning rate: ', lr)\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "\n",
    "LRScheduler = LearningRateScheduler(lrSchedule)\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ca2\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint, LRScheduler]\n",
    "\n",
    "history = model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 300, 300, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 300, 300, 8)       104       \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 300, 300, 8)       32        \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 300, 300, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 150, 150, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 150, 150, 16)      528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 150, 150, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 150, 150, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 75, 75, 32)        2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 75, 75, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 37, 37, 64)        8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 37, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 18, 18, 128)       32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 9, 9, 256)         131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 4, 4, 512)         524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 2, 2, 512)         262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 2, 2, 1024)        2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 2, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 3,860,027\n",
      "Trainable params: 3,854,923\n",
      "Non-trainable params: 5,104\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "inputs = Input((HEIGHT, WIDTH, 3))\n",
    "y = Conv2D(8, kernel_size=2, padding='same')(inputs)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(16, kernel_size=2, padding='same')(y)\n",
    "# y = Conv2D(16, kernel_size=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(32, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(64, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(128, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(256, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(512, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(512, kernel_size=1, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "# y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Conv2D(1024, kernel_size=2, padding='same')(y)\n",
    "y = BatchNormalization()(y)\n",
    "y = Activation('relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "\n",
    "y = Flatten()(y)\n",
    "# y = Dense(128, activation='relu', kernel_initializer='he_normal')(y)\n",
    "y = Dense(512, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "\n",
    "y = Dense(512, activation='relu')(y)\n",
    "y = Dropout(0.3)(y)\n",
    "y = Dense(len(class_list), activation='softmax')(y) \n",
    "\n",
    "\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) //accuracy did not change much with 40 epoch\n",
    "\n",
    "opt = SGD(lr=0.01)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 13s 209ms/step - loss: 1.4753 - acc: 0.3920\n",
      "\n",
      "Epoch 00001: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 2/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 154ms/step - loss: 1.2004 - acc: 0.4375\n",
      "\n",
      "Epoch 00002: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 3/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 1.1176 - acc: 0.4355\n",
      "\n",
      "Epoch 00003: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 4/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 1.0824 - acc: 0.4617\n",
      "\n",
      "Epoch 00004: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 5/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 12s 188ms/step - loss: 1.0751 - acc: 0.4677\n",
      "\n",
      "Epoch 00005: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 6/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 159ms/step - loss: 1.0140 - acc: 0.4787\n",
      "\n",
      "Epoch 00006: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 7/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 157ms/step - loss: 0.9439 - acc: 0.5262\n",
      "\n",
      "Epoch 00007: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 8/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 169ms/step - loss: 0.9250 - acc: 0.5484\n",
      "\n",
      "Epoch 00008: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 9/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.9682 - acc: 0.5161\n",
      "\n",
      "Epoch 00009: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 10/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.9173 - acc: 0.5786\n",
      "\n",
      "Epoch 00010: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 11/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.9002 - acc: 0.5585\n",
      "\n",
      "Epoch 00011: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 12/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.9389 - acc: 0.5302\n",
      "\n",
      "Epoch 00012: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 13/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 183ms/step - loss: 0.8693 - acc: 0.5544\n",
      "\n",
      "Epoch 00013: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 14/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.9097 - acc: 0.5377\n",
      "\n",
      "Epoch 00014: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 15/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 12s 199ms/step - loss: 0.9042 - acc: 0.5363\n",
      "\n",
      "Epoch 00015: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 16/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.8789 - acc: 0.5968\n",
      "\n",
      "Epoch 00016: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 17/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.8456 - acc: 0.5962\n",
      "\n",
      "Epoch 00017: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 18/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.8583 - acc: 0.6149\n",
      "\n",
      "Epoch 00018: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 19/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.8302 - acc: 0.6230\n",
      "\n",
      "Epoch 00019: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 20/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.8745 - acc: 0.5847\n",
      "\n",
      "Epoch 00020: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 21/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.7931 - acc: 0.6190\n",
      "\n",
      "Epoch 00021: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 22/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 181ms/step - loss: 0.8852 - acc: 0.5847\n",
      "\n",
      "Epoch 00022: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 23/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.8762 - acc: 0.5873\n",
      "\n",
      "Epoch 00023: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 24/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.8670 - acc: 0.5706\n",
      "\n",
      "Epoch 00024: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 25/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.7818 - acc: 0.6431\n",
      "\n",
      "Epoch 00025: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 26/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.8452 - acc: 0.5867\n",
      "\n",
      "Epoch 00026: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 27/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.8695 - acc: 0.6169\n",
      "\n",
      "Epoch 00027: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 28/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 169ms/step - loss: 0.7894 - acc: 0.6224\n",
      "\n",
      "Epoch 00028: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 29/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.8445 - acc: 0.6190\n",
      "\n",
      "Epoch 00029: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 30/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.7868 - acc: 0.6325\n",
      "\n",
      "Epoch 00030: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 31/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.8491 - acc: 0.6190\n",
      "\n",
      "Epoch 00031: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 32/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.8243 - acc: 0.6210\n",
      "\n",
      "Epoch 00032: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 33/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.8488 - acc: 0.60690s - loss: 0.8478 - acc: 0.610\n",
      "\n",
      "Epoch 00033: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 34/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.7916 - acc: 0.6250\n",
      "\n",
      "Epoch 00034: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 35/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 160ms/step - loss: 0.8474 - acc: 0.63106s \n",
      "\n",
      "Epoch 00035: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 36/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.8041 - acc: 0.6169\n",
      "\n",
      "Epoch 00036: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 37/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.8374 - acc: 0.6069\n",
      "\n",
      "Epoch 00037: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 38/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.7869 - acc: 0.6310\n",
      "\n",
      "Epoch 00038: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 39/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.8154 - acc: 0.6083\n",
      "\n",
      "Epoch 00039: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 40/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.8079 - acc: 0.6210\n",
      "\n",
      "Epoch 00040: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 41/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.7940 - acc: 0.6391\n",
      "\n",
      "Epoch 00041: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 42/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.8127 - acc: 0.6285\n",
      "\n",
      "Epoch 00042: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 43/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.8086 - acc: 0.6270\n",
      "\n",
      "Epoch 00043: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 44/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 12s 186ms/step - loss: 0.7814 - acc: 0.6512\n",
      "\n",
      "Epoch 00044: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 45/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.8004 - acc: 0.6593\n",
      "\n",
      "Epoch 00045: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 46/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.7619 - acc: 0.6532\n",
      "\n",
      "Epoch 00046: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 47/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.7649 - acc: 0.6506\n",
      "\n",
      "Epoch 00047: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 48/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 160ms/step - loss: 0.8279 - acc: 0.6250\n",
      "\n",
      "Epoch 00048: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 49/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.7891 - acc: 0.6290\n",
      "\n",
      "Epoch 00049: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 50/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7583 - acc: 0.6633\n",
      "\n",
      "Epoch 00050: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 51/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 173ms/step - loss: 0.8078 - acc: 0.6368\n",
      "\n",
      "Epoch 00051: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 52/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.7935 - acc: 0.6250\n",
      "\n",
      "Epoch 00052: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 53/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 176ms/step - loss: 0.7703 - acc: 0.6552\n",
      "\n",
      "Epoch 00053: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 54/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.7487 - acc: 0.6815\n",
      "\n",
      "Epoch 00054: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 55/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 176ms/step - loss: 0.8215 - acc: 0.6452\n",
      "\n",
      "Epoch 00055: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 56/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.8019 - acc: 0.6310\n",
      "\n",
      "Epoch 00056: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 57/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 157ms/step - loss: 0.7281 - acc: 0.6935\n",
      "\n",
      "Epoch 00057: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 58/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.7420 - acc: 0.6786\n",
      "\n",
      "Epoch 00058: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 59/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.7511 - acc: 0.6532\n",
      "\n",
      "Epoch 00059: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 60/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.7066 - acc: 0.7077\n",
      "\n",
      "Epoch 00060: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 61/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.8066 - acc: 0.6371\n",
      "\n",
      "Epoch 00061: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 62/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.7452 - acc: 0.6472\n",
      "\n",
      "Epoch 00062: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 63/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 9s 152ms/step - loss: 0.7239 - acc: 0.6861\n",
      "\n",
      "Epoch 00063: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 64/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.7183 - acc: 0.6875\n",
      "\n",
      "Epoch 00064: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 65/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.7913 - acc: 0.6573\n",
      "\n",
      "Epoch 00065: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 66/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.7517 - acc: 0.6673\n",
      "\n",
      "Epoch 00066: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 67/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 160ms/step - loss: 0.6692 - acc: 0.7339\n",
      "\n",
      "Epoch 00067: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 68/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.6796 - acc: 0.7056\n",
      "\n",
      "Epoch 00068: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 69/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.6541 - acc: 0.7137\n",
      "\n",
      "Epoch 00069: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 70/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.7179 - acc: 0.6935\n",
      "\n",
      "Epoch 00070: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 71/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.7786 - acc: 0.6659\n",
      "\n",
      "Epoch 00071: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 72/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.6208 - acc: 0.7762\n",
      "\n",
      "Epoch 00072: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 73/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.6919 - acc: 0.7218\n",
      "\n",
      "Epoch 00073: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 74/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.7099 - acc: 0.7157\n",
      "\n",
      "Epoch 00074: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 75/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 169ms/step - loss: 0.6621 - acc: 0.7157\n",
      "\n",
      "Epoch 00075: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 76/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.7418 - acc: 0.6904\n",
      "\n",
      "Epoch 00076: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 77/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.7156 - acc: 0.6996\n",
      "\n",
      "Epoch 00077: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 78/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.6967 - acc: 0.6976\n",
      "\n",
      "Epoch 00078: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 79/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.6964 - acc: 0.7238\n",
      "\n",
      "Epoch 00079: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 80/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.6319 - acc: 0.7480\n",
      "\n",
      "Epoch 00080: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 81/200\n",
      "Learning rate:  0.01\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.6888 - acc: 0.7157\n",
      "\n",
      "Epoch 00081: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 82/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 157ms/step - loss: 0.6228 - acc: 0.7379\n",
      "\n",
      "Epoch 00082: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 83/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 156ms/step - loss: 0.6338 - acc: 0.7454\n",
      "\n",
      "Epoch 00083: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 84/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 12s 186ms/step - loss: 0.6831 - acc: 0.7278\n",
      "\n",
      "Epoch 00084: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 85/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.6684 - acc: 0.7117\n",
      "\n",
      "Epoch 00085: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 86/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 9s 153ms/step - loss: 0.6757 - acc: 0.7137\n",
      "\n",
      "Epoch 00086: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 87/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.5851 - acc: 0.7762\n",
      "\n",
      "Epoch 00087: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 88/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.6233 - acc: 0.7537\n",
      "\n",
      "Epoch 00088: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 89/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.6730 - acc: 0.7238\n",
      "\n",
      "Epoch 00089: saving model to ./checkpoints/ca2_model_weights.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6884 - acc: 0.7065\n",
      "\n",
      "Epoch 00090: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 91/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.6172 - acc: 0.7500\n",
      "\n",
      "Epoch 00091: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 92/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.6306 - acc: 0.7298\n",
      "\n",
      "Epoch 00092: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 93/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.6166 - acc: 0.7520\n",
      "\n",
      "Epoch 00093: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 94/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.6520 - acc: 0.7278\n",
      "\n",
      "Epoch 00094: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 95/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.5967 - acc: 0.7520\n",
      "\n",
      "Epoch 00095: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 96/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.6433 - acc: 0.7350\n",
      "\n",
      "Epoch 00096: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 97/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6009 - acc: 0.7621\n",
      "\n",
      "Epoch 00097: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 98/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.6458 - acc: 0.7419\n",
      "\n",
      "Epoch 00098: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 99/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6750 - acc: 0.7198\n",
      "\n",
      "Epoch 00099: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 100/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6395 - acc: 0.7477\n",
      "\n",
      "Epoch 00100: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 101/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.5830 - acc: 0.7641\n",
      "\n",
      "Epoch 00101: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 102/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 159ms/step - loss: 0.6122 - acc: 0.7621\n",
      "\n",
      "Epoch 00102: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 103/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 180ms/step - loss: 0.6420 - acc: 0.7198\n",
      "\n",
      "Epoch 00103: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 104/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6216 - acc: 0.7258\n",
      "\n",
      "Epoch 00104: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 105/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.6888 - acc: 0.7117\n",
      "\n",
      "Epoch 00105: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 106/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 160ms/step - loss: 0.6077 - acc: 0.7440\n",
      "\n",
      "Epoch 00106: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 107/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.6390 - acc: 0.7520\n",
      "\n",
      "Epoch 00107: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 108/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.6405 - acc: 0.7440\n",
      "\n",
      "Epoch 00108: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 109/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.6121 - acc: 0.7581\n",
      "\n",
      "Epoch 00109: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 110/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6152 - acc: 0.7500\n",
      "\n",
      "Epoch 00110: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 111/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6190 - acc: 0.7480\n",
      "\n",
      "Epoch 00111: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 112/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6064 - acc: 0.7560\n",
      "\n",
      "Epoch 00112: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 113/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.7036 - acc: 0.7097\n",
      "\n",
      "Epoch 00113: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 114/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.6226 - acc: 0.7520\n",
      "\n",
      "Epoch 00114: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 115/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.6292 - acc: 0.7520\n",
      "\n",
      "Epoch 00115: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 116/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.5679 - acc: 0.7693\n",
      "\n",
      "Epoch 00116: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 117/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.6183 - acc: 0.7399\n",
      "\n",
      "Epoch 00117: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 118/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.5588 - acc: 0.7658\n",
      "\n",
      "Epoch 00118: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 119/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.6462 - acc: 0.7117\n",
      "\n",
      "Epoch 00119: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 120/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.6827 - acc: 0.7339\n",
      "\n",
      "Epoch 00120: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 121/200\n",
      "Learning rate:  0.001\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6425 - acc: 0.7500\n",
      "\n",
      "Epoch 00121: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 122/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.5963 - acc: 0.7601\n",
      "\n",
      "Epoch 00122: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 123/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.6173 - acc: 0.7474\n",
      "\n",
      "Epoch 00123: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 124/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6129 - acc: 0.7560\n",
      "\n",
      "Epoch 00124: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 125/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.6256 - acc: 0.7601\n",
      "\n",
      "Epoch 00125: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 126/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.5649 - acc: 0.7520\n",
      "\n",
      "Epoch 00126: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 127/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.6644 - acc: 0.7218\n",
      "\n",
      "Epoch 00127: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 128/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.6502 - acc: 0.7480\n",
      "\n",
      "Epoch 00128: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 129/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.5772 - acc: 0.7520\n",
      "\n",
      "Epoch 00129: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 130/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6559 - acc: 0.7238\n",
      "\n",
      "Epoch 00130: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 131/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5745 - acc: 0.7944\n",
      "\n",
      "Epoch 00131: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 132/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.6533 - acc: 0.7434\n",
      "\n",
      "Epoch 00132: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 133/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 9s 150ms/step - loss: 0.6313 - acc: 0.7356\n",
      "\n",
      "Epoch 00133: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 134/200\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 11s 183ms/step - loss: 0.5929 - acc: 0.7339\n",
      "\n",
      "Epoch 00134: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 135/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.7090 - acc: 0.7117\n",
      "\n",
      "Epoch 00135: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 136/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 157ms/step - loss: 0.6341 - acc: 0.7440\n",
      "\n",
      "Epoch 00136: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 137/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.5585 - acc: 0.7823\n",
      "\n",
      "Epoch 00137: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 138/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6142 - acc: 0.7480\n",
      "\n",
      "Epoch 00138: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 139/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.5886 - acc: 0.7782\n",
      "\n",
      "Epoch 00139: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 140/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.5800 - acc: 0.7610\n",
      "\n",
      "Epoch 00140: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 141/200\n",
      "Learning rate:  0.0001\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.6346 - acc: 0.7460\n",
      "\n",
      "Epoch 00141: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 142/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.6557 - acc: 0.7339\n",
      "\n",
      "Epoch 00142: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 143/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.5992 - acc: 0.7480\n",
      "\n",
      "Epoch 00143: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 144/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.5774 - acc: 0.7762\n",
      "\n",
      "Epoch 00144: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 145/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.6455 - acc: 0.72585s - loss: \n",
      "\n",
      "Epoch 00145: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 146/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.5745 - acc: 0.7800\n",
      "\n",
      "Epoch 00146: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 147/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.6083 - acc: 0.7520\n",
      "\n",
      "Epoch 00147: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 148/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.6305 - acc: 0.7137\n",
      "\n",
      "Epoch 00148: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 149/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 174ms/step - loss: 0.6422 - acc: 0.7298\n",
      "\n",
      "Epoch 00149: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 150/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 158ms/step - loss: 0.6475 - acc: 0.7460\n",
      "\n",
      "Epoch 00150: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 151/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6210 - acc: 0.7552\n",
      "\n",
      "Epoch 00151: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 152/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.6188 - acc: 0.7592\n",
      "\n",
      "Epoch 00152: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 153/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 161ms/step - loss: 0.6020 - acc: 0.7581\n",
      "\n",
      "Epoch 00153: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 154/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.6343 - acc: 0.7500\n",
      "\n",
      "Epoch 00154: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 155/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.5663 - acc: 0.7681\n",
      "\n",
      "Epoch 00155: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 156/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.6443 - acc: 0.7520\n",
      "\n",
      "Epoch 00156: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 157/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.6478 - acc: 0.7500\n",
      "\n",
      "Epoch 00157: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 158/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.5727 - acc: 0.7722\n",
      "\n",
      "Epoch 00158: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 159/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6394 - acc: 0.7278\n",
      "\n",
      "Epoch 00159: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 160/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 167ms/step - loss: 0.6107 - acc: 0.7500\n",
      "\n",
      "Epoch 00160: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 161/200\n",
      "Learning rate:  1e-05\n",
      "62/62 [==============================] - 10s 166ms/step - loss: 0.5754 - acc: 0.7612\n",
      "\n",
      "Epoch 00161: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 162/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 178ms/step - loss: 0.6080 - acc: 0.7540\n",
      "\n",
      "Epoch 00162: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 163/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 160ms/step - loss: 0.5972 - acc: 0.7621\n",
      "\n",
      "Epoch 00163: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 164/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 177ms/step - loss: 0.6002 - acc: 0.7716\n",
      "\n",
      "Epoch 00164: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 165/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6201 - acc: 0.7339\n",
      "\n",
      "Epoch 00165: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 166/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 172ms/step - loss: 0.6169 - acc: 0.7399\n",
      "\n",
      "Epoch 00166: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 167/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 169ms/step - loss: 0.6287 - acc: 0.7679\n",
      "\n",
      "Epoch 00167: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 168/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.6075 - acc: 0.7419\n",
      "\n",
      "Epoch 00168: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 169/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 157ms/step - loss: 0.6395 - acc: 0.7379\n",
      "\n",
      "Epoch 00169: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 170/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 179ms/step - loss: 0.6200 - acc: 0.7278\n",
      "\n",
      "Epoch 00170: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 171/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 163ms/step - loss: 0.6309 - acc: 0.7540\n",
      "\n",
      "Epoch 00171: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 172/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 12s 188ms/step - loss: 0.6323 - acc: 0.7399\n",
      "\n",
      "Epoch 00172: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 173/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.6322 - acc: 0.7540\n",
      "\n",
      "Epoch 00173: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 174/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5773 - acc: 0.7681\n",
      "\n",
      "Epoch 00174: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 175/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 170ms/step - loss: 0.6073 - acc: 0.7379\n",
      "\n",
      "Epoch 00175: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 176/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.6084 - acc: 0.7661\n",
      "\n",
      "Epoch 00176: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 177/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 159ms/step - loss: 0.6172 - acc: 0.7298\n",
      "\n",
      "Epoch 00177: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 178/200\n",
      "Learning rate:  5e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 11s 177ms/step - loss: 0.6756 - acc: 0.7258\n",
      "\n",
      "Epoch 00178: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 179/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.6595 - acc: 0.7480\n",
      "\n",
      "Epoch 00179: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 180/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5911 - acc: 0.7719\n",
      "\n",
      "Epoch 00180: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 181/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 180ms/step - loss: 0.5757 - acc: 0.7581\n",
      "\n",
      "Epoch 00181: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 182/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.5966 - acc: 0.7581\n",
      "\n",
      "Epoch 00182: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 183/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 162ms/step - loss: 0.5999 - acc: 0.7702\n",
      "\n",
      "Epoch 00183: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 184/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 12s 186ms/step - loss: 0.6047 - acc: 0.7460\n",
      "\n",
      "Epoch 00184: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 185/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 183ms/step - loss: 0.6524 - acc: 0.7290\n",
      "\n",
      "Epoch 00185: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 186/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 180ms/step - loss: 0.6122 - acc: 0.7500\n",
      "\n",
      "Epoch 00186: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 187/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 12s 192ms/step - loss: 0.6138 - acc: 0.7540\n",
      "\n",
      "Epoch 00187: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 188/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 164ms/step - loss: 0.7239 - acc: 0.6915\n",
      "\n",
      "Epoch 00188: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 189/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 12s 188ms/step - loss: 0.6291 - acc: 0.73330s - loss: 0.6242 - acc: 0.7\n",
      "\n",
      "Epoch 00189: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 190/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 12s 192ms/step - loss: 0.6075 - acc: 0.7560\n",
      "\n",
      "Epoch 00190: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 191/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 184ms/step - loss: 0.6273 - acc: 0.7621\n",
      "\n",
      "Epoch 00191: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 192/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.7023 - acc: 0.7192\n",
      "\n",
      "Epoch 00192: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 193/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 12s 190ms/step - loss: 0.6219 - acc: 0.7540\n",
      "\n",
      "Epoch 00193: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 194/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 168ms/step - loss: 0.5986 - acc: 0.7319\n",
      "\n",
      "Epoch 00194: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 195/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.5562 - acc: 0.7843\n",
      "\n",
      "Epoch 00195: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 196/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 175ms/step - loss: 0.6941 - acc: 0.7077\n",
      "\n",
      "Epoch 00196: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 197/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 10s 165ms/step - loss: 0.5771 - acc: 0.7520\n",
      "\n",
      "Epoch 00197: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 198/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 12s 192ms/step - loss: 0.6337 - acc: 0.7379\n",
      "\n",
      "Epoch 00198: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 199/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 176ms/step - loss: 0.6475 - acc: 0.7393\n",
      "\n",
      "Epoch 00199: saving model to ./checkpoints/ca2_model_weights.h5\n",
      "Epoch 200/200\n",
      "Learning rate:  5e-06\n",
      "62/62 [==============================] - 11s 171ms/step - loss: 0.5978 - acc: 0.7742\n",
      "\n",
      "Epoch 00200: saving model to ./checkpoints/ca2_model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from PIL import Image\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "num_train_images = 1000\n",
    "\n",
    "def lrSchedule(epoch):\n",
    "    lr = 1e-2\n",
    "\n",
    "    if epoch > 160:\n",
    "        lr *= 0.5e-3\n",
    "\n",
    "    elif epoch > 140:\n",
    "        lr *= 1e-3\n",
    "\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "\n",
    "    print('Learning rate: ', lr)\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "LRScheduler = LearningRateScheduler(lrSchedule)\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ca2\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint, LRScheduler]\n",
    "\n",
    "history = model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2423 images belonging to 3 classes.\n",
      "Found 724 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "TRAIN_DIR = r\"D:\\NUS_TERM2_CA2\\Train\"\n",
    "TEST_DIR = r\"D:\\NUS_TERM2_CA2\\Validation\"\n",
    "# TRAIN_DIR = r\"C:\\Users\\guofe\\Downloads\\CA2_Data\"\n",
    "\n",
    "HEIGHT = 300\n",
    "WIDTH = 300\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "datagen =  ImageDataGenerator(\n",
    "      preprocessing_function=preprocess_input,\n",
    "      rotation_range=90,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True\n",
    "    )\n",
    "\n",
    "train_generator = datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                                    target_size=(HEIGHT, WIDTH),\n",
    "                                                    batch_size=BATCH_SIZE)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(TEST_DIR,\n",
    "                                                    target_size=(HEIGHT, WIDTH),\n",
    "                                                    batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "class_list = [\"food\", \"landmark\", \"people\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 300, 300, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_conv (Conv2D)              (None, 300, 300, 16) 448         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_bn (BatchNormalization)    (None, 300, 300, 16) 64          Inpt_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_relu (Activation)          (None, 300, 300, 16) 0           Inpt_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Inpt_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_add (Add)             (None, 300, 300, 16) 0           Inpt_relu[0][0]                  \n",
      "                                                                 Stg1_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_add (Add)             (None, 300, 300, 16) 0           Stg1_Blk1_relu[0][0]             \n",
      "                                                                 Stg1_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_add (Add)             (None, 300, 300, 16) 0           Stg1_Blk2_relu[0][0]             \n",
      "                                                                 Stg1_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 300, 300, 16) 0           Stg1_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_conv (Conv2D)    (None, 150, 150, 32) 4640        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_lin_conv (Conv2D)     (None, 150, 150, 32) 544         dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg2_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk1_relu[0][0]             \n",
      "                                                                 Stg2_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk2_relu[0][0]             \n",
      "                                                                 Stg2_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 150, 150, 32) 0           Stg2_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_conv (Conv2D)    (None, 75, 75, 64)   18496       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_lin_conv (Conv2D)     (None, 75, 75, 64)   2112        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg3_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk1_relu[0][0]             \n",
      "                                                                 Stg3_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk2_relu[0][0]             \n",
      "                                                                 Stg3_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 75, 75, 64)   0           Stg3_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_conv (Conv2D)    (None, 38, 38, 128)  73856       dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_lin_conv (Conv2D)     (None, 38, 38, 128)  8320        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg4_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk1_relu[0][0]             \n",
      "                                                                 Stg4_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk2_relu[0][0]             \n",
      "                                                                 Stg4_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 38, 38, 128)  0           Stg4_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk1_Res1_conv (Conv2D)    (None, 19, 19, 256)  295168      dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk1_Res1_bn (BatchNormali (None, 19, 19, 256)  1024        Stg5_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk1_Res1_relu (Activation (None, 19, 19, 256)  0           Stg5_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk1_Res2_conv (Conv2D)    (None, 19, 19, 256)  590080      Stg5_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk1_lin_conv (Conv2D)     (None, 19, 19, 256)  33024       dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk1_Res2_bn (BatchNormali (None, 19, 19, 256)  1024        Stg5_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk1_add (Add)             (None, 19, 19, 256)  0           Stg5_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg5_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk1_relu (Activation)     (None, 19, 19, 256)  0           Stg5_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk2_Res1_conv (Conv2D)    (None, 19, 19, 256)  590080      Stg5_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk2_Res1_bn (BatchNormali (None, 19, 19, 256)  1024        Stg5_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk2_Res1_relu (Activation (None, 19, 19, 256)  0           Stg5_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk2_Res2_conv (Conv2D)    (None, 19, 19, 256)  590080      Stg5_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk2_Res2_bn (BatchNormali (None, 19, 19, 256)  1024        Stg5_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk2_add (Add)             (None, 19, 19, 256)  0           Stg5_Blk1_relu[0][0]             \n",
      "                                                                 Stg5_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk2_relu (Activation)     (None, 19, 19, 256)  0           Stg5_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk3_Res1_conv (Conv2D)    (None, 19, 19, 256)  590080      Stg5_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk3_Res1_bn (BatchNormali (None, 19, 19, 256)  1024        Stg5_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk3_Res1_relu (Activation (None, 19, 19, 256)  0           Stg5_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk3_Res2_conv (Conv2D)    (None, 19, 19, 256)  590080      Stg5_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk3_Res2_bn (BatchNormali (None, 19, 19, 256)  1024        Stg5_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk3_add (Add)             (None, 19, 19, 256)  0           Stg5_Blk2_relu[0][0]             \n",
      "                                                                 Stg5_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg5_Blk3_relu (Activation)     (None, 19, 19, 256)  0           Stg5_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "AvgPool (AveragePooling2D)      (None, 2, 2, 256)    0           Stg5_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1024)         0           AvgPool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 3)            3075        flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,384,771\n",
      "Trainable params: 4,378,787\n",
      "Non-trainable params: 5,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "\n",
    "optmz = optimizers.Adam(lr=0.001)\n",
    "\n",
    "def resLyr(inputs,\n",
    "           numFilters=16,\n",
    "           kernelSz=3,\n",
    "           strides=1,\n",
    "           activation='relu',\n",
    "           batchNorm=True,\n",
    "           convFirst=True,\n",
    "           lyrName=None):\n",
    "  \n",
    "    convLyr = Conv2D(numFilters,\n",
    "                     kernel_size=kernelSz,\n",
    "                     strides=strides,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4),\n",
    "                     name=lyrName + '_conv' if lyrName else None)\n",
    "    x = inputs\n",
    "    if convFirst:\n",
    "        x = convLyr(x)\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "    else:\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "        x = convLyr(x)\n",
    "  \n",
    "    return x\n",
    "\n",
    "\n",
    "def resBlkV1(inputs,\n",
    "             numFilters=16,\n",
    "             numBlocks=3,\n",
    "             downsampleOnFirst=True,\n",
    "             names=None):\n",
    "  \n",
    "    x = inputs\n",
    "    for run in range(0, numBlocks):\n",
    "        strides = 1\n",
    "        blkStr = str(run + 1)\n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            strides = 2\n",
    "            \n",
    "        y = resLyr(inputs=x, numFilters=numFilters, strides=strides, lyrName=names+'_Blk'+blkStr+'_Res1' if names else None)\n",
    "        y = resLyr(inputs=y, numFilters=numFilters, activation=None, lyrName=names+'_Blk'+blkStr+'_Res2' if names else None) \n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            x = resLyr(inputs=x, numFilters=numFilters, kernelSz=1, \n",
    "                       strides=strides, activation=None, batchNorm=False, \n",
    "                       lyrName=names+'_Blk'+blkStr+'_lin' if names else None)\n",
    "\n",
    "        x = add([x, y], name=names+'_Blk'+blkStr+'_add' if names else None) \n",
    "\n",
    "        x = Activation('relu',  name=names+'_Blk'+blkStr+'_relu' if names else None)(x)   \n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def createResNetV1(inputShape=(32, 32, 3),\n",
    "                   numClasses=3):\n",
    "  \n",
    "    inputs = Input(shape=inputShape)\n",
    "    v = resLyr(inputs,\n",
    "               lyrName='Inpt')\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=16,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=False,\n",
    "                 names='Stg1')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=32,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg2')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=64,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg3')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=128,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg4')    \n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=256,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg5')        \n",
    "    \n",
    "    v = AveragePooling2D(pool_size=8,\n",
    "                         name='AvgPool')(v)\n",
    "    v = Flatten()(v)\n",
    "    outputs = Dense(numClasses,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(v)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optmz,\n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = createResNetV1(inputShape=(HEIGHT, WIDTH, 3))  # This is meant for training\n",
    "modelGo = createResNetV1(inputShape=(HEIGHT, WIDTH, 3))  # This is used for final testing\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 3.0693 - acc: 0.4713\n",
      "Epoch 00001: val_acc improved from -inf to 0.40470, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 102s 2s/step - loss: 3.0794 - acc: 0.4677 - val_loss: 3.9159 - val_acc: 0.4047\n",
      "Learning rate:  0.001\n",
      "Epoch 2/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 2.0591 - acc: 0.5051\n",
      "Epoch 00002: val_acc improved from 0.40470 to 0.48066, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 31s 506ms/step - loss: 2.0572 - acc: 0.5051 - val_loss: 4.1467 - val_acc: 0.4807\n",
      "Learning rate:  0.001\n",
      "Epoch 3/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.8494 - acc: 0.5553\n",
      "Epoch 00003: val_acc did not improve from 0.48066\n",
      "62/62 [==============================] - 30s 488ms/step - loss: 1.8455 - acc: 0.5524 - val_loss: 4.7879 - val_acc: 0.4489\n",
      "Learning rate:  0.001\n",
      "Epoch 4/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.6338 - acc: 0.6434\n",
      "Epoch 00004: val_acc improved from 0.48066 to 0.68785, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 30s 488ms/step - loss: 1.6325 - acc: 0.6452 - val_loss: 1.7095 - val_acc: 0.6878\n",
      "Learning rate:  0.001\n",
      "Epoch 5/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.5882 - acc: 0.6250\n",
      "Epoch 00005: val_acc did not improve from 0.68785\n",
      "62/62 [==============================] - 31s 505ms/step - loss: 1.5814 - acc: 0.6250 - val_loss: 2.0878 - val_acc: 0.6340\n",
      "Learning rate:  0.001\n",
      "Epoch 6/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.5992 - acc: 0.6598\n",
      "Epoch 00006: val_acc did not improve from 0.68785\n",
      "62/62 [==============================] - 30s 482ms/step - loss: 1.6183 - acc: 0.6573 - val_loss: 4.5791 - val_acc: 0.4351\n",
      "Learning rate:  0.001\n",
      "Epoch 7/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.5641 - acc: 0.6414\n",
      "Epoch 00007: val_acc improved from 0.68785 to 0.73757, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 30s 491ms/step - loss: 1.5615 - acc: 0.6411 - val_loss: 1.4394 - val_acc: 0.7376\n",
      "Learning rate:  0.001\n",
      "Epoch 8/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.4248 - acc: 0.6653\n",
      "Epoch 00008: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 30s 492ms/step - loss: 1.4354 - acc: 0.6626 - val_loss: 2.6347 - val_acc: 0.5290\n",
      "Learning rate:  0.001\n",
      "Epoch 9/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.4451 - acc: 0.7008\n",
      "Epoch 00009: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 30s 488ms/step - loss: 1.4376 - acc: 0.7036 - val_loss: 2.6137 - val_acc: 0.5276\n",
      "Learning rate:  0.001\n",
      "Epoch 10/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.4280 - acc: 0.7008\n",
      "Epoch 00010: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 31s 504ms/step - loss: 1.4188 - acc: 0.7056 - val_loss: 1.4217 - val_acc: 0.7141\n",
      "Learning rate:  0.001\n",
      "Epoch 11/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.3431 - acc: 0.7480\n",
      "Epoch 00011: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 30s 491ms/step - loss: 1.3407 - acc: 0.7500 - val_loss: 1.4828 - val_acc: 0.6506\n",
      "Learning rate:  0.001\n",
      "Epoch 12/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.4613 - acc: 0.6680- ETA: 3s - loss: \n",
      "Epoch 00012: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 30s 491ms/step - loss: 1.4625 - acc: 0.6673 - val_loss: 2.4267 - val_acc: 0.6271\n",
      "Learning rate:  0.001\n",
      "Epoch 13/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.3219 - acc: 0.7439\n",
      "Epoch 00013: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 30s 488ms/step - loss: 1.3225 - acc: 0.7460 - val_loss: 1.6834 - val_acc: 0.6796\n",
      "Learning rate:  0.001\n",
      "Epoch 14/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.3814 - acc: 0.7131\n",
      "Epoch 00014: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 32s 509ms/step - loss: 1.3769 - acc: 0.7157 - val_loss: 1.3753 - val_acc: 0.7196\n",
      "Learning rate:  0.001\n",
      "Epoch 15/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.3316 - acc: 0.732 - ETA: 0s - loss: 1.3293 - acc: 0.7331\n",
      "Epoch 00015: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 32s 512ms/step - loss: 1.3224 - acc: 0.7354 - val_loss: 1.5061 - val_acc: 0.6381\n",
      "Learning rate:  0.001\n",
      "Epoch 16/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2092 - acc: 0.7459- ETA: 2s - loss: 1.2171 \n",
      "Epoch 00016: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 31s 496ms/step - loss: 1.2078 - acc: 0.7460 - val_loss: 2.3782 - val_acc: 0.5000\n",
      "Learning rate:  0.001\n",
      "Epoch 17/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.3018 - acc: 0.7172\n",
      "Epoch 00017: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 31s 504ms/step - loss: 1.2993 - acc: 0.7177 - val_loss: 1.6590 - val_acc: 0.5884\n",
      "Learning rate:  0.001\n",
      "Epoch 18/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1935 - acc: 0.7556\n",
      "Epoch 00018: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 31s 498ms/step - loss: 1.1922 - acc: 0.7556 - val_loss: 1.2710 - val_acc: 0.7238\n",
      "Learning rate:  0.001\n",
      "Epoch 19/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2693 - acc: 0.7336- ETA\n",
      "Epoch 00019: val_acc did not improve from 0.73757\n",
      "62/62 [==============================] - 31s 492ms/step - loss: 1.2689 - acc: 0.7339 - val_loss: 2.2895 - val_acc: 0.5580\n",
      "Learning rate:  0.001\n",
      "Epoch 20/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2553 - acc: 0.7234\n",
      "Epoch 00020: val_acc improved from 0.73757 to 0.79006, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 32s 511ms/step - loss: 1.2603 - acc: 0.7218 - val_loss: 1.1031 - val_acc: 0.7901\n",
      "Learning rate:  0.001\n",
      "Epoch 21/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1646 - acc: 0.7439\n",
      "Epoch 00021: val_acc did not improve from 0.79006\n",
      "62/62 [==============================] - 31s 507ms/step - loss: 1.1679 - acc: 0.7399 - val_loss: 1.7363 - val_acc: 0.6312\n",
      "Learning rate:  0.001\n",
      "Epoch 22/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1104 - acc: 0.7848\n",
      "Epoch 00022: val_acc improved from 0.79006 to 0.79144, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 31s 498ms/step - loss: 1.1068 - acc: 0.7863 - val_loss: 1.0865 - val_acc: 0.7914\n",
      "Learning rate:  0.001\n",
      "Epoch 23/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1326 - acc: 0.7659- ETA: 4s - lo\n",
      "Epoch 00023: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 31s 495ms/step - loss: 1.1339 - acc: 0.7657 - val_loss: 2.0465 - val_acc: 0.5483\n",
      "Learning rate:  0.001\n",
      "Epoch 24/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0140 - acc: 0.8156\n",
      "Epoch 00024: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 30s 486ms/step - loss: 1.0283 - acc: 0.8125 - val_loss: 1.3047 - val_acc: 0.7169\n",
      "Learning rate:  0.001\n",
      "Epoch 25/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0727 - acc: 0.8115\n",
      "Epoch 00025: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 31s 504ms/step - loss: 1.0650 - acc: 0.8145 - val_loss: 1.2429 - val_acc: 0.7155\n",
      "Learning rate:  0.001\n",
      "Epoch 26/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1084 - acc: 0.7725\n",
      "Epoch 00026: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 32s 510ms/step - loss: 1.1022 - acc: 0.7762 - val_loss: 2.1387 - val_acc: 0.6064\n",
      "Learning rate:  0.001\n",
      "Epoch 27/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1230 - acc: 0.7664\n",
      "Epoch 00027: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 31s 507ms/step - loss: 1.1221 - acc: 0.7681 - val_loss: 1.4140 - val_acc: 0.6285\n",
      "Learning rate:  0.001\n",
      "Epoch 28/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0481 - acc: 0.7947\n",
      "Epoch 00028: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 31s 497ms/step - loss: 1.0407 - acc: 0.7980 - val_loss: 1.0896 - val_acc: 0.7652\n",
      "Learning rate:  0.001\n",
      "Epoch 29/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0972 - acc: 0.7684\n",
      "Epoch 00029: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 30s 490ms/step - loss: 1.1144 - acc: 0.7641 - val_loss: 1.4644 - val_acc: 0.5525\n",
      "Learning rate:  0.001\n",
      "Epoch 30/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0716 - acc: 0.7664\n",
      "Epoch 00030: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 32s 509ms/step - loss: 1.0653 - acc: 0.7702 - val_loss: 1.0641 - val_acc: 0.7472\n",
      "Learning rate:  0.001\n",
      "Epoch 31/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0098 - acc: 0.8094- ETA: 5s -  - ETA: 1s - loss: 1.0015 - acc: 0\n",
      "Epoch 00031: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 31s 501ms/step - loss: 1.0041 - acc: 0.8125 - val_loss: 2.0021 - val_acc: 0.4641\n",
      "Learning rate:  0.001\n",
      "Epoch 32/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9875 - acc: 0.7762\n",
      "Epoch 00032: val_acc did not improve from 0.79144\n",
      "62/62 [==============================] - 31s 493ms/step - loss: 0.9920 - acc: 0.7737 - val_loss: 3.1769 - val_acc: 0.3992\n",
      "Learning rate:  0.001\n",
      "Epoch 33/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0097 - acc: 0.7828\n",
      "Epoch 00033: val_acc improved from 0.79144 to 0.85359, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 31s 497ms/step - loss: 1.0087 - acc: 0.7823 - val_loss: 0.8283 - val_acc: 0.8536\n",
      "Learning rate:  0.001\n",
      "Epoch 34/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9409 - acc: 0.8135- ETA: 5s - l\n",
      "Epoch 00034: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 31s 507ms/step - loss: 0.9455 - acc: 0.8145 - val_loss: 0.8426 - val_acc: 0.8481\n",
      "Learning rate:  0.001\n",
      "Epoch 35/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9124 - acc: 0.8217\n",
      "Epoch 00035: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 32s 516ms/step - loss: 0.9081 - acc: 0.8226 - val_loss: 1.0255 - val_acc: 0.7514\n",
      "Learning rate:  0.001\n",
      "Epoch 36/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0109 - acc: 0.7659\n",
      "Epoch 00036: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 31s 499ms/step - loss: 1.0105 - acc: 0.7636 - val_loss: 1.0518 - val_acc: 0.7624\n",
      "Learning rate:  0.001\n",
      "Epoch 37/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0077 - acc: 0.7684\n",
      "Epoch 00037: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 31s 502ms/step - loss: 1.0116 - acc: 0.7681 - val_loss: 1.1576 - val_acc: 0.6602\n",
      "Learning rate:  0.001\n",
      "Epoch 38/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9530 - acc: 0.7930\n",
      "Epoch 00038: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 30s 488ms/step - loss: 0.9469 - acc: 0.7964 - val_loss: 1.1394 - val_acc: 0.7224\n",
      "Learning rate:  0.001\n",
      "Epoch 39/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9361 - acc: 0.7951\n",
      "Epoch 00039: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 30s 484ms/step - loss: 0.9320 - acc: 0.7964 - val_loss: 0.8434 - val_acc: 0.8177\n",
      "Learning rate:  0.001\n",
      "Epoch 40/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9220 - acc: 0.8074- ETA: 2s - loss: 0.9093 - a\n",
      "Epoch 00040: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 33s 527ms/step - loss: 0.9190 - acc: 0.8085 - val_loss: 1.8931 - val_acc: 0.3909\n",
      "Learning rate:  0.001\n",
      "Epoch 41/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9868 - acc: 0.7623- ETA: 3s - loss: \n",
      "Epoch 00041: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 31s 498ms/step - loss: 1.0031 - acc: 0.7601 - val_loss: 0.9854 - val_acc: 0.7500\n",
      "Learning rate:  0.001\n",
      "Epoch 42/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9258 - acc: 0.7680- ETA: 2s - loss: 0.8941\n",
      "Epoch 00042: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 32s 509ms/step - loss: 0.9250 - acc: 0.7677 - val_loss: 2.1649 - val_acc: 0.4157\n",
      "Learning rate:  0.001\n",
      "Epoch 43/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9229 - acc: 0.7869\n",
      "Epoch 00043: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 31s 494ms/step - loss: 0.9267 - acc: 0.7863 - val_loss: 0.8644 - val_acc: 0.7942\n",
      "Learning rate:  0.001\n",
      "Epoch 44/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9266 - acc: 0.7541\n",
      "Epoch 00044: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 32s 515ms/step - loss: 0.9221 - acc: 0.7560 - val_loss: 0.8314 - val_acc: 0.8080\n",
      "Learning rate:  0.001\n",
      "Epoch 45/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8862 - acc: 0.7926- E\n",
      "Epoch 00045: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 31s 506ms/step - loss: 0.8903 - acc: 0.7899 - val_loss: 1.1432 - val_acc: 0.6229\n",
      "Learning rate:  0.001\n",
      "Epoch 46/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8482 - acc: 0.8217- ETA: 4s - loss: 0.82 - ETA: 1s - loss: 0.8620 - ac\n",
      "Epoch 00046: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 32s 518ms/step - loss: 0.8495 - acc: 0.8226 - val_loss: 0.9972 - val_acc: 0.7072\n",
      "Learning rate:  0.001\n",
      "Epoch 47/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9085 - acc: 0.7705\n",
      "Epoch 00047: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 31s 506ms/step - loss: 0.9093 - acc: 0.7702 - val_loss: 0.7674 - val_acc: 0.8536\n",
      "Learning rate:  0.001\n",
      "Epoch 48/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8296 - acc: 0.8033\n",
      "Epoch 00048: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 31s 498ms/step - loss: 0.8356 - acc: 0.7984 - val_loss: 0.7515 - val_acc: 0.8494\n",
      "Learning rate:  0.001\n",
      "Epoch 49/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8502 - acc: 0.8074- E\n",
      "Epoch 00049: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 30s 484ms/step - loss: 0.8540 - acc: 0.8044 - val_loss: 0.9496 - val_acc: 0.7334\n",
      "Learning rate:  0.001\n",
      "Epoch 50/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8406 - acc: 0.8111- ETA: 1s - loss: 0.8324 - acc: \n",
      "Epoch 00050: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 29s 461ms/step - loss: 0.8358 - acc: 0.8121 - val_loss: 1.0816 - val_acc: 0.6671\n",
      "Learning rate:  0.001\n",
      "Epoch 51/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7893 - acc: 0.8238\n",
      "Epoch 00051: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 29s 472ms/step - loss: 0.7908 - acc: 0.8226 - val_loss: 0.9035 - val_acc: 0.7638\n",
      "Learning rate:  0.001\n",
      "Epoch 52/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0126 - acc: 0.7357\n",
      "Epoch 00052: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 29s 460ms/step - loss: 1.0117 - acc: 0.7359 - val_loss: 1.0605 - val_acc: 0.6616\n",
      "Learning rate:  0.001\n",
      "Epoch 53/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8024 - acc: 0.8053\n",
      "Epoch 00053: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 29s 474ms/step - loss: 0.7965 - acc: 0.8085 - val_loss: 1.5248 - val_acc: 0.3923\n",
      "Learning rate:  0.001\n",
      "Epoch 54/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7936 - acc: 0.8197- ETA: 4s - lo\n",
      "Epoch 00054: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 30s 483ms/step - loss: 0.7911 - acc: 0.8206 - val_loss: 0.9424 - val_acc: 0.8191\n",
      "Learning rate:  0.001\n",
      "Epoch 55/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7578 - acc: 0.8443\n",
      "Epoch 00055: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.7592 - acc: 0.8448 - val_loss: 1.3903 - val_acc: 0.5525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 56/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.7787\n",
      "Epoch 00056: val_acc did not improve from 0.85359\n",
      "62/62 [==============================] - 29s 464ms/step - loss: 0.8679 - acc: 0.7742 - val_loss: 1.7811 - val_acc: 0.4820\n",
      "Learning rate:  0.001\n",
      "Epoch 57/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7833 - acc: 0.8135\n",
      "Epoch 00057: val_acc improved from 0.85359 to 0.86740, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 29s 472ms/step - loss: 0.7781 - acc: 0.8145 - val_loss: 0.6641 - val_acc: 0.8674\n",
      "Learning rate:  0.001\n",
      "Epoch 58/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7974 - acc: 0.7967\n",
      "Epoch 00058: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 29s 462ms/step - loss: 0.7962 - acc: 0.7960 - val_loss: 0.8751 - val_acc: 0.7182\n",
      "Learning rate:  0.001\n",
      "Epoch 59/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7076 - acc: 0.8402\n",
      "Epoch 00059: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 30s 480ms/step - loss: 0.7083 - acc: 0.8407 - val_loss: 1.5428 - val_acc: 0.4765\n",
      "Learning rate:  0.001\n",
      "Epoch 60/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7876 - acc: 0.7992\n",
      "Epoch 00060: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.7851 - acc: 0.8004 - val_loss: 1.0009 - val_acc: 0.6809\n",
      "Learning rate:  0.001\n",
      "Epoch 61/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7313 - acc: 0.8258\n",
      "Epoch 00061: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 29s 467ms/step - loss: 0.7296 - acc: 0.8266 - val_loss: 0.7637 - val_acc: 0.7997\n",
      "Learning rate:  0.001\n",
      "Epoch 62/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7476 - acc: 0.8316\n",
      "Epoch 00062: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.7436 - acc: 0.8343 - val_loss: 1.0769 - val_acc: 0.6478\n",
      "Learning rate:  0.001\n",
      "Epoch 63/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7153 - acc: 0.8320\n",
      "Epoch 00063: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.7106 - acc: 0.8347 - val_loss: 1.2833 - val_acc: 0.7472\n",
      "Learning rate:  0.001\n",
      "Epoch 64/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7419 - acc: 0.8074\n",
      "Epoch 00064: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 29s 470ms/step - loss: 0.7369 - acc: 0.8105 - val_loss: 1.4466 - val_acc: 0.6036\n",
      "Learning rate:  0.001\n",
      "Epoch 65/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7634 - acc: 0.8115\n",
      "Epoch 00065: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 452ms/step - loss: 0.7630 - acc: 0.8105 - val_loss: 1.0019 - val_acc: 0.6892\n",
      "Learning rate:  0.001\n",
      "Epoch 66/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7433 - acc: 0.8197\n",
      "Epoch 00066: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 452ms/step - loss: 0.7388 - acc: 0.8206 - val_loss: 0.7378 - val_acc: 0.8149\n",
      "Learning rate:  0.001\n",
      "Epoch 67/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7355 - acc: 0.8176\n",
      "Epoch 00067: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 457ms/step - loss: 0.7504 - acc: 0.8165 - val_loss: 0.9378 - val_acc: 0.6547\n",
      "Learning rate:  0.001\n",
      "Epoch 68/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8059 - acc: 0.8008\n",
      "Epoch 00068: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.8109 - acc: 0.7939 - val_loss: 1.1843 - val_acc: 0.6271\n",
      "Learning rate:  0.001\n",
      "Epoch 69/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6900 - acc: 0.8422\n",
      "Epoch 00069: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 29s 472ms/step - loss: 0.6871 - acc: 0.8448 - val_loss: 0.7657 - val_acc: 0.7983\n",
      "Learning rate:  0.001\n",
      "Epoch 70/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7128 - acc: 0.8337- ETA: 5s - l - ETA: 1s - loss: 0.7145 - ac\n",
      "Epoch 00070: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 29s 460ms/step - loss: 0.7166 - acc: 0.8323 - val_loss: 0.6365 - val_acc: 0.8591\n",
      "Learning rate:  0.001\n",
      "Epoch 71/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7105 - acc: 0.8238\n",
      "Epoch 00071: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.7054 - acc: 0.8266 - val_loss: 0.6295 - val_acc: 0.8646\n",
      "Learning rate:  0.001\n",
      "Epoch 72/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7156 - acc: 0.8094\n",
      "Epoch 00072: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 456ms/step - loss: 0.7121 - acc: 0.8105 - val_loss: 0.9799 - val_acc: 0.7307\n",
      "Learning rate:  0.001\n",
      "Epoch 73/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7040 - acc: 0.8340\n",
      "Epoch 00073: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.7003 - acc: 0.8347 - val_loss: 0.8047 - val_acc: 0.7583\n",
      "Learning rate:  0.001\n",
      "Epoch 74/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6991 - acc: 0.8381\n",
      "Epoch 00074: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 29s 466ms/step - loss: 0.6962 - acc: 0.8407 - val_loss: 0.8722 - val_acc: 0.7099\n",
      "Learning rate:  0.001\n",
      "Epoch 75/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6686 - acc: 0.8422\n",
      "Epoch 00075: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.6781 - acc: 0.8407 - val_loss: 1.6449 - val_acc: 0.4448\n",
      "Learning rate:  0.001\n",
      "Epoch 76/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7548 - acc: 0.8074\n",
      "Epoch 00076: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 456ms/step - loss: 0.7553 - acc: 0.8065 - val_loss: 0.8958 - val_acc: 0.7044\n",
      "Learning rate:  0.001\n",
      "Epoch 77/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7100 - acc: 0.8152\n",
      "Epoch 00077: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.7052 - acc: 0.8182 - val_loss: 1.2747 - val_acc: 0.5663\n",
      "Learning rate:  0.001\n",
      "Epoch 78/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7245 - acc: 0.8156\n",
      "Epoch 00078: val_acc did not improve from 0.86740\n",
      "62/62 [==============================] - 28s 456ms/step - loss: 0.7244 - acc: 0.8165 - val_loss: 1.0809 - val_acc: 0.5552\n",
      "Learning rate:  0.001\n",
      "Epoch 79/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6765 - acc: 0.8340\n",
      "Epoch 00079: val_acc improved from 0.86740 to 0.88260, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 30s 479ms/step - loss: 0.6746 - acc: 0.8347 - val_loss: 0.6032 - val_acc: 0.8826\n",
      "Learning rate:  0.001\n",
      "Epoch 80/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6592 - acc: 0.8504\n",
      "Epoch 00080: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 447ms/step - loss: 0.6618 - acc: 0.8468 - val_loss: 0.7605 - val_acc: 0.7914\n",
      "Learning rate:  0.001\n",
      "Epoch 81/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6431 - acc: 0.8299\n",
      "Epoch 00081: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 455ms/step - loss: 0.6457 - acc: 0.8286 - val_loss: 0.6814 - val_acc: 0.8273\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6050 - acc: 0.8460- ETA: 8s - loss: 0.6059\n",
      "Epoch 00082: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.6034 - acc: 0.8465 - val_loss: 0.6586 - val_acc: 0.8287\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6740 - acc: 0.8381\n",
      "Epoch 00083: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 463ms/step - loss: 0.6758 - acc: 0.8367 - val_loss: 0.7246 - val_acc: 0.7873\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/62 [============================>.] - ETA: 0s - loss: 0.6161 - acc: 0.8381\n",
      "Epoch 00084: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 30s 476ms/step - loss: 0.6183 - acc: 0.8367 - val_loss: 0.6571 - val_acc: 0.8260\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6106 - acc: 0.8484\n",
      "Epoch 00085: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.6094 - acc: 0.8488 - val_loss: 0.6706 - val_acc: 0.8191\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.8706\n",
      "Epoch 00086: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 457ms/step - loss: 0.5581 - acc: 0.8687 - val_loss: 0.6745 - val_acc: 0.8108\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.9037\n",
      "Epoch 00087: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 455ms/step - loss: 0.5481 - acc: 0.8952 - val_loss: 0.7299 - val_acc: 0.7804\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6057 - acc: 0.8586\n",
      "Epoch 00088: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 468ms/step - loss: 0.6045 - acc: 0.8589 - val_loss: 0.6336 - val_acc: 0.8412\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8996\n",
      "Epoch 00089: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 462ms/step - loss: 0.4964 - acc: 0.9012 - val_loss: 0.6530 - val_acc: 0.8204\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6204 - acc: 0.8361\n",
      "Epoch 00090: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 456ms/step - loss: 0.6253 - acc: 0.8347 - val_loss: 0.7750 - val_acc: 0.7624\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5835 - acc: 0.8525\n",
      "Epoch 00091: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 455ms/step - loss: 0.5859 - acc: 0.8528 - val_loss: 0.7256 - val_acc: 0.7776\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6412 - acc: 0.8422\n",
      "Epoch 00092: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 456ms/step - loss: 0.6373 - acc: 0.8448 - val_loss: 0.7375 - val_acc: 0.7804\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.8891\n",
      "Epoch 00093: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 469ms/step - loss: 0.5476 - acc: 0.8869 - val_loss: 0.6857 - val_acc: 0.8122\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5445 - acc: 0.8750\n",
      "Epoch 00094: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 452ms/step - loss: 0.5534 - acc: 0.8730 - val_loss: 0.6632 - val_acc: 0.8052\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5669 - acc: 0.8522\n",
      "Epoch 00095: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 465ms/step - loss: 0.5671 - acc: 0.8525 - val_loss: 0.7434 - val_acc: 0.7652\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5408 - acc: 0.8791\n",
      "Epoch 00096: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.5421 - acc: 0.8790 - val_loss: 0.6909 - val_acc: 0.7914\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8750- ETA\n",
      "Epoch 00097: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.5187 - acc: 0.8750 - val_loss: 0.5790 - val_acc: 0.8467\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5338 - acc: 0.8627\n",
      "Epoch 00098: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 470ms/step - loss: 0.5312 - acc: 0.8629 - val_loss: 0.5831 - val_acc: 0.8453\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5677 - acc: 0.8689\n",
      "Epoch 00099: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.5681 - acc: 0.8669 - val_loss: 0.6338 - val_acc: 0.8094\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4918 - acc: 0.9037\n",
      "Epoch 00100: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 464ms/step - loss: 0.4926 - acc: 0.9032 - val_loss: 0.7903 - val_acc: 0.7569\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4909 - acc: 0.8873\n",
      "Epoch 00101: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.4892 - acc: 0.8891 - val_loss: 0.6342 - val_acc: 0.8177\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5171 - acc: 0.8607\n",
      "Epoch 00102: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.5155 - acc: 0.8609 - val_loss: 0.5508 - val_acc: 0.8591\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5631 - acc: 0.8624\n",
      "Epoch 00103: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 465ms/step - loss: 0.5590 - acc: 0.8646 - val_loss: 0.6694 - val_acc: 0.8080\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5018 - acc: 0.8953\n",
      "Epoch 00104: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.5018 - acc: 0.8949 - val_loss: 0.6648 - val_acc: 0.8066\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5784 - acc: 0.8668- ETA: 3s - loss: 0.5\n",
      "Epoch 00105: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 467ms/step - loss: 0.5736 - acc: 0.8690 - val_loss: 0.6519 - val_acc: 0.8066\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5285 - acc: 0.8586\n",
      "Epoch 00106: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 455ms/step - loss: 0.5285 - acc: 0.8589 - val_loss: 0.6346 - val_acc: 0.7928\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5385 - acc: 0.8770\n",
      "Epoch 00107: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.5474 - acc: 0.8730 - val_loss: 0.6262 - val_acc: 0.8218\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8730\n",
      "Epoch 00108: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 468ms/step - loss: 0.5068 - acc: 0.8710 - val_loss: 0.7624 - val_acc: 0.7514\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.8484\n",
      "Epoch 00109: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.5298 - acc: 0.8448 - val_loss: 0.7282 - val_acc: 0.7790\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4841 - acc: 0.9037\n",
      "Epoch 00110: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 0.4869 - acc: 0.9012 - val_loss: 0.7405 - val_acc: 0.7804\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5276 - acc: 0.8830\n",
      "Epoch 00111: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.5239 - acc: 0.8848 - val_loss: 0.8516 - val_acc: 0.7155\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8770\n",
      "Epoch 00112: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.5120 - acc: 0.8790 - val_loss: 0.6522 - val_acc: 0.8066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0001\n",
      "Epoch 113/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4648 - acc: 0.9037\n",
      "Epoch 00113: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 470ms/step - loss: 0.4690 - acc: 0.8992 - val_loss: 0.7434 - val_acc: 0.7638\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4709 - acc: 0.8791\n",
      "Epoch 00114: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 457ms/step - loss: 0.4731 - acc: 0.8770 - val_loss: 0.6024 - val_acc: 0.8315\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.8627\n",
      "Epoch 00115: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 30s 477ms/step - loss: 0.5452 - acc: 0.8609 - val_loss: 0.6519 - val_acc: 0.7901\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4853 - acc: 0.8811\n",
      "Epoch 00116: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.4832 - acc: 0.8810 - val_loss: 0.7465 - val_acc: 0.7528\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5593 - acc: 0.8624\n",
      "Epoch 00117: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 464ms/step - loss: 0.5574 - acc: 0.8626 - val_loss: 0.8650 - val_acc: 0.7099\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5262 - acc: 0.8686\n",
      "Epoch 00118: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 470ms/step - loss: 0.5270 - acc: 0.8667 - val_loss: 0.6753 - val_acc: 0.7818\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8914\n",
      "Epoch 00119: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.4663 - acc: 0.8911 - val_loss: 0.5737 - val_acc: 0.8370\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.8709- ETA: 5\n",
      "Epoch 00120: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 463ms/step - loss: 0.5064 - acc: 0.8730 - val_loss: 0.5991 - val_acc: 0.8356\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8668\n",
      "Epoch 00121: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.5346 - acc: 0.8669 - val_loss: 0.5438 - val_acc: 0.8522\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4813 - acc: 0.9016\n",
      "Epoch 00122: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.4773 - acc: 0.9032 - val_loss: 0.5798 - val_acc: 0.8370\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8893\n",
      "Epoch 00123: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 475ms/step - loss: 0.4739 - acc: 0.8891 - val_loss: 0.5982 - val_acc: 0.8370\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4540 - acc: 0.9180\n",
      "Epoch 00124: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.4533 - acc: 0.9194 - val_loss: 0.6372 - val_acc: 0.8149\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8852\n",
      "Epoch 00125: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 0.4632 - acc: 0.8831 - val_loss: 0.5927 - val_acc: 0.8287\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4672 - acc: 0.8873\n",
      "Epoch 00126: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 460ms/step - loss: 0.4651 - acc: 0.8871 - val_loss: 0.6081 - val_acc: 0.8356\n",
      "Learning rate:  1e-05\n",
      "Epoch 127/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.9014\n",
      "Epoch 00127: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 27s 443ms/step - loss: 0.4663 - acc: 0.9030 - val_loss: 0.5852 - val_acc: 0.8467\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5590 - acc: 0.8607\n",
      "Epoch 00128: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 30s 483ms/step - loss: 0.5560 - acc: 0.8609 - val_loss: 0.5915 - val_acc: 0.8398\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4873 - acc: 0.8975- ETA: 9s - loss: 0.402\n",
      "Epoch 00129: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.4882 - acc: 0.8952 - val_loss: 0.5876 - val_acc: 0.8370\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4518 - acc: 0.8850  ETA: 9s - loss: 0.536 -\n",
      "Epoch 00130: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 468ms/step - loss: 0.4554 - acc: 0.8848 - val_loss: 0.5696 - val_acc: 0.8577\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.8811\n",
      "Epoch 00131: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 462ms/step - loss: 0.4740 - acc: 0.8831 - val_loss: 0.6081 - val_acc: 0.8218\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4376 - acc: 0.9119\n",
      "Epoch 00132: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 466ms/step - loss: 0.4357 - acc: 0.9133 - val_loss: 0.5941 - val_acc: 0.8343\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4666 - acc: 0.8871\n",
      "Epoch 00133: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 469ms/step - loss: 0.4700 - acc: 0.8848 - val_loss: 0.5905 - val_acc: 0.8384\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4929 - acc: 0.8914\n",
      "Epoch 00134: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 452ms/step - loss: 0.4893 - acc: 0.8931 - val_loss: 0.5847 - val_acc: 0.8425\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4828 - acc: 0.8955\n",
      "Epoch 00135: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.4836 - acc: 0.8952 - val_loss: 0.5804 - val_acc: 0.8301\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5134 - acc: 0.8791\n",
      "Epoch 00136: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.5123 - acc: 0.8810 - val_loss: 0.6081 - val_acc: 0.8246\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.9180\n",
      "Epoch 00137: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 471ms/step - loss: 0.4400 - acc: 0.9153 - val_loss: 0.5796 - val_acc: 0.8329\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5422 - acc: 0.8709\n",
      "Epoch 00138: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.5383 - acc: 0.8710 - val_loss: 0.6009 - val_acc: 0.8287\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4613 - acc: 0.9035\n",
      "Epoch 00139: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.4598 - acc: 0.9030 - val_loss: 0.5922 - val_acc: 0.8260\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4734 - acc: 0.9078\n",
      "Epoch 00140: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 455ms/step - loss: 0.4696 - acc: 0.9093 - val_loss: 0.5817 - val_acc: 0.8398\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4563 - acc: 0.8832\n",
      "Epoch 00141: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 0.4548 - acc: 0.8831 - val_loss: 0.6092 - val_acc: 0.8287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-06\n",
      "Epoch 142/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4415 - acc: 0.9016\n",
      "Epoch 00142: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 466ms/step - loss: 0.4426 - acc: 0.9012 - val_loss: 0.6066 - val_acc: 0.8301\n",
      "Learning rate:  1e-06\n",
      "Epoch 143/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4388 - acc: 0.8953- ETA: 6s - loss: 0.4537 - acc: 0.90\n",
      "Epoch 00143: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.4384 - acc: 0.8949 - val_loss: 0.6009 - val_acc: 0.8177\n",
      "Learning rate:  1e-06\n",
      "Epoch 144/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4867 - acc: 0.8893\n",
      "Epoch 00144: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.4853 - acc: 0.8891 - val_loss: 0.6092 - val_acc: 0.8301\n",
      "Learning rate:  1e-06\n",
      "Epoch 145/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4985 - acc: 0.8852\n",
      "Epoch 00145: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 27s 443ms/step - loss: 0.4989 - acc: 0.8851 - val_loss: 0.5726 - val_acc: 0.8439\n",
      "Learning rate:  1e-06\n",
      "Epoch 146/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4776 - acc: 0.8852\n",
      "Epoch 00146: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 0.4796 - acc: 0.8851 - val_loss: 0.5687 - val_acc: 0.8453\n",
      "Learning rate:  1e-06\n",
      "Epoch 147/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.9057\n",
      "Epoch 00147: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 470ms/step - loss: 0.4687 - acc: 0.9073 - val_loss: 0.5681 - val_acc: 0.8398\n",
      "Learning rate:  1e-06\n",
      "Epoch 148/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4419 - acc: 0.9138\n",
      "Epoch 00148: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 469ms/step - loss: 0.4493 - acc: 0.9071 - val_loss: 0.5747 - val_acc: 0.8356\n",
      "Learning rate:  1e-06\n",
      "Epoch 149/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4475 - acc: 0.8893\n",
      "Epoch 00149: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 445ms/step - loss: 0.4463 - acc: 0.8891 - val_loss: 0.6011 - val_acc: 0.8370\n",
      "Learning rate:  1e-06\n",
      "Epoch 150/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4560 - acc: 0.8996\n",
      "Epoch 00150: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.4548 - acc: 0.8992 - val_loss: 0.6195 - val_acc: 0.8163\n",
      "Learning rate:  1e-06\n",
      "Epoch 151/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5342 - acc: 0.8607\n",
      "Epoch 00151: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.5328 - acc: 0.8609 - val_loss: 0.6166 - val_acc: 0.8177\n",
      "Learning rate:  1e-06\n",
      "Epoch 152/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4529 - acc: 0.9016\n",
      "Epoch 00152: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 30s 487ms/step - loss: 0.4528 - acc: 0.9012 - val_loss: 0.5867 - val_acc: 0.8370\n",
      "Learning rate:  1e-06\n",
      "Epoch 153/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.8955\n",
      "Epoch 00153: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 452ms/step - loss: 0.4613 - acc: 0.8952 - val_loss: 0.5946 - val_acc: 0.8163\n",
      "Learning rate:  1e-06\n",
      "Epoch 154/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4744 - acc: 0.8871\n",
      "Epoch 00154: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.4763 - acc: 0.8869 - val_loss: 0.5902 - val_acc: 0.8384\n",
      "Learning rate:  1e-06\n",
      "Epoch 155/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4613 - acc: 0.8750\n",
      "Epoch 00155: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.4627 - acc: 0.8750 - val_loss: 0.5907 - val_acc: 0.8260\n",
      "Learning rate:  1e-06\n",
      "Epoch 156/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5011 - acc: 0.8730\n",
      "Epoch 00156: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 456ms/step - loss: 0.5022 - acc: 0.8710 - val_loss: 0.6183 - val_acc: 0.8232\n",
      "Learning rate:  1e-06\n",
      "Epoch 157/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4528 - acc: 0.8934\n",
      "Epoch 00157: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 468ms/step - loss: 0.4489 - acc: 0.8952 - val_loss: 0.6117 - val_acc: 0.8232\n",
      "Learning rate:  1e-06\n",
      "Epoch 158/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.9037\n",
      "Epoch 00158: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 457ms/step - loss: 0.4738 - acc: 0.9052 - val_loss: 0.5790 - val_acc: 0.8481\n",
      "Learning rate:  1e-06\n",
      "Epoch 159/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5013 - acc: 0.8689\n",
      "Epoch 00159: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 444ms/step - loss: 0.5009 - acc: 0.8669 - val_loss: 0.5868 - val_acc: 0.8398\n",
      "Learning rate:  1e-06\n",
      "Epoch 160/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.9098\n",
      "Epoch 00160: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 447ms/step - loss: 0.4511 - acc: 0.9113 - val_loss: 0.5808 - val_acc: 0.8356\n",
      "Learning rate:  1e-06\n",
      "Epoch 161/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4099 - acc: 0.9199\n",
      "Epoch 00161: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.4165 - acc: 0.9152 - val_loss: 0.5640 - val_acc: 0.8481\n",
      "Learning rate:  5e-07\n",
      "Epoch 162/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4729 - acc: 0.8873\n",
      "Epoch 00162: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 471ms/step - loss: 0.4718 - acc: 0.8871 - val_loss: 0.6049 - val_acc: 0.8329\n",
      "Learning rate:  5e-07\n",
      "Epoch 163/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4752 - acc: 0.8996\n",
      "Epoch 00163: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 446ms/step - loss: 0.4782 - acc: 0.8992 - val_loss: 0.5971 - val_acc: 0.8273\n",
      "Learning rate:  5e-07\n",
      "Epoch 164/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4551 - acc: 0.8996\n",
      "Epoch 00164: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.4537 - acc: 0.9012 - val_loss: 0.5799 - val_acc: 0.8398\n",
      "Learning rate:  5e-07\n",
      "Epoch 165/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4321 - acc: 0.9055\n",
      "Epoch 00165: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.4359 - acc: 0.9030 - val_loss: 0.5831 - val_acc: 0.8356\n",
      "Learning rate:  5e-07\n",
      "Epoch 166/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4261 - acc: 0.9119\n",
      "Epoch 00166: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 457ms/step - loss: 0.4368 - acc: 0.9093 - val_loss: 0.5842 - val_acc: 0.8356\n",
      "Learning rate:  5e-07\n",
      "Epoch 167/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5291 - acc: 0.8525\n",
      "Epoch 00167: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 472ms/step - loss: 0.5288 - acc: 0.8528 - val_loss: 0.5686 - val_acc: 0.8370\n",
      "Learning rate:  5e-07\n",
      "Epoch 168/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4637 - acc: 0.8791\n",
      "Epoch 00168: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 0.4652 - acc: 0.8790 - val_loss: 0.6150 - val_acc: 0.8204\n",
      "Learning rate:  5e-07\n",
      "Epoch 169/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.8873\n",
      "Epoch 00169: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 461ms/step - loss: 0.4735 - acc: 0.8891 - val_loss: 0.5879 - val_acc: 0.8370\n",
      "Learning rate:  5e-07\n",
      "Epoch 170/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4749 - acc: 0.8934\n",
      "Epoch 00170: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.4768 - acc: 0.8931 - val_loss: 0.5715 - val_acc: 0.8384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  5e-07\n",
      "Epoch 171/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4540 - acc: 0.8932\n",
      "Epoch 00171: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 27s 438ms/step - loss: 0.4515 - acc: 0.8949 - val_loss: 0.5831 - val_acc: 0.8439\n",
      "Learning rate:  5e-07\n",
      "Epoch 172/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4948 - acc: 0.8873\n",
      "Epoch 00172: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 30s 491ms/step - loss: 0.4996 - acc: 0.8831 - val_loss: 0.5879 - val_acc: 0.8370\n",
      "Learning rate:  5e-07\n",
      "Epoch 173/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4455 - acc: 0.9057\n",
      "Epoch 00173: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 457ms/step - loss: 0.4450 - acc: 0.9032 - val_loss: 0.5749 - val_acc: 0.8315\n",
      "Learning rate:  5e-07\n",
      "Epoch 174/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4777 - acc: 0.8873\n",
      "Epoch 00174: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 461ms/step - loss: 0.4794 - acc: 0.8851 - val_loss: 0.5860 - val_acc: 0.8356\n",
      "Learning rate:  5e-07\n",
      "Epoch 175/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4119 - acc: 0.9179\n",
      "Epoch 00175: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.4096 - acc: 0.9192 - val_loss: 0.5949 - val_acc: 0.8301\n",
      "Learning rate:  5e-07\n",
      "Epoch 176/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4333 - acc: 0.8893\n",
      "Epoch 00176: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 30s 481ms/step - loss: 0.4308 - acc: 0.8911 - val_loss: 0.5878 - val_acc: 0.8329\n",
      "Learning rate:  5e-07\n",
      "Epoch 177/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4291 - acc: 0.9097- ETA: 4s - l\n",
      "Epoch 00177: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 447ms/step - loss: 0.4294 - acc: 0.9091 - val_loss: 0.5611 - val_acc: 0.8439\n",
      "Learning rate:  5e-07\n",
      "Epoch 178/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8934\n",
      "Epoch 00178: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 461ms/step - loss: 0.4591 - acc: 0.8952 - val_loss: 0.5784 - val_acc: 0.8398\n",
      "Learning rate:  5e-07\n",
      "Epoch 179/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8770- ETA: 2s - loss: 0.4732 -\n",
      "Epoch 00179: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 447ms/step - loss: 0.4821 - acc: 0.8790 - val_loss: 0.5701 - val_acc: 0.8398\n",
      "Learning rate:  5e-07\n",
      "Epoch 180/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.9221\n",
      "Epoch 00180: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.4421 - acc: 0.9214 - val_loss: 0.5970 - val_acc: 0.8301\n",
      "Learning rate:  5e-07\n",
      "Epoch 181/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5012 - acc: 0.8852- ETA: 5s\n",
      "Epoch 00181: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 466ms/step - loss: 0.4992 - acc: 0.8851 - val_loss: 0.5831 - val_acc: 0.8398\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4322 - acc: 0.9119\n",
      "Epoch 00182: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 455ms/step - loss: 0.4307 - acc: 0.9133 - val_loss: 0.5792 - val_acc: 0.8356\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5217 - acc: 0.8665\n",
      "Epoch 00183: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 464ms/step - loss: 0.5182 - acc: 0.8667 - val_loss: 0.5938 - val_acc: 0.8412\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4568 - acc: 0.8934\n",
      "Epoch 00184: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.4561 - acc: 0.8952 - val_loss: 0.5990 - val_acc: 0.8370\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4634 - acc: 0.8996\n",
      "Epoch 00185: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 0.4613 - acc: 0.9012 - val_loss: 0.5882 - val_acc: 0.8370\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4535 - acc: 0.9078\n",
      "Epoch 00186: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 466ms/step - loss: 0.4554 - acc: 0.9073 - val_loss: 0.5871 - val_acc: 0.8287\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4992 - acc: 0.8934\n",
      "Epoch 00187: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 459ms/step - loss: 0.4954 - acc: 0.8952 - val_loss: 0.5790 - val_acc: 0.8315\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8873- ETA: 2s - loss: 0.4633\n",
      "Epoch 00188: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 461ms/step - loss: 0.4682 - acc: 0.8871 - val_loss: 0.5907 - val_acc: 0.8287\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4597 - acc: 0.8994\n",
      "Epoch 00189: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.4578 - acc: 0.8990 - val_loss: 0.5956 - val_acc: 0.8204\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4405 - acc: 0.9016\n",
      "Epoch 00190: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.4372 - acc: 0.9032 - val_loss: 0.5807 - val_acc: 0.8453\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.9016\n",
      "Epoch 00191: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 472ms/step - loss: 0.4473 - acc: 0.9012 - val_loss: 0.5995 - val_acc: 0.8218\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8955\n",
      "Epoch 00192: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 446ms/step - loss: 0.4819 - acc: 0.8952 - val_loss: 0.5922 - val_acc: 0.8398\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4761 - acc: 0.8852\n",
      "Epoch 00193: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 466ms/step - loss: 0.4761 - acc: 0.8851 - val_loss: 0.5883 - val_acc: 0.8343\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.9098\n",
      "Epoch 00194: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.4464 - acc: 0.9113 - val_loss: 0.5976 - val_acc: 0.8232\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4786 - acc: 0.8955\n",
      "Epoch 00195: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.4808 - acc: 0.8952 - val_loss: 0.5958 - val_acc: 0.8315\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.8994\n",
      "Epoch 00196: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 29s 467ms/step - loss: 0.4278 - acc: 0.9010 - val_loss: 0.5865 - val_acc: 0.8343\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4517 - acc: 0.9076\n",
      "Epoch 00197: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.4513 - acc: 0.9071 - val_loss: 0.6035 - val_acc: 0.8315\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4586 - acc: 0.8934\n",
      "Epoch 00198: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.4559 - acc: 0.8952 - val_loss: 0.5875 - val_acc: 0.8301\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4442 - acc: 0.9119- ETA: 1s - loss: 0.4341 - ac\n",
      "Epoch 00199: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.4408 - acc: 0.9133 - val_loss: 0.6001 - val_acc: 0.8329\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.8914\n",
      "Epoch 00200: val_acc did not improve from 0.88260\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.4630 - acc: 0.8911 - val_loss: 0.5780 - val_acc: 0.8439\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "num_train_images = 1000\n",
    "\n",
    "def lrSchedule(epoch):\n",
    "    lr = 1e-3\n",
    "\n",
    "    if epoch > 160:\n",
    "        lr *= 0.5e-3\n",
    "\n",
    "    elif epoch > 140:\n",
    "        lr *= 1e-3\n",
    "\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "\n",
    "    print('Learning rate: ', lr)\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "LRScheduler = LearningRateScheduler(lrSchedule)\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ca2\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "callbacks_list = [checkpoint, LRScheduler]\n",
    "\n",
    "history = model.fit_generator(train_generator, \n",
    "                              validation_data=test_generator,\n",
    "                              epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300, 300, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_conv (Conv2D)              (None, 300, 300, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_bn (BatchNormalization)    (None, 300, 300, 16) 64          Inpt_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_relu (Activation)          (None, 300, 300, 16) 0           Inpt_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Inpt_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_add (Add)             (None, 300, 300, 16) 0           Inpt_relu[0][0]                  \n",
      "                                                                 Stg1_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_add (Add)             (None, 300, 300, 16) 0           Stg1_Blk1_relu[0][0]             \n",
      "                                                                 Stg1_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_add (Add)             (None, 300, 300, 16) 0           Stg1_Blk2_relu[0][0]             \n",
      "                                                                 Stg1_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 300, 300, 16) 0           Stg1_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_conv (Conv2D)    (None, 150, 150, 32) 4640        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_lin_conv (Conv2D)     (None, 150, 150, 32) 544         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg2_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk1_relu[0][0]             \n",
      "                                                                 Stg2_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk2_relu[0][0]             \n",
      "                                                                 Stg2_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150, 150, 32) 0           Stg2_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_conv (Conv2D)    (None, 75, 75, 64)   18496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_lin_conv (Conv2D)     (None, 75, 75, 64)   2112        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg3_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk1_relu[0][0]             \n",
      "                                                                 Stg3_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk2_relu[0][0]             \n",
      "                                                                 Stg3_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 75, 75, 64)   0           Stg3_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_conv (Conv2D)    (None, 38, 38, 128)  73856       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_lin_conv (Conv2D)     (None, 38, 38, 128)  8320        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg4_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk1_relu[0][0]             \n",
      "                                                                 Stg4_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk2_relu[0][0]             \n",
      "                                                                 Stg4_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "AvgPool (AveragePooling2D)      (None, 4, 4, 128)    0           Stg4_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           AvgPool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            6147        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,103,107\n",
      "Trainable params: 1,100,195\n",
      "Non-trainable params: 2,912\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "\n",
    "optmz = optimizers.Adam(lr=0.001)\n",
    "\n",
    "def resLyr(inputs,\n",
    "           numFilters=16,\n",
    "           kernelSz=3,\n",
    "           strides=1,\n",
    "           activation='relu',\n",
    "           batchNorm=True,\n",
    "           convFirst=True,\n",
    "           lyrName=None):\n",
    "  \n",
    "    convLyr = Conv2D(numFilters,\n",
    "                     kernel_size=kernelSz,\n",
    "                     strides=strides,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4),\n",
    "                     name=lyrName + '_conv' if lyrName else None)\n",
    "    x = inputs\n",
    "    if convFirst:\n",
    "        x = convLyr(x)\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "    else:\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "        x = convLyr(x)\n",
    "  \n",
    "    return x\n",
    "\n",
    "\n",
    "def resBlkV1(inputs,\n",
    "             numFilters=16,\n",
    "             numBlocks=3,\n",
    "             downsampleOnFirst=True,\n",
    "             names=None):\n",
    "  \n",
    "    x = inputs\n",
    "    for run in range(0, numBlocks):\n",
    "        strides = 1\n",
    "        blkStr = str(run + 1)\n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            strides = 2\n",
    "            \n",
    "        y = resLyr(inputs=x, numFilters=numFilters, strides=strides, lyrName=names+'_Blk'+blkStr+'_Res1' if names else None)\n",
    "        y = resLyr(inputs=y, numFilters=numFilters, activation=None, lyrName=names+'_Blk'+blkStr+'_Res2' if names else None) \n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            x = resLyr(inputs=x, numFilters=numFilters, kernelSz=1, \n",
    "                       strides=strides, activation=None, batchNorm=False, \n",
    "                       lyrName=names+'_Blk'+blkStr+'_lin' if names else None)\n",
    "\n",
    "        x = add([x, y], name=names+'_Blk'+blkStr+'_add' if names else None) \n",
    "\n",
    "        x = Activation('relu',  name=names+'_Blk'+blkStr+'_relu' if names else None)(x)   \n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def createResNetV1(inputShape=(32, 32, 3),\n",
    "                   numClasses=3):\n",
    "  \n",
    "    inputs = Input(shape=inputShape)\n",
    "    v = resLyr(inputs,\n",
    "               lyrName='Inpt')\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=16,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=False,\n",
    "                 names='Stg1')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=32,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg2')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=64,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg3')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=128,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg4')    \n",
    "    \n",
    "    v = AveragePooling2D(pool_size=8,\n",
    "                         name='AvgPool')(v)\n",
    "    v = Flatten()(v)\n",
    "    outputs = Dense(numClasses,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(v)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optmz,\n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = createResNetV1(inputShape=(HEIGHT, WIDTH, 3))  # This is meant for training\n",
    "modelGo = createResNetV1(inputShape=(HEIGHT, WIDTH, 3))  # This is used for final testing\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 2.9252 - acc: 0.4743\n",
      "Epoch 00001: val_acc improved from -inf to 0.37155, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 104s 2s/step - loss: 2.9251 - acc: 0.4727 - val_loss: 9.5510 - val_acc: 0.3715\n",
      "Learning rate:  0.001\n",
      "Epoch 2/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.5240 - acc: 0.5943\n",
      "Epoch 00002: val_acc improved from 0.37155 to 0.55663, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 41s 667ms/step - loss: 1.5301 - acc: 0.5927 - val_loss: 3.6536 - val_acc: 0.5566\n",
      "Learning rate:  0.001\n",
      "Epoch 3/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.6329 - acc: 0.5471\n",
      "Epoch 00003: val_acc did not improve from 0.55663\n",
      "62/62 [==============================] - 43s 689ms/step - loss: 1.6308 - acc: 0.5504 - val_loss: 1.9694 - val_acc: 0.5483\n",
      "Learning rate:  0.001\n",
      "Epoch 4/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2889 - acc: 0.6148\n",
      "Epoch 00004: val_acc improved from 0.55663 to 0.72376, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 43s 698ms/step - loss: 1.2772 - acc: 0.6210 - val_loss: 1.2688 - val_acc: 0.7238\n",
      "Learning rate:  0.001\n",
      "Epoch 5/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2474 - acc: 0.6557\n",
      "Epoch 00005: val_acc did not improve from 0.72376\n",
      "62/62 [==============================] - 39s 632ms/step - loss: 1.2387 - acc: 0.6573 - val_loss: 1.4104 - val_acc: 0.7099\n",
      "Learning rate:  0.001\n",
      "Epoch 6/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2058 - acc: 0.6496\n",
      "Epoch 00006: val_acc improved from 0.72376 to 0.75829, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 417ms/step - loss: 1.2041 - acc: 0.6492 - val_loss: 0.9965 - val_acc: 0.7583\n",
      "Learning rate:  0.001\n",
      "Epoch 7/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1405 - acc: 0.6967\n",
      "Epoch 00007: val_acc did not improve from 0.75829\n",
      "62/62 [==============================] - 26s 412ms/step - loss: 1.1384 - acc: 0.6956 - val_loss: 1.2440 - val_acc: 0.6727\n",
      "Learning rate:  0.001\n",
      "Epoch 8/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1694 - acc: 0.6680\n",
      "Epoch 00008: val_acc did not improve from 0.75829\n",
      "62/62 [==============================] - 26s 415ms/step - loss: 1.1653 - acc: 0.6694 - val_loss: 1.2396 - val_acc: 0.6699\n",
      "Learning rate:  0.001\n",
      "Epoch 9/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2188 - acc: 0.6352\n",
      "Epoch 00009: val_acc improved from 0.75829 to 0.79558, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 1.2100 - acc: 0.6391 - val_loss: 0.8548 - val_acc: 0.7956\n",
      "Learning rate:  0.001\n",
      "Epoch 10/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0833 - acc: 0.7125\n",
      "Epoch 00010: val_acc did not improve from 0.79558\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 1.0775 - acc: 0.7131 - val_loss: 1.4135 - val_acc: 0.6367\n",
      "Learning rate:  0.001\n",
      "Epoch 11/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0441 - acc: 0.7187\n",
      "Epoch 00011: val_acc did not improve from 0.79558\n",
      "62/62 [==============================] - 26s 412ms/step - loss: 1.0637 - acc: 0.7192 - val_loss: 0.9065 - val_acc: 0.7762\n",
      "Learning rate:  0.001\n",
      "Epoch 12/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1143 - acc: 0.6824\n",
      "Epoch 00012: val_acc improved from 0.79558 to 0.80801, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 413ms/step - loss: 1.1154 - acc: 0.6815 - val_loss: 0.9191 - val_acc: 0.8080\n",
      "Learning rate:  0.001\n",
      "Epoch 13/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0883 - acc: 0.7111\n",
      "Epoch 00013: val_acc did not improve from 0.80801\n",
      "62/62 [==============================] - 26s 418ms/step - loss: 1.0801 - acc: 0.7157 - val_loss: 0.8551 - val_acc: 0.7638\n",
      "Learning rate:  0.001\n",
      "Epoch 14/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0284 - acc: 0.7029\n",
      "Epoch 00014: val_acc did not improve from 0.80801\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 1.0332 - acc: 0.7016 - val_loss: 0.8318 - val_acc: 0.7997\n",
      "Learning rate:  0.001\n",
      "Epoch 15/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9083 - acc: 0.7377\n",
      "Epoch 00015: val_acc did not improve from 0.80801\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.9062 - acc: 0.7379 - val_loss: 1.0891 - val_acc: 0.7390\n",
      "Learning rate:  0.001\n",
      "Epoch 16/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9937 - acc: 0.7234\n",
      "Epoch 00016: val_acc did not improve from 0.80801\n",
      "62/62 [==============================] - 26s 416ms/step - loss: 0.9987 - acc: 0.7238 - val_loss: 0.9039 - val_acc: 0.7997\n",
      "Learning rate:  0.001\n",
      "Epoch 17/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9969 - acc: 0.7357\n",
      "Epoch 00017: val_acc did not improve from 0.80801\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.9920 - acc: 0.7379 - val_loss: 1.2805 - val_acc: 0.6354\n",
      "Learning rate:  0.001\n",
      "Epoch 18/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9551 - acc: 0.7254\n",
      "Epoch 00018: val_acc did not improve from 0.80801\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.9544 - acc: 0.7238 - val_loss: 1.1260 - val_acc: 0.6285\n",
      "Learning rate:  0.001\n",
      "Epoch 19/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8983 - acc: 0.7618\n",
      "Epoch 00019: val_acc improved from 0.80801 to 0.83011, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 27s 438ms/step - loss: 0.8911 - acc: 0.7657 - val_loss: 0.7068 - val_acc: 0.8301\n",
      "Learning rate:  0.001\n",
      "Epoch 20/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9095 - acc: 0.7398\n",
      "Epoch 00020: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.9163 - acc: 0.7399 - val_loss: 0.8139 - val_acc: 0.8039\n",
      "Learning rate:  0.001\n",
      "Epoch 21/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9832 - acc: 0.7275\n",
      "Epoch 00021: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.9763 - acc: 0.7298 - val_loss: 1.2626 - val_acc: 0.5912\n",
      "Learning rate:  0.001\n",
      "Epoch 22/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8432 - acc: 0.7807\n",
      "Epoch 00022: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 26s 417ms/step - loss: 0.8496 - acc: 0.7762 - val_loss: 1.1132 - val_acc: 0.6298\n",
      "Learning rate:  0.001\n",
      "Epoch 23/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8222 - acc: 0.7602\n",
      "Epoch 00023: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 26s 415ms/step - loss: 0.8204 - acc: 0.7601 - val_loss: 0.8733 - val_acc: 0.7403\n",
      "Learning rate:  0.001\n",
      "Epoch 24/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8770 - acc: 0.7541\n",
      "Epoch 00024: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 27s 433ms/step - loss: 0.8775 - acc: 0.7520 - val_loss: 0.7183 - val_acc: 0.8273\n",
      "Learning rate:  0.001\n",
      "Epoch 25/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9863 - acc: 0.7413\n",
      "Epoch 00025: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 0.9894 - acc: 0.7374 - val_loss: 0.8863 - val_acc: 0.7500\n",
      "Learning rate:  0.001\n",
      "Epoch 26/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8586 - acc: 0.7725\n",
      "Epoch 00026: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.8521 - acc: 0.7762 - val_loss: 1.2025 - val_acc: 0.6381\n",
      "Learning rate:  0.001\n",
      "Epoch 27/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8146 - acc: 0.7766\n",
      "Epoch 00027: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.8155 - acc: 0.7762 - val_loss: 0.9414 - val_acc: 0.6920\n",
      "Learning rate:  0.001\n",
      "Epoch 28/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8270 - acc: 0.7680\n",
      "Epoch 00028: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.8243 - acc: 0.7677 - val_loss: 0.8126 - val_acc: 0.7956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 29/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8760 - acc: 0.7520\n",
      "Epoch 00029: val_acc did not improve from 0.83011\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.8733 - acc: 0.7520 - val_loss: 0.7572 - val_acc: 0.7970\n",
      "Learning rate:  0.001\n",
      "Epoch 30/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8546 - acc: 0.7520\n",
      "Epoch 00030: val_acc improved from 0.83011 to 0.83149, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.8549 - acc: 0.7500 - val_loss: 0.7323 - val_acc: 0.8315\n",
      "Learning rate:  0.001\n",
      "Epoch 31/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8111 - acc: 0.7721\n",
      "Epoch 00031: val_acc did not improve from 0.83149\n",
      "62/62 [==============================] - 26s 418ms/step - loss: 0.8053 - acc: 0.7758 - val_loss: 0.8735 - val_acc: 0.7265\n",
      "Learning rate:  0.001\n",
      "Epoch 32/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7881 - acc: 0.7869\n",
      "Epoch 00032: val_acc did not improve from 0.83149\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.7858 - acc: 0.7883 - val_loss: 0.9437 - val_acc: 0.7182\n",
      "Learning rate:  0.001\n",
      "Epoch 33/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8771 - acc: 0.7705\n",
      "Epoch 00033: val_acc did not improve from 0.83149\n",
      "62/62 [==============================] - 27s 441ms/step - loss: 0.8707 - acc: 0.7722 - val_loss: 0.9916 - val_acc: 0.6809\n",
      "Learning rate:  0.001\n",
      "Epoch 34/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8018 - acc: 0.7705\n",
      "Epoch 00034: val_acc did not improve from 0.83149\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.8002 - acc: 0.7681 - val_loss: 0.6742 - val_acc: 0.8135\n",
      "Learning rate:  0.001\n",
      "Epoch 35/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7833 - acc: 0.7848\n",
      "Epoch 00035: val_acc did not improve from 0.83149\n",
      "62/62 [==============================] - 28s 447ms/step - loss: 0.7769 - acc: 0.7883 - val_loss: 0.7909 - val_acc: 0.7749\n",
      "Learning rate:  0.001\n",
      "Epoch 36/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7525 - acc: 0.7782\n",
      "Epoch 00036: val_acc did not improve from 0.83149\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.7600 - acc: 0.7758 - val_loss: 0.9922 - val_acc: 0.7265\n",
      "Learning rate:  0.001\n",
      "Epoch 37/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8086 - acc: 0.7848\n",
      "Epoch 00037: val_acc did not improve from 0.83149\n",
      "62/62 [==============================] - 26s 419ms/step - loss: 0.8082 - acc: 0.7843 - val_loss: 0.7396 - val_acc: 0.8039\n",
      "Learning rate:  0.001\n",
      "Epoch 38/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7779 - acc: 0.7787\n",
      "Epoch 00038: val_acc improved from 0.83149 to 0.86050, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.7748 - acc: 0.7802 - val_loss: 0.6567 - val_acc: 0.8605\n",
      "Learning rate:  0.001\n",
      "Epoch 39/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7821 - acc: 0.7766\n",
      "Epoch 00039: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 414ms/step - loss: 0.7883 - acc: 0.7742 - val_loss: 0.9815 - val_acc: 0.6851\n",
      "Learning rate:  0.001\n",
      "Epoch 40/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6447 - acc: 0.8320\n",
      "Epoch 00040: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.6411 - acc: 0.8327 - val_loss: 0.7022 - val_acc: 0.7859\n",
      "Learning rate:  0.001\n",
      "Epoch 41/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7657 - acc: 0.7988\n",
      "Epoch 00041: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.7588 - acc: 0.8020 - val_loss: 1.1819 - val_acc: 0.5829\n",
      "Learning rate:  0.001\n",
      "Epoch 42/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7442 - acc: 0.7992\n",
      "Epoch 00042: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.7393 - acc: 0.8004 - val_loss: 0.6447 - val_acc: 0.8384\n",
      "Learning rate:  0.001\n",
      "Epoch 43/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7342 - acc: 0.7766\n",
      "Epoch 00043: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 0.7309 - acc: 0.7782 - val_loss: 0.7395 - val_acc: 0.7776\n",
      "Learning rate:  0.001\n",
      "Epoch 44/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7480 - acc: 0.7869\n",
      "Epoch 00044: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 0.7429 - acc: 0.7883 - val_loss: 1.0398 - val_acc: 0.7403\n",
      "Learning rate:  0.001\n",
      "Epoch 45/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6701 - acc: 0.8193\n",
      "Epoch 00045: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.6734 - acc: 0.8182 - val_loss: 0.8173 - val_acc: 0.7086\n",
      "Learning rate:  0.001\n",
      "Epoch 46/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6898 - acc: 0.8033\n",
      "Epoch 00046: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.6859 - acc: 0.8065 - val_loss: 0.5878 - val_acc: 0.8536\n",
      "Learning rate:  0.001\n",
      "Epoch 47/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6549 - acc: 0.8135\n",
      "Epoch 00047: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.6597 - acc: 0.8125 - val_loss: 0.7144 - val_acc: 0.7873\n",
      "Learning rate:  0.001\n",
      "Epoch 48/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6138 - acc: 0.8340\n",
      "Epoch 00048: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 418ms/step - loss: 0.6097 - acc: 0.8367 - val_loss: 0.5668 - val_acc: 0.8564\n",
      "Learning rate:  0.001\n",
      "Epoch 49/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6652 - acc: 0.8074\n",
      "Epoch 00049: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.6645 - acc: 0.8085 - val_loss: 0.9605 - val_acc: 0.7666\n",
      "Learning rate:  0.001\n",
      "Epoch 50/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7609 - acc: 0.7992\n",
      "Epoch 00050: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.7626 - acc: 0.7980 - val_loss: 0.7779 - val_acc: 0.7610\n",
      "Learning rate:  0.001\n",
      "Epoch 51/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7488 - acc: 0.7602\n",
      "Epoch 00051: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.7449 - acc: 0.7621 - val_loss: 0.6926 - val_acc: 0.7956\n",
      "Learning rate:  0.001\n",
      "Epoch 52/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7026 - acc: 0.8012\n",
      "Epoch 00052: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.6973 - acc: 0.8044 - val_loss: 0.5916 - val_acc: 0.8536\n",
      "Learning rate:  0.001\n",
      "Epoch 53/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7263 - acc: 0.7828\n",
      "Epoch 00053: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 27s 439ms/step - loss: 0.7225 - acc: 0.7863 - val_loss: 0.7485 - val_acc: 0.7500\n",
      "Learning rate:  0.001\n",
      "Epoch 54/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6350 - acc: 0.8361\n",
      "Epoch 00054: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.6390 - acc: 0.8306 - val_loss: 0.5733 - val_acc: 0.8494\n",
      "Learning rate:  0.001\n",
      "Epoch 55/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5955 - acc: 0.8049\n",
      "Epoch 00055: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.5929 - acc: 0.8061 - val_loss: 0.6281 - val_acc: 0.7983\n",
      "Learning rate:  0.001\n",
      "Epoch 56/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6507 - acc: 0.7889\n",
      "Epoch 00056: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 0.6474 - acc: 0.7903 - val_loss: 0.5856 - val_acc: 0.8536\n",
      "Learning rate:  0.001\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/62 [============================>.] - ETA: 0s - loss: 0.6002 - acc: 0.8217\n",
      "Epoch 00057: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.5984 - acc: 0.8226 - val_loss: 0.6810 - val_acc: 0.7790\n",
      "Learning rate:  0.001\n",
      "Epoch 58/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6544 - acc: 0.8094\n",
      "Epoch 00058: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.6576 - acc: 0.8065 - val_loss: 0.7460 - val_acc: 0.7901\n",
      "Learning rate:  0.001\n",
      "Epoch 59/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5466 - acc: 0.8566\n",
      "Epoch 00059: val_acc did not improve from 0.86050\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.5411 - acc: 0.8589 - val_loss: 0.6154 - val_acc: 0.8301\n",
      "Learning rate:  0.001\n",
      "Epoch 60/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6394 - acc: 0.8049\n",
      "Epoch 00060: val_acc improved from 0.86050 to 0.86326, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.6391 - acc: 0.8040 - val_loss: 0.5474 - val_acc: 0.8633\n",
      "Learning rate:  0.001\n",
      "Epoch 61/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6578 - acc: 0.7951\n",
      "Epoch 00061: val_acc did not improve from 0.86326\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.6546 - acc: 0.7964 - val_loss: 0.6001 - val_acc: 0.8246\n",
      "Learning rate:  0.001\n",
      "Epoch 62/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6367 - acc: 0.8238\n",
      "Epoch 00062: val_acc did not improve from 0.86326\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.6304 - acc: 0.8266 - val_loss: 0.6827 - val_acc: 0.7776\n",
      "Learning rate:  0.001\n",
      "Epoch 63/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5747 - acc: 0.8463\n",
      "Epoch 00063: val_acc improved from 0.86326 to 0.86878, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.5718 - acc: 0.8448 - val_loss: 0.5177 - val_acc: 0.8688\n",
      "Learning rate:  0.001\n",
      "Epoch 64/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5835 - acc: 0.8340\n",
      "Epoch 00064: val_acc did not improve from 0.86878\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 0.5899 - acc: 0.8306 - val_loss: 1.0885 - val_acc: 0.6492\n",
      "Learning rate:  0.001\n",
      "Epoch 65/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6650 - acc: 0.8176\n",
      "Epoch 00065: val_acc did not improve from 0.86878\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.6625 - acc: 0.8185 - val_loss: 0.6079 - val_acc: 0.8273\n",
      "Learning rate:  0.001\n",
      "Epoch 66/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6168 - acc: 0.8214\n",
      "Epoch 00066: val_acc improved from 0.86878 to 0.87845, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.6207 - acc: 0.8182 - val_loss: 0.5173 - val_acc: 0.8785\n",
      "Learning rate:  0.001\n",
      "Epoch 67/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.8279\n",
      "Epoch 00067: val_acc did not improve from 0.87845\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.6022 - acc: 0.8306 - val_loss: 0.6075 - val_acc: 0.8177\n",
      "Learning rate:  0.001\n",
      "Epoch 68/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5560 - acc: 0.8443\n",
      "Epoch 00068: val_acc improved from 0.87845 to 0.91022, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 419ms/step - loss: 0.5502 - acc: 0.8468 - val_loss: 0.4495 - val_acc: 0.9102\n",
      "Learning rate:  0.001\n",
      "Epoch 69/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5953 - acc: 0.8381\n",
      "Epoch 00069: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.5964 - acc: 0.8367 - val_loss: 0.5805 - val_acc: 0.8356\n",
      "Learning rate:  0.001\n",
      "Epoch 70/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5919 - acc: 0.8217\n",
      "Epoch 00070: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 0.5915 - acc: 0.8226 - val_loss: 0.6514 - val_acc: 0.8218\n",
      "Learning rate:  0.001\n",
      "Epoch 71/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5526 - acc: 0.8299\n",
      "Epoch 00071: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.5494 - acc: 0.8306 - val_loss: 0.5077 - val_acc: 0.8619\n",
      "Learning rate:  0.001\n",
      "Epoch 72/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6163 - acc: 0.7971\n",
      "Epoch 00072: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.6172 - acc: 0.7984 - val_loss: 0.6763 - val_acc: 0.7859\n",
      "Learning rate:  0.001\n",
      "Epoch 73/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5771 - acc: 0.8152\n",
      "Epoch 00073: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.5712 - acc: 0.8182 - val_loss: 0.4595 - val_acc: 0.8867\n",
      "Learning rate:  0.001\n",
      "Epoch 74/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5181 - acc: 0.8463\n",
      "Epoch 00074: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.5139 - acc: 0.8488 - val_loss: 0.5079 - val_acc: 0.8536\n",
      "Learning rate:  0.001\n",
      "Epoch 75/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6398 - acc: 0.8074\n",
      "Epoch 00075: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.6391 - acc: 0.8065 - val_loss: 0.7210 - val_acc: 0.7459\n",
      "Learning rate:  0.001\n",
      "Epoch 76/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5682 - acc: 0.8357\n",
      "Epoch 00076: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 418ms/step - loss: 0.5663 - acc: 0.8384 - val_loss: 0.6078 - val_acc: 0.8122\n",
      "Learning rate:  0.001\n",
      "Epoch 77/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5858 - acc: 0.8402\n",
      "Epoch 00077: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.5838 - acc: 0.8407 - val_loss: 0.6781 - val_acc: 0.7583\n",
      "Learning rate:  0.001\n",
      "Epoch 78/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8504\n",
      "Epoch 00078: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.4965 - acc: 0.8528 - val_loss: 0.6593 - val_acc: 0.7859\n",
      "Learning rate:  0.001\n",
      "Epoch 79/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8279\n",
      "Epoch 00079: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.5672 - acc: 0.8266 - val_loss: 0.6661 - val_acc: 0.7666\n",
      "Learning rate:  0.001\n",
      "Epoch 80/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6073 - acc: 0.8340\n",
      "Epoch 00080: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.6029 - acc: 0.8347 - val_loss: 0.8906 - val_acc: 0.7334\n",
      "Learning rate:  0.001\n",
      "Epoch 81/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5940 - acc: 0.8238\n",
      "Epoch 00081: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 0.5957 - acc: 0.8246 - val_loss: 0.5585 - val_acc: 0.8343\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5448 - acc: 0.8320\n",
      "Epoch 00082: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 439ms/step - loss: 0.5423 - acc: 0.8347 - val_loss: 0.5128 - val_acc: 0.8729\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5127 - acc: 0.8337\n",
      "Epoch 00083: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.5104 - acc: 0.8343 - val_loss: 0.5070 - val_acc: 0.8564\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4756 - acc: 0.8566\n",
      "Epoch 00084: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 455ms/step - loss: 0.4772 - acc: 0.8569 - val_loss: 0.4668 - val_acc: 0.8812\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/62 [============================>.] - ETA: 0s - loss: 0.4701 - acc: 0.8504\n",
      "Epoch 00085: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.4752 - acc: 0.8468 - val_loss: 0.4841 - val_acc: 0.8577\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5430 - acc: 0.8361\n",
      "Epoch 00086: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.5409 - acc: 0.8347 - val_loss: 0.4757 - val_acc: 0.8674\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8955\n",
      "Epoch 00087: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.4265 - acc: 0.8949 - val_loss: 0.4686 - val_acc: 0.8688\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.8525\n",
      "Epoch 00088: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.5287 - acc: 0.8508 - val_loss: 0.4343 - val_acc: 0.8950\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8501\n",
      "Epoch 00089: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.5060 - acc: 0.8505 - val_loss: 0.4249 - val_acc: 0.8923\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5005 - acc: 0.8627\n",
      "Epoch 00090: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.5043 - acc: 0.8629 - val_loss: 0.4560 - val_acc: 0.8798\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4525 - acc: 0.8811\n",
      "Epoch 00091: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.4600 - acc: 0.8750 - val_loss: 0.4415 - val_acc: 0.8840\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4302 - acc: 0.8832\n",
      "Epoch 00092: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.4285 - acc: 0.8831 - val_loss: 0.4225 - val_acc: 0.8978\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.8689\n",
      "Epoch 00093: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 441ms/step - loss: 0.4231 - acc: 0.8710 - val_loss: 0.4274 - val_acc: 0.8812\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8809\n",
      "Epoch 00094: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.4451 - acc: 0.8808 - val_loss: 0.4290 - val_acc: 0.8826\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4401 - acc: 0.8811\n",
      "Epoch 00095: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.4371 - acc: 0.8831 - val_loss: 0.4305 - val_acc: 0.8771\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4877 - acc: 0.8648\n",
      "Epoch 00096: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.4870 - acc: 0.8629 - val_loss: 0.4454 - val_acc: 0.8743\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.8893\n",
      "Epoch 00097: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.4440 - acc: 0.8871 - val_loss: 0.4164 - val_acc: 0.8854\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8811\n",
      "Epoch 00098: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.4126 - acc: 0.8810 - val_loss: 0.4267 - val_acc: 0.8854\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4573 - acc: 0.8791\n",
      "Epoch 00099: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.4540 - acc: 0.8810 - val_loss: 0.4604 - val_acc: 0.8688\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.8750\n",
      "Epoch 00100: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.4451 - acc: 0.8750 - val_loss: 0.4421 - val_acc: 0.8757\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4079 - acc: 0.8791\n",
      "Epoch 00101: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.4042 - acc: 0.8810 - val_loss: 0.4072 - val_acc: 0.8923\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8809\n",
      "Epoch 00102: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.4499 - acc: 0.8828 - val_loss: 0.4434 - val_acc: 0.8619\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4666 - acc: 0.8689\n",
      "Epoch 00103: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 0.4668 - acc: 0.8690 - val_loss: 0.4385 - val_acc: 0.8812\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4289 - acc: 0.9016\n",
      "Epoch 00104: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.4255 - acc: 0.9032 - val_loss: 0.4328 - val_acc: 0.8743\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.8504\n",
      "Epoch 00105: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.4881 - acc: 0.8508 - val_loss: 0.3919 - val_acc: 0.9019\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4094 - acc: 0.9016\n",
      "Epoch 00106: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.4050 - acc: 0.9032 - val_loss: 0.4243 - val_acc: 0.8964\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4767 - acc: 0.8665\n",
      "Epoch 00107: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.4821 - acc: 0.8646 - val_loss: 0.4098 - val_acc: 0.8895\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8299\n",
      "Epoch 00108: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.4826 - acc: 0.8306 - val_loss: 0.4325 - val_acc: 0.8812\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8750\n",
      "Epoch 00109: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.4260 - acc: 0.8750 - val_loss: 0.4424 - val_acc: 0.8633\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.8912\n",
      "Epoch 00110: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.3937 - acc: 0.8889 - val_loss: 0.4296 - val_acc: 0.8798\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4583 - acc: 0.8770\n",
      "Epoch 00111: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.4545 - acc: 0.8790 - val_loss: 0.4167 - val_acc: 0.8867\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4146 - acc: 0.8955\n",
      "Epoch 00112: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 0.4133 - acc: 0.8952 - val_loss: 0.4771 - val_acc: 0.8550\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.9016\n",
      "Epoch 00113: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 438ms/step - loss: 0.4051 - acc: 0.8992 - val_loss: 0.4712 - val_acc: 0.8660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0001\n",
      "Epoch 114/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4314 - acc: 0.8709\n",
      "Epoch 00114: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.4275 - acc: 0.8730 - val_loss: 0.4158 - val_acc: 0.8867\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4388 - acc: 0.8686\n",
      "Epoch 00115: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 417ms/step - loss: 0.4371 - acc: 0.8687 - val_loss: 0.4545 - val_acc: 0.8757\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4522 - acc: 0.8770\n",
      "Epoch 00116: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.4479 - acc: 0.8790 - val_loss: 0.4341 - val_acc: 0.8798\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4403 - acc: 0.8607\n",
      "Epoch 00117: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.4385 - acc: 0.8629 - val_loss: 0.4601 - val_acc: 0.8674\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4134 - acc: 0.8893\n",
      "Epoch 00118: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.4140 - acc: 0.8911 - val_loss: 0.4726 - val_acc: 0.8412\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4112 - acc: 0.8832\n",
      "Epoch 00119: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.4161 - acc: 0.8810 - val_loss: 0.4861 - val_acc: 0.8522\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4450 - acc: 0.8709\n",
      "Epoch 00120: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 0.4409 - acc: 0.8730 - val_loss: 0.4537 - val_acc: 0.8757\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8893\n",
      "Epoch 00121: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.3963 - acc: 0.8891 - val_loss: 0.4205 - val_acc: 0.8881\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8850\n",
      "Epoch 00122: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.4155 - acc: 0.8828 - val_loss: 0.4132 - val_acc: 0.8840\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8975\n",
      "Epoch 00123: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.3988 - acc: 0.8972 - val_loss: 0.4021 - val_acc: 0.8950\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4526 - acc: 0.8689\n",
      "Epoch 00124: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 415ms/step - loss: 0.4487 - acc: 0.8710 - val_loss: 0.3945 - val_acc: 0.9061\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3723 - acc: 0.9139\n",
      "Epoch 00125: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.3737 - acc: 0.9133 - val_loss: 0.4224 - val_acc: 0.8840\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.9016\n",
      "Epoch 00126: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.3645 - acc: 0.8992 - val_loss: 0.4216 - val_acc: 0.8840\n",
      "Learning rate:  1e-05\n",
      "Epoch 127/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.8973\n",
      "Epoch 00127: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 25s 407ms/step - loss: 0.3895 - acc: 0.8949 - val_loss: 0.4234 - val_acc: 0.8771\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3869 - acc: 0.8832\n",
      "Epoch 00128: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 445ms/step - loss: 0.3874 - acc: 0.8831 - val_loss: 0.4155 - val_acc: 0.8729\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8955\n",
      "Epoch 00129: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.3899 - acc: 0.8952 - val_loss: 0.4229 - val_acc: 0.8785\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8891\n",
      "Epoch 00130: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.4281 - acc: 0.8889 - val_loss: 0.4193 - val_acc: 0.8771\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4036 - acc: 0.8852\n",
      "Epoch 00131: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.3999 - acc: 0.8871 - val_loss: 0.4398 - val_acc: 0.8729\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4179 - acc: 0.8750\n",
      "Epoch 00132: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 0.4170 - acc: 0.8750 - val_loss: 0.4286 - val_acc: 0.8591\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4473 - acc: 0.8665\n",
      "Epoch 00133: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.4432 - acc: 0.8687 - val_loss: 0.4412 - val_acc: 0.8757\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.8934\n",
      "Epoch 00134: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.4073 - acc: 0.8952 - val_loss: 0.4263 - val_acc: 0.8674\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.8914\n",
      "Epoch 00135: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.4093 - acc: 0.8891 - val_loss: 0.4095 - val_acc: 0.8840\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3600 - acc: 0.9221\n",
      "Epoch 00136: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.3566 - acc: 0.9234 - val_loss: 0.4229 - val_acc: 0.8798\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8791\n",
      "Epoch 00137: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.4003 - acc: 0.8790 - val_loss: 0.4179 - val_acc: 0.8826\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.9016\n",
      "Epoch 00138: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 32s 511ms/step - loss: 0.3816 - acc: 0.9032 - val_loss: 0.4357 - val_acc: 0.8840\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8932\n",
      "Epoch 00139: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.3817 - acc: 0.8929 - val_loss: 0.4297 - val_acc: 0.8771\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4721 - acc: 0.8811\n",
      "Epoch 00140: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 29s 467ms/step - loss: 0.4689 - acc: 0.8810 - val_loss: 0.4166 - val_acc: 0.8881\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.8791\n",
      "Epoch 00141: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 459ms/step - loss: 0.4339 - acc: 0.8770 - val_loss: 0.4336 - val_acc: 0.8715\n",
      "Learning rate:  1e-06\n",
      "Epoch 142/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8914\n",
      "Epoch 00142: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 31s 503ms/step - loss: 0.4107 - acc: 0.8931 - val_loss: 0.4309 - val_acc: 0.8785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-06\n",
      "Epoch 143/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3913 - acc: 0.8934\n",
      "Epoch 00143: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.3895 - acc: 0.8952 - val_loss: 0.4237 - val_acc: 0.8743\n",
      "Learning rate:  1e-06\n",
      "Epoch 144/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8791\n",
      "Epoch 00144: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.4163 - acc: 0.8810 - val_loss: 0.4208 - val_acc: 0.8785\n",
      "Learning rate:  1e-06\n",
      "Epoch 145/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3318 - acc: 0.9199\n",
      "Epoch 00145: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.3337 - acc: 0.9192 - val_loss: 0.4301 - val_acc: 0.8674\n",
      "Learning rate:  1e-06\n",
      "Epoch 146/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3591 - acc: 0.8996\n",
      "Epoch 00146: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 434ms/step - loss: 0.3567 - acc: 0.9012 - val_loss: 0.4332 - val_acc: 0.8729\n",
      "Learning rate:  1e-06\n",
      "Epoch 147/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.9037\n",
      "Epoch 00147: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 0.4007 - acc: 0.8992 - val_loss: 0.4290 - val_acc: 0.8743\n",
      "Learning rate:  1e-06\n",
      "Epoch 148/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.8873\n",
      "Epoch 00148: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.3910 - acc: 0.8891 - val_loss: 0.4325 - val_acc: 0.8619\n",
      "Learning rate:  1e-06\n",
      "Epoch 149/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3854 - acc: 0.8893\n",
      "Epoch 00149: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.3834 - acc: 0.8911 - val_loss: 0.4145 - val_acc: 0.8812\n",
      "Learning rate:  1e-06\n",
      "Epoch 150/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.9016\n",
      "Epoch 00150: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.3743 - acc: 0.8992 - val_loss: 0.4535 - val_acc: 0.8646\n",
      "Learning rate:  1e-06\n",
      "Epoch 151/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4545 - acc: 0.8706\n",
      "Epoch 00151: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.4513 - acc: 0.8727 - val_loss: 0.4365 - val_acc: 0.8798\n",
      "Learning rate:  1e-06\n",
      "Epoch 152/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8914\n",
      "Epoch 00152: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.3774 - acc: 0.8931 - val_loss: 0.4330 - val_acc: 0.8633\n",
      "Learning rate:  1e-06\n",
      "Epoch 153/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.8747\n",
      "Epoch 00153: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.4473 - acc: 0.8747 - val_loss: 0.4303 - val_acc: 0.8757\n",
      "Learning rate:  1e-06\n",
      "Epoch 154/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3874 - acc: 0.8893\n",
      "Epoch 00154: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.3844 - acc: 0.8911 - val_loss: 0.4122 - val_acc: 0.8895\n",
      "Learning rate:  1e-06\n",
      "Epoch 155/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.8955\n",
      "Epoch 00155: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.3947 - acc: 0.8972 - val_loss: 0.4225 - val_acc: 0.8715\n",
      "Learning rate:  1e-06\n",
      "Epoch 156/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8852\n",
      "Epoch 00156: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.3860 - acc: 0.8871 - val_loss: 0.4335 - val_acc: 0.8826\n",
      "Learning rate:  1e-06\n",
      "Epoch 157/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4144 - acc: 0.8996\n",
      "Epoch 00157: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.4153 - acc: 0.8972 - val_loss: 0.4231 - val_acc: 0.8854\n",
      "Learning rate:  1e-06\n",
      "Epoch 158/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3440 - acc: 0.9160\n",
      "Epoch 00158: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.3463 - acc: 0.9153 - val_loss: 0.4318 - val_acc: 0.8729\n",
      "Learning rate:  1e-06\n",
      "Epoch 159/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4558 - acc: 0.8730\n",
      "Epoch 00159: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.4544 - acc: 0.8750 - val_loss: 0.4286 - val_acc: 0.8854\n",
      "Learning rate:  1e-06\n",
      "Epoch 160/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4213 - acc: 0.8830\n",
      "Epoch 00160: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.4246 - acc: 0.8828 - val_loss: 0.4290 - val_acc: 0.8743\n",
      "Learning rate:  1e-06\n",
      "Epoch 161/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8996\n",
      "Epoch 00161: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3962 - acc: 0.9012 - val_loss: 0.4319 - val_acc: 0.8771\n",
      "Learning rate:  5e-07\n",
      "Epoch 162/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.9076\n",
      "Epoch 00162: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.3443 - acc: 0.9051 - val_loss: 0.4325 - val_acc: 0.8798\n",
      "Learning rate:  5e-07\n",
      "Epoch 163/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3787 - acc: 0.8893\n",
      "Epoch 00163: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.3840 - acc: 0.8891 - val_loss: 0.4436 - val_acc: 0.8674\n",
      "Learning rate:  5e-07\n",
      "Epoch 164/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.8852\n",
      "Epoch 00164: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.3845 - acc: 0.8871 - val_loss: 0.4237 - val_acc: 0.8688\n",
      "Learning rate:  5e-07\n",
      "Epoch 165/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4304 - acc: 0.8955\n",
      "Epoch 00165: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.4379 - acc: 0.8931 - val_loss: 0.4249 - val_acc: 0.8840\n",
      "Learning rate:  5e-07\n",
      "Epoch 166/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3581 - acc: 0.9057\n",
      "Epoch 00166: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.3583 - acc: 0.9073 - val_loss: 0.4264 - val_acc: 0.8619\n",
      "Learning rate:  5e-07\n",
      "Epoch 167/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4288 - acc: 0.8750\n",
      "Epoch 00167: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 446ms/step - loss: 0.4262 - acc: 0.8750 - val_loss: 0.4346 - val_acc: 0.8702\n",
      "Learning rate:  5e-07\n",
      "Epoch 168/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3812 - acc: 0.8996\n",
      "Epoch 00168: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.3808 - acc: 0.8992 - val_loss: 0.4199 - val_acc: 0.8826\n",
      "Learning rate:  5e-07\n",
      "Epoch 169/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8809\n",
      "Epoch 00169: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.3939 - acc: 0.8808 - val_loss: 0.4371 - val_acc: 0.8798\n",
      "Learning rate:  5e-07\n",
      "Epoch 170/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4002 - acc: 0.8934\n",
      "Epoch 00170: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3969 - acc: 0.8952 - val_loss: 0.4271 - val_acc: 0.8757\n",
      "Learning rate:  5e-07\n",
      "Epoch 171/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8996\n",
      "Epoch 00171: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 415ms/step - loss: 0.3746 - acc: 0.8992 - val_loss: 0.4283 - val_acc: 0.8785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  5e-07\n",
      "Epoch 172/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4369 - acc: 0.8668\n",
      "Epoch 00172: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 446ms/step - loss: 0.4358 - acc: 0.8669 - val_loss: 0.4343 - val_acc: 0.8757\n",
      "Learning rate:  5e-07\n",
      "Epoch 173/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4022 - acc: 0.8871\n",
      "Epoch 00173: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.4086 - acc: 0.8828 - val_loss: 0.4061 - val_acc: 0.8881\n",
      "Learning rate:  5e-07\n",
      "Epoch 174/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3535 - acc: 0.9160\n",
      "Epoch 00174: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.3601 - acc: 0.9133 - val_loss: 0.4240 - val_acc: 0.8591\n",
      "Learning rate:  5e-07\n",
      "Epoch 175/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4105 - acc: 0.8811\n",
      "Epoch 00175: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.4092 - acc: 0.8810 - val_loss: 0.4341 - val_acc: 0.8688\n",
      "Learning rate:  5e-07\n",
      "Epoch 176/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4432 - acc: 0.8689\n",
      "Epoch 00176: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 443ms/step - loss: 0.4438 - acc: 0.8669 - val_loss: 0.4413 - val_acc: 0.8757\n",
      "Learning rate:  5e-07\n",
      "Epoch 177/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.9016\n",
      "Epoch 00177: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.3739 - acc: 0.9032 - val_loss: 0.4344 - val_acc: 0.8674\n",
      "Learning rate:  5e-07\n",
      "Epoch 178/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3411 - acc: 0.9078\n",
      "Epoch 00178: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.3455 - acc: 0.9073 - val_loss: 0.4171 - val_acc: 0.8812\n",
      "Learning rate:  5e-07\n",
      "Epoch 179/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8873\n",
      "Epoch 00179: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.4035 - acc: 0.8891 - val_loss: 0.4303 - val_acc: 0.8757\n",
      "Learning rate:  5e-07\n",
      "Epoch 180/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4051 - acc: 0.8871\n",
      "Epoch 00180: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.4022 - acc: 0.8889 - val_loss: 0.4439 - val_acc: 0.8702\n",
      "Learning rate:  5e-07\n",
      "Epoch 181/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4100 - acc: 0.8893\n",
      "Epoch 00181: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 445ms/step - loss: 0.4116 - acc: 0.8891 - val_loss: 0.4123 - val_acc: 0.8881\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.8832\n",
      "Epoch 00182: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.4199 - acc: 0.8810 - val_loss: 0.4184 - val_acc: 0.8646\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3496 - acc: 0.9037\n",
      "Epoch 00183: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.3469 - acc: 0.9052 - val_loss: 0.4359 - val_acc: 0.8619\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.9098\n",
      "Epoch 00184: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 419ms/step - loss: 0.3828 - acc: 0.9073 - val_loss: 0.4407 - val_acc: 0.8785\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.8789\n",
      "Epoch 00185: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.4209 - acc: 0.8808 - val_loss: 0.4248 - val_acc: 0.8729\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.9098\n",
      "Epoch 00186: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.3427 - acc: 0.9113 - val_loss: 0.4160 - val_acc: 0.8812\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8973\n",
      "Epoch 00187: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.3688 - acc: 0.8970 - val_loss: 0.4254 - val_acc: 0.8798\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3691 - acc: 0.8934\n",
      "Epoch 00188: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 0.3827 - acc: 0.8871 - val_loss: 0.4175 - val_acc: 0.8812\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4217 - acc: 0.8832\n",
      "Epoch 00189: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.4178 - acc: 0.8851 - val_loss: 0.4221 - val_acc: 0.8826\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3745 - acc: 0.9037\n",
      "Epoch 00190: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.3862 - acc: 0.8992 - val_loss: 0.4393 - val_acc: 0.8702\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4164 - acc: 0.8689\n",
      "Epoch 00191: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 434ms/step - loss: 0.4126 - acc: 0.8710 - val_loss: 0.4231 - val_acc: 0.8826\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8832\n",
      "Epoch 00192: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 418ms/step - loss: 0.4107 - acc: 0.8831 - val_loss: 0.4311 - val_acc: 0.8785\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8566\n",
      "Epoch 00193: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.4614 - acc: 0.8569 - val_loss: 0.4220 - val_acc: 0.8785\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3541 - acc: 0.8996\n",
      "Epoch 00194: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 434ms/step - loss: 0.3538 - acc: 0.8992 - val_loss: 0.4337 - val_acc: 0.8715\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8873\n",
      "Epoch 00195: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.4194 - acc: 0.8831 - val_loss: 0.4289 - val_acc: 0.8688\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.9035\n",
      "Epoch 00196: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 438ms/step - loss: 0.3731 - acc: 0.9030 - val_loss: 0.4215 - val_acc: 0.8743\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.8914\n",
      "Epoch 00197: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.3851 - acc: 0.8911 - val_loss: 0.4355 - val_acc: 0.8729\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.9014\n",
      "Epoch 00198: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3767 - acc: 0.8990 - val_loss: 0.4287 - val_acc: 0.8771\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8934\n",
      "Epoch 00199: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 418ms/step - loss: 0.3910 - acc: 0.8911 - val_loss: 0.4379 - val_acc: 0.8674\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8668\n",
      "Epoch 00200: val_acc did not improve from 0.91022\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.4255 - acc: 0.8690 - val_loss: 0.4376 - val_acc: 0.8674\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "num_train_images = 1000\n",
    "\n",
    "def lrSchedule(epoch):\n",
    "    lr = 1e-3\n",
    "\n",
    "    if epoch > 160:\n",
    "        lr *= 0.5e-3\n",
    "\n",
    "    elif epoch > 140:\n",
    "        lr *= 1e-3\n",
    "\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "\n",
    "    print('Learning rate: ', lr)\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "LRScheduler = LearningRateScheduler(lrSchedule)\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ca2\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "callbacks_list = [checkpoint, LRScheduler]\n",
    "\n",
    "history = model.fit_generator(train_generator, \n",
    "                              validation_data=test_generator,\n",
    "                              epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 300, 300, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_conv (Conv2D)              (None, 300, 300, 16) 448         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_bn (BatchNormalization)    (None, 300, 300, 16) 64          Inpt_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_relu (Activation)          (None, 300, 300, 16) 0           Inpt_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Inpt_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_add (Add)             (None, 300, 300, 16) 0           Inpt_relu[0][0]                  \n",
      "                                                                 Stg1_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_add (Add)             (None, 300, 300, 16) 0           Stg1_Blk1_relu[0][0]             \n",
      "                                                                 Stg1_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_add (Add)             (None, 300, 300, 16) 0           Stg1_Blk2_relu[0][0]             \n",
      "                                                                 Stg1_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 300, 300, 16) 0           Stg1_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_conv (Conv2D)    (None, 150, 150, 32) 4640        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_lin_conv (Conv2D)     (None, 150, 150, 32) 544         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg2_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk1_relu[0][0]             \n",
      "                                                                 Stg2_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk2_relu[0][0]             \n",
      "                                                                 Stg2_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 150, 150, 32) 0           Stg2_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_conv (Conv2D)    (None, 75, 75, 64)   18496       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_lin_conv (Conv2D)     (None, 75, 75, 64)   2112        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg3_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk1_relu[0][0]             \n",
      "                                                                 Stg3_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk2_relu[0][0]             \n",
      "                                                                 Stg3_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 75, 75, 64)   0           Stg3_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_conv (Conv2D)    (None, 38, 38, 128)  73856       dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_lin_conv (Conv2D)     (None, 38, 38, 128)  8320        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg4_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk1_relu[0][0]             \n",
      "                                                                 Stg4_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk2_relu[0][0]             \n",
      "                                                                 Stg4_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "AvgPool (AveragePooling2D)      (None, 4, 4, 128)    0           Stg4_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2048)         0           AvgPool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            6147        flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,103,107\n",
      "Trainable params: 1,100,195\n",
      "Non-trainable params: 2,912\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "\n",
    "optmz = optimizers.Adam(lr=0.001)\n",
    "\n",
    "def resLyr(inputs,\n",
    "           numFilters=16,\n",
    "           kernelSz=3,\n",
    "           strides=1,\n",
    "           activation='relu',\n",
    "           batchNorm=True,\n",
    "           convFirst=True,\n",
    "           lyrName=None):\n",
    "  \n",
    "    convLyr = Conv2D(numFilters,\n",
    "                     kernel_size=kernelSz,\n",
    "                     strides=strides,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4),\n",
    "                     name=lyrName + '_conv' if lyrName else None)\n",
    "    x = inputs\n",
    "    if convFirst:\n",
    "        x = convLyr(x)\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "    else:\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "        x = convLyr(x)\n",
    "  \n",
    "    return x\n",
    "\n",
    "\n",
    "def resBlkV1(inputs,\n",
    "             numFilters=16,\n",
    "             numBlocks=3,\n",
    "             downsampleOnFirst=True,\n",
    "             names=None):\n",
    "  \n",
    "    x = inputs\n",
    "    for run in range(0, numBlocks):\n",
    "        strides = 1\n",
    "        blkStr = str(run + 1)\n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            strides = 2\n",
    "            \n",
    "        y = resLyr(inputs=x, numFilters=numFilters, strides=strides, lyrName=names+'_Blk'+blkStr+'_Res1' if names else None)\n",
    "        y = resLyr(inputs=y, numFilters=numFilters, activation=None, lyrName=names+'_Blk'+blkStr+'_Res2' if names else None) \n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            x = resLyr(inputs=x, numFilters=numFilters, kernelSz=1, \n",
    "                       strides=strides, activation=None, batchNorm=False, \n",
    "                       lyrName=names+'_Blk'+blkStr+'_lin' if names else None)\n",
    "\n",
    "        x = add([x, y], name=names+'_Blk'+blkStr+'_add' if names else None) \n",
    "\n",
    "        x = Activation('relu',  name=names+'_Blk'+blkStr+'_relu' if names else None)(x)   \n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def createResNetV1(inputShape=(32, 32, 3),\n",
    "                   numClasses=3):\n",
    "  \n",
    "    inputs = Input(shape=inputShape)\n",
    "    v = resLyr(inputs,\n",
    "               lyrName='Inpt')\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=16,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=False,\n",
    "                 names='Stg1')\n",
    "    v = Dropout(0.4)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=32,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg2')\n",
    "    v = Dropout(0.4)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=64,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg3')\n",
    "    v = Dropout(0.4)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=128,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg4')    \n",
    "    \n",
    "    v = AveragePooling2D(pool_size=8,\n",
    "                         name='AvgPool')(v)\n",
    "    v = Flatten()(v)\n",
    "    outputs = Dense(numClasses,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(v)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optmz,\n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = createResNetV1(inputShape=(HEIGHT, WIDTH, 3))  # This is meant for training\n",
    "modelGo = createResNetV1(inputShape=(HEIGHT, WIDTH, 3))  # This is used for final testing\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 3.1897 - acc: 0.4324\n",
      "Epoch 00001: val_acc improved from -inf to 0.31906, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 40s 651ms/step - loss: 3.1658 - acc: 0.4315 - val_loss: 51.0077 - val_acc: 0.3191\n",
      "Learning rate:  0.001\n",
      "Epoch 2/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.5690 - acc: 0.5246\n",
      "Epoch 00002: val_acc improved from 0.31906 to 0.37983, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 25s 399ms/step - loss: 1.5633 - acc: 0.5262 - val_loss: 6.9169 - val_acc: 0.3798\n",
      "Learning rate:  0.001\n",
      "Epoch 3/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.4644 - acc: 0.5606\n",
      "Epoch 00003: val_acc improved from 0.37983 to 0.50829, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 25s 410ms/step - loss: 1.4559 - acc: 0.5657 - val_loss: 1.9937 - val_acc: 0.5083\n",
      "Learning rate:  0.001\n",
      "Epoch 4/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.4045 - acc: 0.5512\n",
      "Epoch 00004: val_acc improved from 0.50829 to 0.63260, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 416ms/step - loss: 1.3943 - acc: 0.5565 - val_loss: 1.9330 - val_acc: 0.6326\n",
      "Learning rate:  0.001\n",
      "Epoch 5/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2762 - acc: 0.6209\n",
      "Epoch 00005: val_acc improved from 0.63260 to 0.72238, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 1.2674 - acc: 0.6250 - val_loss: 1.1393 - val_acc: 0.7224\n",
      "Learning rate:  0.001\n",
      "Epoch 6/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1176 - acc: 0.6926\n",
      "Epoch 00006: val_acc did not improve from 0.72238\n",
      "62/62 [==============================] - 26s 416ms/step - loss: 1.1107 - acc: 0.6956 - val_loss: 1.1806 - val_acc: 0.7099\n",
      "Learning rate:  0.001\n",
      "Epoch 7/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1277 - acc: 0.6407\n",
      "Epoch 00007: val_acc improved from 0.72238 to 0.73481, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 1.1285 - acc: 0.6444 - val_loss: 1.1113 - val_acc: 0.7348\n",
      "Learning rate:  0.001\n",
      "Epoch 8/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2954 - acc: 0.6148\n",
      "Epoch 00008: val_acc did not improve from 0.73481\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 1.3017 - acc: 0.6169 - val_loss: 1.2196 - val_acc: 0.6699\n",
      "Learning rate:  0.001\n",
      "Epoch 9/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1575 - acc: 0.6680\n",
      "Epoch 00009: val_acc improved from 0.73481 to 0.74862, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 1.1552 - acc: 0.6694 - val_loss: 0.9805 - val_acc: 0.7486\n",
      "Learning rate:  0.001\n",
      "Epoch 10/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0929 - acc: 0.6947\n",
      "Epoch 00010: val_acc did not improve from 0.74862\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 1.0938 - acc: 0.6895 - val_loss: 1.4200 - val_acc: 0.5691\n",
      "Learning rate:  0.001\n",
      "Epoch 11/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0585 - acc: 0.7049\n",
      "Epoch 00011: val_acc did not improve from 0.74862\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 1.0617 - acc: 0.7077 - val_loss: 1.2987 - val_acc: 0.6257\n",
      "Learning rate:  0.001\n",
      "Epoch 12/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0392 - acc: 0.7090\n",
      "Epoch 00012: val_acc did not improve from 0.74862\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 1.0494 - acc: 0.7016 - val_loss: 0.9567 - val_acc: 0.7307\n",
      "Learning rate:  0.001\n",
      "Epoch 13/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0621 - acc: 0.7023\n",
      "Epoch 00013: val_acc improved from 0.74862 to 0.83425, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 1.0601 - acc: 0.7051 - val_loss: 0.8440 - val_acc: 0.8343\n",
      "Learning rate:  0.001\n",
      "Epoch 14/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0261 - acc: 0.7029\n",
      "Epoch 00014: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 416ms/step - loss: 1.0216 - acc: 0.7056 - val_loss: 0.9621 - val_acc: 0.7251\n",
      "Learning rate:  0.001\n",
      "Epoch 15/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1156 - acc: 0.6701\n",
      "Epoch 00015: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 1.1103 - acc: 0.6714 - val_loss: 0.9875 - val_acc: 0.7251\n",
      "Learning rate:  0.001\n",
      "Epoch 16/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0491 - acc: 0.6885\n",
      "Epoch 00016: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 419ms/step - loss: 1.0413 - acc: 0.6935 - val_loss: 0.9888 - val_acc: 0.7376\n",
      "Learning rate:  0.001\n",
      "Epoch 17/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0495 - acc: 0.6947\n",
      "Epoch 00017: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 1.0469 - acc: 0.6956 - val_loss: 1.0744 - val_acc: 0.8052\n",
      "Learning rate:  0.001\n",
      "Epoch 18/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0381 - acc: 0.7105\n",
      "Epoch 00018: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 1.0371 - acc: 0.7111 - val_loss: 1.1950 - val_acc: 0.5483\n",
      "Learning rate:  0.001\n",
      "Epoch 19/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9660 - acc: 0.7418\n",
      "Epoch 00019: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.9653 - acc: 0.7440 - val_loss: 0.8099 - val_acc: 0.7956\n",
      "Learning rate:  0.001\n",
      "Epoch 20/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9034 - acc: 0.7684\n",
      "Epoch 00020: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.9054 - acc: 0.7661 - val_loss: 0.7921 - val_acc: 0.8177\n",
      "Learning rate:  0.001\n",
      "Epoch 21/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9807 - acc: 0.7295\n",
      "Epoch 00021: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.9837 - acc: 0.7278 - val_loss: 1.4961 - val_acc: 0.5925\n",
      "Learning rate:  0.001\n",
      "Epoch 22/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8644 - acc: 0.7725\n",
      "Epoch 00022: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.8669 - acc: 0.7722 - val_loss: 1.0295 - val_acc: 0.6961\n",
      "Learning rate:  0.001\n",
      "Epoch 23/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8443 - acc: 0.7930\n",
      "Epoch 00023: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.8488 - acc: 0.7923 - val_loss: 0.8763 - val_acc: 0.7472\n",
      "Learning rate:  0.001\n",
      "Epoch 24/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8912 - acc: 0.7741\n",
      "Epoch 00024: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.8883 - acc: 0.7758 - val_loss: 0.8381 - val_acc: 0.7928\n",
      "Learning rate:  0.001\n",
      "Epoch 25/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8996 - acc: 0.7725\n",
      "Epoch 00025: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 443ms/step - loss: 0.9055 - acc: 0.7681 - val_loss: 1.0748 - val_acc: 0.7265\n",
      "Learning rate:  0.001\n",
      "Epoch 26/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8305 - acc: 0.7828\n",
      "Epoch 00026: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.8349 - acc: 0.7823 - val_loss: 0.9814 - val_acc: 0.7030\n",
      "Learning rate:  0.001\n",
      "Epoch 27/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9224 - acc: 0.7331\n",
      "Epoch 00027: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.9206 - acc: 0.7354 - val_loss: 1.6927 - val_acc: 0.4461\n",
      "Learning rate:  0.001\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/62 [============================>.] - ETA: 0s - loss: 0.8715 - acc: 0.7746\n",
      "Epoch 00028: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.8727 - acc: 0.7722 - val_loss: 0.7953 - val_acc: 0.7666\n",
      "Learning rate:  0.001\n",
      "Epoch 29/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7760 - acc: 0.8074\n",
      "Epoch 00029: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.7731 - acc: 0.8105 - val_loss: 1.2091 - val_acc: 0.5912\n",
      "Learning rate:  0.001\n",
      "Epoch 30/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8467 - acc: 0.7582\n",
      "Epoch 00030: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 438ms/step - loss: 0.8417 - acc: 0.7601 - val_loss: 0.8767 - val_acc: 0.7224\n",
      "Learning rate:  0.001\n",
      "Epoch 31/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7953 - acc: 0.7848\n",
      "Epoch 00031: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.8236 - acc: 0.7742 - val_loss: 0.9207 - val_acc: 0.6961\n",
      "Learning rate:  0.001\n",
      "Epoch 32/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7665 - acc: 0.8033\n",
      "Epoch 00032: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.7661 - acc: 0.8024 - val_loss: 0.7009 - val_acc: 0.8343\n",
      "Learning rate:  0.001\n",
      "Epoch 33/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7578 - acc: 0.8115\n",
      "Epoch 00033: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.7569 - acc: 0.8125 - val_loss: 0.7760 - val_acc: 0.7776\n",
      "Learning rate:  0.001\n",
      "Epoch 34/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8227 - acc: 0.7556\n",
      "Epoch 00034: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.8206 - acc: 0.7576 - val_loss: 1.0080 - val_acc: 0.6533\n",
      "Learning rate:  0.001\n",
      "Epoch 35/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7826 - acc: 0.7992\n",
      "Epoch 00035: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.7755 - acc: 0.8024 - val_loss: 0.7835 - val_acc: 0.8011\n",
      "Learning rate:  0.001\n",
      "Epoch 36/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8014 - acc: 0.7947\n",
      "Epoch 00036: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.7970 - acc: 0.7960 - val_loss: 0.8302 - val_acc: 0.7293\n",
      "Learning rate:  0.001\n",
      "Epoch 37/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6779 - acc: 0.8258\n",
      "Epoch 00037: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.6778 - acc: 0.8246 - val_loss: 0.8071 - val_acc: 0.7818\n",
      "Learning rate:  0.001\n",
      "Epoch 38/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7932 - acc: 0.7828\n",
      "Epoch 00038: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.7859 - acc: 0.7863 - val_loss: 1.2910 - val_acc: 0.5939\n",
      "Learning rate:  0.001\n",
      "Epoch 39/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8168 - acc: 0.7828\n",
      "Epoch 00039: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.8240 - acc: 0.7802 - val_loss: 0.9488 - val_acc: 0.6768\n",
      "Learning rate:  0.001\n",
      "Epoch 40/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7455 - acc: 0.8238\n",
      "Epoch 00040: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 28s 446ms/step - loss: 0.7498 - acc: 0.8226 - val_loss: 1.5247 - val_acc: 0.4779\n",
      "Learning rate:  0.001\n",
      "Epoch 41/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6539 - acc: 0.8337\n",
      "Epoch 00041: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 433ms/step - loss: 0.6566 - acc: 0.8323 - val_loss: 1.0496 - val_acc: 0.6533\n",
      "Learning rate:  0.001\n",
      "Epoch 42/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7783 - acc: 0.8033\n",
      "Epoch 00042: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.7812 - acc: 0.8024 - val_loss: 0.7242 - val_acc: 0.7914\n",
      "Learning rate:  0.001\n",
      "Epoch 43/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7808 - acc: 0.7971\n",
      "Epoch 00043: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.7777 - acc: 0.7984 - val_loss: 0.9725 - val_acc: 0.6533\n",
      "Learning rate:  0.001\n",
      "Epoch 44/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7032 - acc: 0.7971\n",
      "Epoch 00044: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.7089 - acc: 0.7944 - val_loss: 0.7220 - val_acc: 0.7956\n",
      "Learning rate:  0.001\n",
      "Epoch 45/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7321 - acc: 0.7992\n",
      "Epoch 00045: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.7299 - acc: 0.7984 - val_loss: 0.6277 - val_acc: 0.8329\n",
      "Learning rate:  0.001\n",
      "Epoch 46/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7358 - acc: 0.7951\n",
      "Epoch 00046: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.7302 - acc: 0.7984 - val_loss: 0.8131 - val_acc: 0.7569\n",
      "Learning rate:  0.001\n",
      "Epoch 47/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7713 - acc: 0.7823\n",
      "Epoch 00047: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.7735 - acc: 0.7818 - val_loss: 0.8433 - val_acc: 0.6809\n",
      "Learning rate:  0.001\n",
      "Epoch 48/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.8258\n",
      "Epoch 00048: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.6506 - acc: 0.8246 - val_loss: 1.2453 - val_acc: 0.6036\n",
      "Learning rate:  0.001\n",
      "Epoch 49/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7085 - acc: 0.8053\n",
      "Epoch 00049: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 433ms/step - loss: 0.7176 - acc: 0.8024 - val_loss: 1.2478 - val_acc: 0.5635\n",
      "Learning rate:  0.001\n",
      "Epoch 50/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7163 - acc: 0.8094\n",
      "Epoch 00050: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.7175 - acc: 0.8085 - val_loss: 0.8627 - val_acc: 0.6975\n",
      "Learning rate:  0.001\n",
      "Epoch 51/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6747 - acc: 0.8074\n",
      "Epoch 00051: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 434ms/step - loss: 0.6724 - acc: 0.8085 - val_loss: 0.9587 - val_acc: 0.6519\n",
      "Learning rate:  0.001\n",
      "Epoch 52/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6536 - acc: 0.8135\n",
      "Epoch 00052: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.6485 - acc: 0.8145 - val_loss: 0.7463 - val_acc: 0.7390\n",
      "Learning rate:  0.001\n",
      "Epoch 53/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6966 - acc: 0.8258\n",
      "Epoch 00053: val_acc did not improve from 0.83425\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.6934 - acc: 0.8266 - val_loss: 1.1721 - val_acc: 0.5746\n",
      "Learning rate:  0.001\n",
      "Epoch 54/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6452 - acc: 0.8357\n",
      "Epoch 00054: val_acc improved from 0.83425 to 0.85083, saving model to ./checkpoints/ca2_model_weights.h5\n",
      "62/62 [==============================] - 28s 444ms/step - loss: 0.6444 - acc: 0.8343 - val_loss: 0.6002 - val_acc: 0.8508\n",
      "Learning rate:  0.001\n",
      "Epoch 55/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5814 - acc: 0.8484\n",
      "Epoch 00055: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.5860 - acc: 0.8448 - val_loss: 0.6096 - val_acc: 0.8315\n",
      "Learning rate:  0.001\n",
      "Epoch 56/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7108 - acc: 0.8131\n",
      "Epoch 00056: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 439ms/step - loss: 0.7203 - acc: 0.8121 - val_loss: 0.8527 - val_acc: 0.7652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 57/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6831 - acc: 0.8156\n",
      "Epoch 00057: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 439ms/step - loss: 0.6890 - acc: 0.8145 - val_loss: 0.9469 - val_acc: 0.6506\n",
      "Learning rate:  0.001\n",
      "Epoch 58/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6286 - acc: 0.8279\n",
      "Epoch 00058: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.6238 - acc: 0.8306 - val_loss: 0.7657 - val_acc: 0.7541\n",
      "Learning rate:  0.001\n",
      "Epoch 59/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6482 - acc: 0.8197\n",
      "Epoch 00059: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 31s 499ms/step - loss: 0.6601 - acc: 0.8165 - val_loss: 0.8375 - val_acc: 0.7265\n",
      "Learning rate:  0.001\n",
      "Epoch 60/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6383 - acc: 0.8381\n",
      "Epoch 00060: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 30s 476ms/step - loss: 0.6374 - acc: 0.8367 - val_loss: 0.7836 - val_acc: 0.7141\n",
      "Learning rate:  0.001\n",
      "Epoch 61/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6353 - acc: 0.8197\n",
      "Epoch 00061: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 30s 476ms/step - loss: 0.6368 - acc: 0.8185 - val_loss: 0.6419 - val_acc: 0.8025\n",
      "Learning rate:  0.001\n",
      "Epoch 62/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6349 - acc: 0.8115\n",
      "Epoch 00062: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 459ms/step - loss: 0.6398 - acc: 0.8085 - val_loss: 0.7777 - val_acc: 0.7472\n",
      "Learning rate:  0.001\n",
      "Epoch 63/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6625 - acc: 0.8111\n",
      "Epoch 00063: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 29s 471ms/step - loss: 0.6564 - acc: 0.8141 - val_loss: 0.7127 - val_acc: 0.8039\n",
      "Learning rate:  0.001\n",
      "Epoch 64/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6096 - acc: 0.8361\n",
      "Epoch 00064: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 455ms/step - loss: 0.6105 - acc: 0.8327 - val_loss: 0.6519 - val_acc: 0.7983\n",
      "Learning rate:  0.001\n",
      "Epoch 65/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6175 - acc: 0.8320\n",
      "Epoch 00065: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 457ms/step - loss: 0.6109 - acc: 0.8347 - val_loss: 0.7125 - val_acc: 0.7804\n",
      "Learning rate:  0.001\n",
      "Epoch 66/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6651 - acc: 0.8238\n",
      "Epoch 00066: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 29s 468ms/step - loss: 0.6644 - acc: 0.8246 - val_loss: 0.8582 - val_acc: 0.6865\n",
      "Learning rate:  0.001\n",
      "Epoch 67/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6074 - acc: 0.8111\n",
      "Epoch 00067: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.6059 - acc: 0.8121 - val_loss: 0.5777 - val_acc: 0.8467\n",
      "Learning rate:  0.001\n",
      "Epoch 68/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6092 - acc: 0.8238\n",
      "Epoch 00068: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 29s 461ms/step - loss: 0.6059 - acc: 0.8246 - val_loss: 0.5996 - val_acc: 0.8191\n",
      "Learning rate:  0.001\n",
      "Epoch 69/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5996 - acc: 0.8234\n",
      "Epoch 00069: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.5991 - acc: 0.8242 - val_loss: 0.6646 - val_acc: 0.8011\n",
      "Learning rate:  0.001\n",
      "Epoch 70/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6291 - acc: 0.8320\n",
      "Epoch 00070: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.6294 - acc: 0.8327 - val_loss: 0.9055 - val_acc: 0.6506\n",
      "Learning rate:  0.001\n",
      "Epoch 71/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5905 - acc: 0.8320\n",
      "Epoch 00071: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 433ms/step - loss: 0.5909 - acc: 0.8306 - val_loss: 0.7583 - val_acc: 0.7417\n",
      "Learning rate:  0.001\n",
      "Epoch 72/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.8320\n",
      "Epoch 00072: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.5676 - acc: 0.8347 - val_loss: 0.6743 - val_acc: 0.7735\n",
      "Learning rate:  0.001\n",
      "Epoch 73/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5648 - acc: 0.8422\n",
      "Epoch 00073: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.5705 - acc: 0.8387 - val_loss: 0.6861 - val_acc: 0.7624\n",
      "Learning rate:  0.001\n",
      "Epoch 74/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6045 - acc: 0.8398\n",
      "Epoch 00074: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 444ms/step - loss: 0.6037 - acc: 0.8404 - val_loss: 0.7812 - val_acc: 0.7210\n",
      "Learning rate:  0.001\n",
      "Epoch 75/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6490 - acc: 0.7951\n",
      "Epoch 00075: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.6481 - acc: 0.7964 - val_loss: 0.6618 - val_acc: 0.7901\n",
      "Learning rate:  0.001\n",
      "Epoch 76/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5852 - acc: 0.8299\n",
      "Epoch 00076: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 418ms/step - loss: 0.5809 - acc: 0.8327 - val_loss: 0.6778 - val_acc: 0.7970\n",
      "Learning rate:  0.001\n",
      "Epoch 77/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6266 - acc: 0.8156\n",
      "Epoch 00077: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.6268 - acc: 0.8145 - val_loss: 0.8890 - val_acc: 0.6616\n",
      "Learning rate:  0.001\n",
      "Epoch 78/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5380 - acc: 0.8463\n",
      "Epoch 00078: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.5326 - acc: 0.8488 - val_loss: 0.6424 - val_acc: 0.8343\n",
      "Learning rate:  0.001\n",
      "Epoch 79/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5788 - acc: 0.8217\n",
      "Epoch 00079: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.5841 - acc: 0.8206 - val_loss: 0.9071 - val_acc: 0.6423\n",
      "Learning rate:  0.001\n",
      "Epoch 80/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5474 - acc: 0.8422\n",
      "Epoch 00080: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.5454 - acc: 0.8427 - val_loss: 1.0110 - val_acc: 0.5912\n",
      "Learning rate:  0.001\n",
      "Epoch 81/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.6088 - acc: 0.8217\n",
      "Epoch 00081: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.6087 - acc: 0.8206 - val_loss: 0.8755 - val_acc: 0.6616\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5616 - acc: 0.8381\n",
      "Epoch 00082: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.5578 - acc: 0.8407 - val_loss: 0.8648 - val_acc: 0.6575\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.8439\n",
      "Epoch 00083: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 416ms/step - loss: 0.5481 - acc: 0.8465 - val_loss: 0.8571 - val_acc: 0.6588\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4924 - acc: 0.8586\n",
      "Epoch 00084: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 453ms/step - loss: 0.4881 - acc: 0.8609 - val_loss: 0.7421 - val_acc: 0.7169\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5215 - acc: 0.8645\n",
      "Epoch 00085: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.5182 - acc: 0.8646 - val_loss: 0.7392 - val_acc: 0.7348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0001\n",
      "Epoch 86/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.8648\n",
      "Epoch 00086: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.5097 - acc: 0.8609 - val_loss: 0.7072 - val_acc: 0.7472\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.5260 - acc: 0.8586\n",
      "Epoch 00087: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.5265 - acc: 0.8589 - val_loss: 0.6388 - val_acc: 0.7790\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4595 - acc: 0.8627\n",
      "Epoch 00088: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 452ms/step - loss: 0.4599 - acc: 0.8629 - val_loss: 0.6854 - val_acc: 0.7445\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4904 - acc: 0.8422\n",
      "Epoch 00089: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.4940 - acc: 0.8387 - val_loss: 0.6365 - val_acc: 0.7804\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.8730\n",
      "Epoch 00090: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.4553 - acc: 0.8730 - val_loss: 0.6770 - val_acc: 0.7693\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8832\n",
      "Epoch 00091: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.4293 - acc: 0.8851 - val_loss: 0.6395 - val_acc: 0.7707\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4863 - acc: 0.8686\n",
      "Epoch 00092: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 459ms/step - loss: 0.4842 - acc: 0.8687 - val_loss: 0.6865 - val_acc: 0.7500\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8811\n",
      "Epoch 00093: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 30s 477ms/step - loss: 0.4526 - acc: 0.8790 - val_loss: 0.7030 - val_acc: 0.7528\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.8750\n",
      "Epoch 00094: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 454ms/step - loss: 0.4460 - acc: 0.8770 - val_loss: 0.6388 - val_acc: 0.7776\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.8852\n",
      "Epoch 00095: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 443ms/step - loss: 0.4492 - acc: 0.8851 - val_loss: 0.7374 - val_acc: 0.7445\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4583 - acc: 0.8791\n",
      "Epoch 00096: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 451ms/step - loss: 0.4574 - acc: 0.8790 - val_loss: 0.6592 - val_acc: 0.7845\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4433 - acc: 0.8809\n",
      "Epoch 00097: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.4510 - acc: 0.8788 - val_loss: 0.6142 - val_acc: 0.7956\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.8791\n",
      "Epoch 00098: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 439ms/step - loss: 0.4536 - acc: 0.8770 - val_loss: 0.6445 - val_acc: 0.7790\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.8645\n",
      "Epoch 00099: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.4814 - acc: 0.8667 - val_loss: 0.6652 - val_acc: 0.7666\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4420 - acc: 0.8791\n",
      "Epoch 00100: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.4392 - acc: 0.8810 - val_loss: 0.5860 - val_acc: 0.8011\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8955\n",
      "Epoch 00101: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 458ms/step - loss: 0.4151 - acc: 0.8931 - val_loss: 0.6418 - val_acc: 0.7790\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8955\n",
      "Epoch 00102: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.4099 - acc: 0.8952 - val_loss: 0.6481 - val_acc: 0.7790\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4299 - acc: 0.8873\n",
      "Epoch 00103: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 30s 491ms/step - loss: 0.4260 - acc: 0.8891 - val_loss: 0.6434 - val_acc: 0.7928\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4895 - acc: 0.8522\n",
      "Epoch 00104: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.4899 - acc: 0.8505 - val_loss: 0.6967 - val_acc: 0.7693\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4514 - acc: 0.8832\n",
      "Epoch 00105: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 0.4544 - acc: 0.8831 - val_loss: 0.6870 - val_acc: 0.7555\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8873\n",
      "Epoch 00106: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.4186 - acc: 0.8891 - val_loss: 0.6857 - val_acc: 0.7666\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4610 - acc: 0.8750\n",
      "Epoch 00107: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.4569 - acc: 0.8770 - val_loss: 0.6750 - val_acc: 0.7583\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4581 - acc: 0.8914\n",
      "Epoch 00108: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 439ms/step - loss: 0.4538 - acc: 0.8931 - val_loss: 0.6455 - val_acc: 0.7790\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.8912\n",
      "Epoch 00109: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 427ms/step - loss: 0.4023 - acc: 0.8929 - val_loss: 0.5775 - val_acc: 0.8108\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3738 - acc: 0.9119\n",
      "Epoch 00110: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.3706 - acc: 0.9133 - val_loss: 0.5882 - val_acc: 0.8039\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8914\n",
      "Epoch 00111: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.4099 - acc: 0.8891 - val_loss: 0.6271 - val_acc: 0.7804\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.8566\n",
      "Epoch 00112: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.4572 - acc: 0.8548 - val_loss: 0.6102 - val_acc: 0.7942\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4065 - acc: 0.8975\n",
      "Epoch 00113: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 445ms/step - loss: 0.4067 - acc: 0.8972 - val_loss: 0.5377 - val_acc: 0.8315\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3694 - acc: 0.9078\n",
      "Epoch 00114: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.3736 - acc: 0.9073 - val_loss: 0.5215 - val_acc: 0.8287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0001\n",
      "Epoch 115/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4188 - acc: 0.8809\n",
      "Epoch 00115: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.4216 - acc: 0.8808 - val_loss: 0.5827 - val_acc: 0.8163\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4149 - acc: 0.8934\n",
      "Epoch 00116: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.4134 - acc: 0.8931 - val_loss: 0.6847 - val_acc: 0.7652\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4671 - acc: 0.8811\n",
      "Epoch 00117: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.4713 - acc: 0.8790 - val_loss: 0.6322 - val_acc: 0.7956\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8955\n",
      "Epoch 00118: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 452ms/step - loss: 0.4017 - acc: 0.8972 - val_loss: 0.6606 - val_acc: 0.7680\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4684 - acc: 0.8668\n",
      "Epoch 00119: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 439ms/step - loss: 0.4637 - acc: 0.8690 - val_loss: 0.7192 - val_acc: 0.7348\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8811\n",
      "Epoch 00120: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.3982 - acc: 0.8831 - val_loss: 0.6533 - val_acc: 0.7762\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4008 - acc: 0.8932\n",
      "Epoch 00121: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 455ms/step - loss: 0.3983 - acc: 0.8949 - val_loss: 0.5809 - val_acc: 0.8066\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8893\n",
      "Epoch 00122: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 443ms/step - loss: 0.4092 - acc: 0.8891 - val_loss: 0.5982 - val_acc: 0.8135\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8934\n",
      "Epoch 00123: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 29s 464ms/step - loss: 0.3765 - acc: 0.8931 - val_loss: 0.5724 - val_acc: 0.8163\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.9037\n",
      "Epoch 00124: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 30s 485ms/step - loss: 0.3748 - acc: 0.9052 - val_loss: 0.6084 - val_acc: 0.7956\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8871\n",
      "Epoch 00125: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 441ms/step - loss: 0.4346 - acc: 0.8869 - val_loss: 0.6154 - val_acc: 0.7970\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4009 - acc: 0.8873\n",
      "Epoch 00126: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 434ms/step - loss: 0.3997 - acc: 0.8871 - val_loss: 0.6010 - val_acc: 0.7901\n",
      "Learning rate:  1e-05\n",
      "Epoch 127/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.8750\n",
      "Epoch 00127: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 419ms/step - loss: 0.4347 - acc: 0.8730 - val_loss: 0.6355 - val_acc: 0.7693\n",
      "Learning rate:  1e-05\n",
      "Epoch 128/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3629 - acc: 0.9119\n",
      "Epoch 00128: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 450ms/step - loss: 0.3633 - acc: 0.9113 - val_loss: 0.6374 - val_acc: 0.7707\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.8893\n",
      "Epoch 00129: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 433ms/step - loss: 0.3991 - acc: 0.8911 - val_loss: 0.6266 - val_acc: 0.7873\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.9262\n",
      "Epoch 00130: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.3435 - acc: 0.9274 - val_loss: 0.6229 - val_acc: 0.7790\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.9055\n",
      "Epoch 00131: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.3986 - acc: 0.9071 - val_loss: 0.5815 - val_acc: 0.8094\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8832\n",
      "Epoch 00132: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.4257 - acc: 0.8851 - val_loss: 0.6166 - val_acc: 0.7914\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3512 - acc: 0.9119\n",
      "Epoch 00133: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.3490 - acc: 0.9133 - val_loss: 0.6297 - val_acc: 0.7652\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.9055\n",
      "Epoch 00134: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3680 - acc: 0.9051 - val_loss: 0.6360 - val_acc: 0.7804\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3606 - acc: 0.9057\n",
      "Epoch 00135: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 447ms/step - loss: 0.3577 - acc: 0.9073 - val_loss: 0.6265 - val_acc: 0.7776\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4220 - acc: 0.8955\n",
      "Epoch 00136: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.4185 - acc: 0.8972 - val_loss: 0.6193 - val_acc: 0.7831\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.8791\n",
      "Epoch 00137: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.4251 - acc: 0.8810 - val_loss: 0.5855 - val_acc: 0.8135\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3979 - acc: 0.9037\n",
      "Epoch 00138: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3971 - acc: 0.9052 - val_loss: 0.6128 - val_acc: 0.7983\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8955\n",
      "Epoch 00139: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.3791 - acc: 0.8952 - val_loss: 0.6207 - val_acc: 0.7914\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8955\n",
      "Epoch 00140: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.3780 - acc: 0.8931 - val_loss: 0.5976 - val_acc: 0.7818\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8975\n",
      "Epoch 00141: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 0.4169 - acc: 0.8972 - val_loss: 0.6292 - val_acc: 0.7873\n",
      "Learning rate:  1e-06\n",
      "Epoch 142/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8850\n",
      "Epoch 00142: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 443ms/step - loss: 0.4259 - acc: 0.8848 - val_loss: 0.5991 - val_acc: 0.8094\n",
      "Learning rate:  1e-06\n",
      "Epoch 143/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.9016\n",
      "Epoch 00143: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.3597 - acc: 0.9012 - val_loss: 0.6045 - val_acc: 0.8025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-06\n",
      "Epoch 144/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4352 - acc: 0.8914\n",
      "Epoch 00144: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.4366 - acc: 0.8911 - val_loss: 0.6068 - val_acc: 0.8011\n",
      "Learning rate:  1e-06\n",
      "Epoch 145/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4130 - acc: 0.8893\n",
      "Epoch 00145: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.4092 - acc: 0.8911 - val_loss: 0.6307 - val_acc: 0.7859\n",
      "Learning rate:  1e-06\n",
      "Epoch 146/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.9076\n",
      "Epoch 00146: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.3516 - acc: 0.9091 - val_loss: 0.6277 - val_acc: 0.7776\n",
      "Learning rate:  1e-06\n",
      "Epoch 147/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3726 - acc: 0.9221\n",
      "Epoch 00147: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 430ms/step - loss: 0.3708 - acc: 0.9214 - val_loss: 0.6505 - val_acc: 0.7804\n",
      "Learning rate:  1e-06\n",
      "Epoch 148/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.9057\n",
      "Epoch 00148: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.3730 - acc: 0.9073 - val_loss: 0.6008 - val_acc: 0.8066\n",
      "Learning rate:  1e-06\n",
      "Epoch 149/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3665 - acc: 0.9117\n",
      "Epoch 00149: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3665 - acc: 0.9111 - val_loss: 0.6297 - val_acc: 0.7721\n",
      "Learning rate:  1e-06\n",
      "Epoch 150/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4319 - acc: 0.8893\n",
      "Epoch 00150: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.4273 - acc: 0.8911 - val_loss: 0.6197 - val_acc: 0.7928\n",
      "Learning rate:  1e-06\n",
      "Epoch 151/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.9078\n",
      "Epoch 00151: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.3849 - acc: 0.9052 - val_loss: 0.5999 - val_acc: 0.8011\n",
      "Learning rate:  1e-06\n",
      "Epoch 152/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4356 - acc: 0.8727\n",
      "Epoch 00152: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 433ms/step - loss: 0.4316 - acc: 0.8747 - val_loss: 0.6184 - val_acc: 0.7887\n",
      "Learning rate:  1e-06\n",
      "Epoch 153/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8873\n",
      "Epoch 00153: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 441ms/step - loss: 0.3763 - acc: 0.8891 - val_loss: 0.6091 - val_acc: 0.7901\n",
      "Learning rate:  1e-06\n",
      "Epoch 154/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4014 - acc: 0.8975\n",
      "Epoch 00154: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.3974 - acc: 0.8992 - val_loss: 0.6212 - val_acc: 0.7859\n",
      "Learning rate:  1e-06\n",
      "Epoch 155/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4089 - acc: 0.8996\n",
      "Epoch 00155: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.4052 - acc: 0.9012 - val_loss: 0.6211 - val_acc: 0.7859\n",
      "Learning rate:  1e-06\n",
      "Epoch 156/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3861 - acc: 0.8914\n",
      "Epoch 00156: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.3872 - acc: 0.8911 - val_loss: 0.6194 - val_acc: 0.7887\n",
      "Learning rate:  1e-06\n",
      "Epoch 157/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.8730\n",
      "Epoch 00157: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 29s 460ms/step - loss: 0.4268 - acc: 0.8750 - val_loss: 0.6129 - val_acc: 0.7845\n",
      "Learning rate:  1e-06\n",
      "Epoch 158/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8809\n",
      "Epoch 00158: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 431ms/step - loss: 0.3894 - acc: 0.8808 - val_loss: 0.5986 - val_acc: 0.7970\n",
      "Learning rate:  1e-06\n",
      "Epoch 159/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8996\n",
      "Epoch 00159: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.3744 - acc: 0.9012 - val_loss: 0.6087 - val_acc: 0.7997\n",
      "Learning rate:  1e-06\n",
      "Epoch 160/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3882 - acc: 0.8975\n",
      "Epoch 00160: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.3872 - acc: 0.8972 - val_loss: 0.6043 - val_acc: 0.7997\n",
      "Learning rate:  1e-06\n",
      "Epoch 161/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.9201\n",
      "Epoch 00161: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.3773 - acc: 0.9173 - val_loss: 0.5817 - val_acc: 0.8080\n",
      "Learning rate:  5e-07\n",
      "Epoch 162/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3582 - acc: 0.8955\n",
      "Epoch 00162: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 446ms/step - loss: 0.3588 - acc: 0.8972 - val_loss: 0.5942 - val_acc: 0.7997\n",
      "Learning rate:  5e-07\n",
      "Epoch 163/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.9078\n",
      "Epoch 00163: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.3559 - acc: 0.9073 - val_loss: 0.6262 - val_acc: 0.7790\n",
      "Learning rate:  5e-07\n",
      "Epoch 164/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.9160\n",
      "Epoch 00164: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 433ms/step - loss: 0.3823 - acc: 0.9153 - val_loss: 0.6083 - val_acc: 0.7914\n",
      "Learning rate:  5e-07\n",
      "Epoch 165/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3985 - acc: 0.9035\n",
      "Epoch 00165: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 444ms/step - loss: 0.4008 - acc: 0.9030 - val_loss: 0.5999 - val_acc: 0.7914\n",
      "Learning rate:  5e-07\n",
      "Epoch 166/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.9057\n",
      "Epoch 00166: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.3837 - acc: 0.9052 - val_loss: 0.6169 - val_acc: 0.7818\n",
      "Learning rate:  5e-07\n",
      "Epoch 167/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8893\n",
      "Epoch 00167: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 0.3832 - acc: 0.8891 - val_loss: 0.5979 - val_acc: 0.7928\n",
      "Learning rate:  5e-07\n",
      "Epoch 168/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3885 - acc: 0.8934\n",
      "Epoch 00168: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.3854 - acc: 0.8952 - val_loss: 0.6206 - val_acc: 0.7901\n",
      "Learning rate:  5e-07\n",
      "Epoch 169/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8996\n",
      "Epoch 00169: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.3951 - acc: 0.8992 - val_loss: 0.6269 - val_acc: 0.7859\n",
      "Learning rate:  5e-07\n",
      "Epoch 170/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.9057- ETA: 3s - loss: 0.\n",
      "Epoch 00170: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 441ms/step - loss: 0.3504 - acc: 0.9073 - val_loss: 0.6355 - val_acc: 0.7887\n",
      "Learning rate:  5e-07\n",
      "Epoch 171/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8912\n",
      "Epoch 00171: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.4335 - acc: 0.8889 - val_loss: 0.6011 - val_acc: 0.8052\n",
      "Learning rate:  5e-07\n",
      "Epoch 172/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8912\n",
      "Epoch 00172: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 443ms/step - loss: 0.3919 - acc: 0.8909 - val_loss: 0.6283 - val_acc: 0.7749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  5e-07\n",
      "Epoch 173/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8914\n",
      "Epoch 00173: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 0.4024 - acc: 0.8891 - val_loss: 0.6201 - val_acc: 0.7790\n",
      "Learning rate:  5e-07\n",
      "Epoch 174/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8873\n",
      "Epoch 00174: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 423ms/step - loss: 0.3861 - acc: 0.8871 - val_loss: 0.5976 - val_acc: 0.7956\n",
      "Learning rate:  5e-07\n",
      "Epoch 175/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3935 - acc: 0.8955\n",
      "Epoch 00175: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.3927 - acc: 0.8952 - val_loss: 0.6342 - val_acc: 0.7845\n",
      "Learning rate:  5e-07\n",
      "Epoch 176/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.8852\n",
      "Epoch 00176: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 442ms/step - loss: 0.4285 - acc: 0.8871 - val_loss: 0.6218 - val_acc: 0.7845\n",
      "Learning rate:  5e-07\n",
      "Epoch 177/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8934\n",
      "Epoch 00177: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 0.3933 - acc: 0.8952 - val_loss: 0.6349 - val_acc: 0.7721\n",
      "Learning rate:  5e-07\n",
      "Epoch 178/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4113 - acc: 0.8934\n",
      "Epoch 00178: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.4091 - acc: 0.8952 - val_loss: 0.6307 - val_acc: 0.7914\n",
      "Learning rate:  5e-07\n",
      "Epoch 179/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3603 - acc: 0.9078\n",
      "Epoch 00179: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3601 - acc: 0.9093 - val_loss: 0.6129 - val_acc: 0.7928\n",
      "Learning rate:  5e-07\n",
      "Epoch 180/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4153 - acc: 0.8953\n",
      "Epoch 00180: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.4116 - acc: 0.8970 - val_loss: 0.5915 - val_acc: 0.7997\n",
      "Learning rate:  5e-07\n",
      "Epoch 181/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3525 - acc: 0.9201\n",
      "Epoch 00181: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.3567 - acc: 0.9194 - val_loss: 0.6381 - val_acc: 0.7859\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3583 - acc: 0.9139\n",
      "Epoch 00182: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 428ms/step - loss: 0.3569 - acc: 0.9133 - val_loss: 0.6298 - val_acc: 0.7956\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8893\n",
      "Epoch 00183: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.3920 - acc: 0.8911 - val_loss: 0.6145 - val_acc: 0.7997\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3876 - acc: 0.9078\n",
      "Epoch 00184: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 0.3916 - acc: 0.9073 - val_loss: 0.6127 - val_acc: 0.7983\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8955\n",
      "Epoch 00185: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 447ms/step - loss: 0.3921 - acc: 0.8972 - val_loss: 0.5993 - val_acc: 0.8025\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.9199\n",
      "Epoch 00186: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 447ms/step - loss: 0.3353 - acc: 0.9192 - val_loss: 0.5835 - val_acc: 0.8108\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.9139\n",
      "Epoch 00187: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.3438 - acc: 0.9133 - val_loss: 0.6196 - val_acc: 0.7942\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8955\n",
      "Epoch 00188: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3834 - acc: 0.8911 - val_loss: 0.6174 - val_acc: 0.7859\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8830\n",
      "Epoch 00189: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3995 - acc: 0.8828 - val_loss: 0.6293 - val_acc: 0.7859\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3588 - acc: 0.9201\n",
      "Epoch 00190: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 436ms/step - loss: 0.3602 - acc: 0.9173 - val_loss: 0.6187 - val_acc: 0.7914\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4121 - acc: 0.8932\n",
      "Epoch 00191: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 0.4139 - acc: 0.8929 - val_loss: 0.6210 - val_acc: 0.7845\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8893\n",
      "Epoch 00192: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.3952 - acc: 0.8891 - val_loss: 0.6178 - val_acc: 0.7914\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.9037\n",
      "Epoch 00193: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.3666 - acc: 0.9052 - val_loss: 0.6208 - val_acc: 0.7804\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3858 - acc: 0.9016\n",
      "Epoch 00194: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 447ms/step - loss: 0.3840 - acc: 0.9032 - val_loss: 0.6219 - val_acc: 0.7721\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3562 - acc: 0.9160\n",
      "Epoch 00195: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.3712 - acc: 0.9113 - val_loss: 0.6459 - val_acc: 0.7790\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.9220\n",
      "Epoch 00196: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 444ms/step - loss: 0.3478 - acc: 0.9192 - val_loss: 0.6238 - val_acc: 0.7928\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.8955\n",
      "Epoch 00197: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 0.4156 - acc: 0.8911 - val_loss: 0.5947 - val_acc: 0.7873\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8852\n",
      "Epoch 00198: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.3817 - acc: 0.8871 - val_loss: 0.6253 - val_acc: 0.7762\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3642 - acc: 0.9098\n",
      "Epoch 00199: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.3607 - acc: 0.9113 - val_loss: 0.6193 - val_acc: 0.7901\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/200\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8955\n",
      "Epoch 00200: val_acc did not improve from 0.85083\n",
      "62/62 [==============================] - 27s 437ms/step - loss: 0.3768 - acc: 0.8952 - val_loss: 0.6114 - val_acc: 0.7997\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "num_train_images = 1000\n",
    "\n",
    "def lrSchedule(epoch):\n",
    "    lr = 1e-3\n",
    "\n",
    "    if epoch > 160:\n",
    "        lr *= 0.5e-3\n",
    "\n",
    "    elif epoch > 140:\n",
    "        lr *= 1e-3\n",
    "\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "\n",
    "    print('Learning rate: ', lr)\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "LRScheduler = LearningRateScheduler(lrSchedule)\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ca2\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "callbacks_list = [checkpoint, LRScheduler]\n",
    "\n",
    "history = model.fit_generator(train_generator, \n",
    "                              validation_data=test_generator,\n",
    "                              epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 300, 300, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_conv (Conv2D)              (None, 300, 300, 16) 448         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_bn (BatchNormalization)    (None, 300, 300, 16) 64          Inpt_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Inpt_relu (Activation)          (None, 300, 300, 16) 0           Inpt_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Inpt_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_add (Add)             (None, 300, 300, 16) 0           Inpt_relu[0][0]                  \n",
      "                                                                 Stg1_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk1_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_add (Add)             (None, 300, 300, 16) 0           Stg1_Blk1_relu[0][0]             \n",
      "                                                                 Stg1_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk2_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res1_relu (Activation (None, 300, 300, 16) 0           Stg1_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_conv (Conv2D)    (None, 300, 300, 16) 2320        Stg1_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_Res2_bn (BatchNormali (None, 300, 300, 16) 64          Stg1_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_add (Add)             (None, 300, 300, 16) 0           Stg1_Blk2_relu[0][0]             \n",
      "                                                                 Stg1_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg1_Blk3_relu (Activation)     (None, 300, 300, 16) 0           Stg1_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 300, 300, 16) 0           Stg1_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_conv (Conv2D)    (None, 150, 150, 32) 4640        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_lin_conv (Conv2D)     (None, 150, 150, 32) 544         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg2_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk1_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk1_relu[0][0]             \n",
      "                                                                 Stg2_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk2_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res1_relu (Activation (None, 150, 150, 32) 0           Stg2_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_conv (Conv2D)    (None, 150, 150, 32) 9248        Stg2_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_Res2_bn (BatchNormali (None, 150, 150, 32) 128         Stg2_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_add (Add)             (None, 150, 150, 32) 0           Stg2_Blk2_relu[0][0]             \n",
      "                                                                 Stg2_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg2_Blk3_relu (Activation)     (None, 150, 150, 32) 0           Stg2_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 150, 150, 32) 0           Stg2_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_conv (Conv2D)    (None, 75, 75, 64)   18496       dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_lin_conv (Conv2D)     (None, 75, 75, 64)   2112        dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg3_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk1_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk1_relu[0][0]             \n",
      "                                                                 Stg3_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk2_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res1_relu (Activation (None, 75, 75, 64)   0           Stg3_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_conv (Conv2D)    (None, 75, 75, 64)   36928       Stg3_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_Res2_bn (BatchNormali (None, 75, 75, 64)   256         Stg3_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_add (Add)             (None, 75, 75, 64)   0           Stg3_Blk2_relu[0][0]             \n",
      "                                                                 Stg3_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg3_Blk3_relu (Activation)     (None, 75, 75, 64)   0           Stg3_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 75, 75, 64)   0           Stg3_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_conv (Conv2D)    (None, 38, 38, 128)  73856       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk1_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk1_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk1_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_lin_conv (Conv2D)     (None, 38, 38, 128)  8320        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk1_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk1_lin_conv[0][0]         \n",
      "                                                                 Stg4_Blk1_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk1_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk1_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk1_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk2_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk2_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk2_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk2_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk1_relu[0][0]             \n",
      "                                                                 Stg4_Blk2_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk2_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk2_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk2_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk3_Res1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res1_relu (Activation (None, 38, 38, 128)  0           Stg4_Blk3_Res1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_conv (Conv2D)    (None, 38, 38, 128)  147584      Stg4_Blk3_Res1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_Res2_bn (BatchNormali (None, 38, 38, 128)  512         Stg4_Blk3_Res2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_add (Add)             (None, 38, 38, 128)  0           Stg4_Blk2_relu[0][0]             \n",
      "                                                                 Stg4_Blk3_Res2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Stg4_Blk3_relu (Activation)     (None, 38, 38, 128)  0           Stg4_Blk3_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 38, 38, 128)  0           Stg4_Blk3_relu[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "AvgPool (AveragePooling2D)      (None, 4, 4, 128)    0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2048)         0           AvgPool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            6147        flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,103,107\n",
      "Trainable params: 1,100,195\n",
      "Non-trainable params: 2,912\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "\n",
    "optmz = optimizers.Adam(lr=0.001)\n",
    "\n",
    "def resLyr(inputs,\n",
    "           numFilters=16,\n",
    "           kernelSz=3,\n",
    "           strides=1,\n",
    "           activation='relu',\n",
    "           batchNorm=True,\n",
    "           convFirst=True,\n",
    "           lyrName=None):\n",
    "  \n",
    "    convLyr = Conv2D(numFilters,\n",
    "                     kernel_size=kernelSz,\n",
    "                     strides=strides,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4),\n",
    "                     name=lyrName + '_conv' if lyrName else None)\n",
    "    x = inputs\n",
    "    if convFirst:\n",
    "        x = convLyr(x)\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "    else:\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "        x = convLyr(x)\n",
    "  \n",
    "    return x\n",
    "\n",
    "\n",
    "def resBlkV1(inputs,\n",
    "             numFilters=16,\n",
    "             numBlocks=3,\n",
    "             downsampleOnFirst=True,\n",
    "             names=None):\n",
    "  \n",
    "    x = inputs\n",
    "    for run in range(0, numBlocks):\n",
    "        strides = 1\n",
    "        blkStr = str(run + 1)\n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            strides = 2\n",
    "            \n",
    "        y = resLyr(inputs=x, numFilters=numFilters, strides=strides, lyrName=names+'_Blk'+blkStr+'_Res1' if names else None)\n",
    "        y = resLyr(inputs=y, numFilters=numFilters, activation=None, lyrName=names+'_Blk'+blkStr+'_Res2' if names else None) \n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            x = resLyr(inputs=x, numFilters=numFilters, kernelSz=1, \n",
    "                       strides=strides, activation=None, batchNorm=False, \n",
    "                       lyrName=names+'_Blk'+blkStr+'_lin' if names else None)\n",
    "\n",
    "        x = add([x, y], name=names+'_Blk'+blkStr+'_add' if names else None) \n",
    "\n",
    "        x = Activation('relu',  name=names+'_Blk'+blkStr+'_relu' if names else None)(x)   \n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def createResNetV1(inputShape=(32, 32, 3),\n",
    "                   numClasses=3):\n",
    "  \n",
    "    inputs = Input(shape=inputShape)\n",
    "    v = resLyr(inputs,\n",
    "               lyrName='Inpt')\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=16,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=False,\n",
    "                 names='Stg1')\n",
    "    v = Dropout(0.2)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=32,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg2')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=64,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg3')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=128,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg4')    \n",
    "    v = Dropout(0.3)(v)\n",
    "    \n",
    "    v = AveragePooling2D(pool_size=8,\n",
    "                         name='AvgPool')(v)\n",
    "    v = Flatten()(v)\n",
    "    outputs = Dense(numClasses,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(v)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optmz,\n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "    return model\n",
    "\n",
    "model = createResNetV1(inputShape=(HEIGHT, WIDTH, 3))  # This is meant for training\n",
    "modelGo = createResNetV1(inputShape=(HEIGHT, WIDTH, 3))  # This is used for final testing\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 2.4129 - acc: 0.3922\n",
      "Epoch 00001: val_acc improved from -inf to 0.29558, saving model to ./checkpoints/ca2_v6_model_weights.h5\n",
      "62/62 [==============================] - 46s 741ms/step - loss: 2.3893 - acc: 0.4000 - val_loss: 2.4106 - val_acc: 0.2956\n",
      "Learning rate:  0.001\n",
      "Epoch 2/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.6799 - acc: 0.5082\n",
      "Epoch 00002: val_acc improved from 0.29558 to 0.38536, saving model to ./checkpoints/ca2_v6_model_weights.h5\n",
      "62/62 [==============================] - 25s 406ms/step - loss: 1.6666 - acc: 0.5101 - val_loss: 2.8113 - val_acc: 0.3854\n",
      "Learning rate:  0.001\n",
      "Epoch 3/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.7250 - acc: 0.5861\n",
      "Epoch 00003: val_acc improved from 0.38536 to 0.61464, saving model to ./checkpoints/ca2_v6_model_weights.h5\n",
      "62/62 [==============================] - 25s 403ms/step - loss: 1.7153 - acc: 0.5887 - val_loss: 3.9371 - val_acc: 0.6146\n",
      "Learning rate:  0.001\n",
      "Epoch 4/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.5721 - acc: 0.5943\n",
      "Epoch 00004: val_acc improved from 0.61464 to 0.61740, saving model to ./checkpoints/ca2_v6_model_weights.h5\n",
      "62/62 [==============================] - 27s 440ms/step - loss: 1.5656 - acc: 0.5968 - val_loss: 2.1572 - val_acc: 0.6174\n",
      "Learning rate:  0.001\n",
      "Epoch 5/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2560 - acc: 0.6639\n",
      "Epoch 00005: val_acc did not improve from 0.61740\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 1.2568 - acc: 0.6593 - val_loss: 1.8320 - val_acc: 0.6091\n",
      "Learning rate:  0.001\n",
      "Epoch 6/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.2858 - acc: 0.6468\n",
      "Epoch 00006: val_acc did not improve from 0.61740\n",
      "62/62 [==============================] - 26s 425ms/step - loss: 1.2790 - acc: 0.6505 - val_loss: 2.0260 - val_acc: 0.5953\n",
      "Learning rate:  0.001\n",
      "Epoch 7/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1028 - acc: 0.7008\n",
      "Epoch 00007: val_acc did not improve from 0.61740\n",
      "62/62 [==============================] - 26s 415ms/step - loss: 1.1123 - acc: 0.6935 - val_loss: 1.5311 - val_acc: 0.5566\n",
      "Learning rate:  0.001\n",
      "Epoch 8/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0708 - acc: 0.7090\n",
      "Epoch 00008: val_acc improved from 0.61740 to 0.66298, saving model to ./checkpoints/ca2_v6_model_weights.h5\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 1.0698 - acc: 0.7097 - val_loss: 1.3976 - val_acc: 0.6630\n",
      "Learning rate:  0.001\n",
      "Epoch 9/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0792 - acc: 0.7152\n",
      "Epoch 00009: val_acc did not improve from 0.66298\n",
      "62/62 [==============================] - 26s 419ms/step - loss: 1.0760 - acc: 0.7177 - val_loss: 1.8654 - val_acc: 0.5580\n",
      "Learning rate:  0.001\n",
      "Epoch 10/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0605 - acc: 0.7357\n",
      "Epoch 00010: val_acc improved from 0.66298 to 0.78729, saving model to ./checkpoints/ca2_v6_model_weights.h5\n",
      "62/62 [==============================] - 28s 449ms/step - loss: 1.0654 - acc: 0.7339 - val_loss: 0.9398 - val_acc: 0.7873\n",
      "Learning rate:  0.001\n",
      "Epoch 11/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.1207 - acc: 0.7049\n",
      "Epoch 00011: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 1.1216 - acc: 0.7036 - val_loss: 1.1926 - val_acc: 0.7555\n",
      "Learning rate:  0.001\n",
      "Epoch 12/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0955 - acc: 0.7248\n",
      "Epoch 00012: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 26s 415ms/step - loss: 1.0987 - acc: 0.7232 - val_loss: 1.7108 - val_acc: 0.5401\n",
      "Learning rate:  0.001\n",
      "Epoch 13/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0568 - acc: 0.6988\n",
      "Epoch 00013: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 26s 420ms/step - loss: 1.0639 - acc: 0.6976 - val_loss: 1.0414 - val_acc: 0.7500\n",
      "Learning rate:  0.001\n",
      "Epoch 14/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8904 - acc: 0.7848\n",
      "Epoch 00014: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 27s 429ms/step - loss: 0.8908 - acc: 0.7843 - val_loss: 1.0992 - val_acc: 0.6934\n",
      "Learning rate:  0.001\n",
      "Epoch 15/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9963 - acc: 0.7746\n",
      "Epoch 00015: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 27s 438ms/step - loss: 1.0037 - acc: 0.7681 - val_loss: 1.3398 - val_acc: 0.6464\n",
      "Learning rate:  0.001\n",
      "Epoch 16/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0944 - acc: 0.6660\n",
      "Epoch 00016: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 27s 427ms/step - loss: 1.0997 - acc: 0.6653 - val_loss: 0.9219 - val_acc: 0.7707\n",
      "Learning rate:  0.001\n",
      "Epoch 17/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9829 - acc: 0.7275\n",
      "Epoch 00017: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 26s 424ms/step - loss: 0.9791 - acc: 0.7278 - val_loss: 1.4338 - val_acc: 0.5925\n",
      "Learning rate:  0.001\n",
      "Epoch 18/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9634 - acc: 0.7500\n",
      "Epoch 00018: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 26s 421ms/step - loss: 0.9687 - acc: 0.7460 - val_loss: 1.3640 - val_acc: 0.6174\n",
      "Learning rate:  0.001\n",
      "Epoch 19/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0447 - acc: 0.6906\n",
      "Epoch 00019: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 1.0430 - acc: 0.6915 - val_loss: 1.5166 - val_acc: 0.5843\n",
      "Learning rate:  0.001\n",
      "Epoch 20/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9712 - acc: 0.7490\n",
      "Epoch 00020: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 27s 435ms/step - loss: 0.9683 - acc: 0.7510 - val_loss: 1.3230 - val_acc: 0.5677\n",
      "Learning rate:  0.001\n",
      "Epoch 21/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8914 - acc: 0.7725\n",
      "Epoch 00021: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.8859 - acc: 0.7762 - val_loss: 2.1142 - val_acc: 0.4724\n",
      "Learning rate:  0.001\n",
      "Epoch 22/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9022 - acc: 0.7480\n",
      "Epoch 00022: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 27s 443ms/step - loss: 0.8994 - acc: 0.7500 - val_loss: 2.0185 - val_acc: 0.5000\n",
      "Learning rate:  0.001\n",
      "Epoch 23/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7941 - acc: 0.8033\n",
      "Epoch 00023: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 28s 446ms/step - loss: 0.8071 - acc: 0.8004 - val_loss: 1.1626 - val_acc: 0.6492\n",
      "Learning rate:  0.001\n",
      "Epoch 24/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 1.0099 - acc: 0.7295\n",
      "Epoch 00024: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 27s 434ms/step - loss: 1.0055 - acc: 0.7298 - val_loss: 1.2707 - val_acc: 0.5925\n",
      "Learning rate:  0.001\n",
      "Epoch 25/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8855 - acc: 0.7659\n",
      "Epoch 00025: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 29s 475ms/step - loss: 0.8812 - acc: 0.7697 - val_loss: 1.4444 - val_acc: 0.5387\n",
      "Learning rate:  0.001\n",
      "Epoch 26/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8385 - acc: 0.7725\n",
      "Epoch 00026: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 26s 426ms/step - loss: 0.8521 - acc: 0.7722 - val_loss: 1.0192 - val_acc: 0.6865\n",
      "Learning rate:  0.001\n",
      "Epoch 27/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8925 - acc: 0.7746\n",
      "Epoch 00027: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 26s 422ms/step - loss: 0.8991 - acc: 0.7742 - val_loss: 1.1105 - val_acc: 0.7044\n",
      "Learning rate:  0.001\n",
      "Epoch 28/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8542 - acc: 0.7725\n",
      "Epoch 00028: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 26s 427ms/step - loss: 0.8544 - acc: 0.7722 - val_loss: 1.6195 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 29/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.9007 - acc: 0.7459\n",
      "Epoch 00029: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 27s 432ms/step - loss: 0.9058 - acc: 0.7399 - val_loss: 1.0207 - val_acc: 0.7293\n",
      "Learning rate:  0.001\n",
      "Epoch 30/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8406 - acc: 0.8012\n",
      "Epoch 00030: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 28s 448ms/step - loss: 0.8373 - acc: 0.8024 - val_loss: 1.2589 - val_acc: 0.6423\n",
      "Learning rate:  0.001\n",
      "Epoch 31/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8121 - acc: 0.8033\n",
      "Epoch 00031: val_acc did not improve from 0.78729\n",
      "62/62 [==============================] - 27s 439ms/step - loss: 0.8074 - acc: 0.8044 - val_loss: 1.5434 - val_acc: 0.5539\n",
      "Learning rate:  0.001\n",
      "Epoch 32/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8236 - acc: 0.7725\n",
      "Epoch 00032: val_acc improved from 0.78729 to 0.79834, saving model to ./checkpoints/ca2_v6_model_weights.h5\n",
      "62/62 [==============================] - 27s 434ms/step - loss: 0.8219 - acc: 0.7722 - val_loss: 0.9679 - val_acc: 0.7983\n",
      "Learning rate:  0.001\n",
      "Epoch 33/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8053 - acc: 0.7889\n",
      "Epoch 00033: val_acc did not improve from 0.79834\n",
      "62/62 [==============================] - 27s 438ms/step - loss: 0.8004 - acc: 0.7923 - val_loss: 1.1960 - val_acc: 0.6685\n",
      "Learning rate:  0.001\n",
      "Epoch 34/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8432 - acc: 0.7988\n",
      "Epoch 00034: val_acc did not improve from 0.79834\n",
      "62/62 [==============================] - 27s 433ms/step - loss: 0.8397 - acc: 0.8000 - val_loss: 0.7828 - val_acc: 0.7818\n",
      "Learning rate:  0.001\n",
      "Epoch 35/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.7916 - acc: 0.7910\n",
      "Epoch 00035: val_acc did not improve from 0.79834\n",
      "62/62 [==============================] - 28s 444ms/step - loss: 0.7910 - acc: 0.7923 - val_loss: 1.4974 - val_acc: 0.5718\n",
      "Learning rate:  0.001\n",
      "Epoch 36/100\n",
      "61/62 [============================>.] - ETA: 0s - loss: 0.8315 - acc: 0.7889"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "num_train_images = 1000\n",
    "\n",
    "def lrSchedule(epoch):\n",
    "    lr = 1e-3\n",
    "\n",
    "    if epoch > 160:\n",
    "        lr *= 0.5e-3\n",
    "\n",
    "    elif epoch > 140:\n",
    "        lr *= 1e-3\n",
    "\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "\n",
    "    elif epoch > 70:\n",
    "        lr *= 1e-1\n",
    "\n",
    "    print('Learning rate: ', lr)\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "LRScheduler = LearningRateScheduler(lrSchedule)\n",
    "\n",
    "filepath=\"./checkpoints/\" + \"ca2_v6\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "callbacks_list = [checkpoint, LRScheduler]\n",
    "\n",
    "history = model.fit_generator(train_generator, \n",
    "                              validation_data=test_generator,\n",
    "                              epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGo.load_weights(filepath)\n",
    "modelGo.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optmz,\n",
    "                metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}