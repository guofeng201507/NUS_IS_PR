{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "# plt.style.use('ggplot')\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Dense, Dropout, LSTM, Input\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "filterwarnings('ignore')\n",
    "# % matplotlib\n",
    "# inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set data folder path\n",
    "# DATA_FOLDER = 'D:\\\\NUS\\\\semester 2\\\\Course 3\\\\CA\\\\MAREA_dataset'\n",
    "# DATA_FOLDER = '/Users/jiahao/Downloads/MAREA_dataset'\n",
    "DATA_FOLDER = r'D:/NUS_TERM2_CA3/MAREA_dataset'\n",
    "# DATA_FOLDER = 'C:/Users/david/Documents/CA3/MAREA_dataset-201/MAREA_dataset'\n",
    "ACTIVITY_FOLDER = os.path.join(DATA_FOLDER, 'Activity Timings')\n",
    "SUBJECT_FOLDER = os.path.join(DATA_FOLDER, 'Subject Data_txt format')\n",
    "PROCESSED_FOLDER = os.path.join(DATA_FOLDER, 'Processed_data')\n",
    "\n",
    "# define activity timing labels\n",
    "label_indoor = ['tread_flat_walk_start',\n",
    "                'tread_flat_walk_end',\n",
    "                'tread_flat_run_end',\n",
    "                'tread_slope_walk_start',\n",
    "                'tread_slope_walk_end',\n",
    "                'indoor_flat_walk_start',\n",
    "                'indoor_flat_walk_end',\n",
    "                'indoor_flat_run_end']\n",
    "\n",
    "label_outdoor = ['outdoor_walk_start',\n",
    "                 'outdoor_walk_end',\n",
    "                 'outdoor_run_end']\n",
    "\n",
    "# prepare timing index for different activities\n",
    "df_indoor_time = pd.read_csv(os.path.join(ACTIVITY_FOLDER, 'Indoor Experiment Timings.txt')\n",
    "                             , names=label_indoor)\n",
    "\n",
    "df_outdoor_time = pd.read_csv(os.path.join(ACTIVITY_FOLDER, 'Outdoor Experiment Timings.txt')\n",
    "                              , names=label_outdoor)\n",
    "\n",
    "df_indoor_time[\"subject\"] = [\"Sub\" + str(i) for i in range(1, 12)]\n",
    "df_outdoor_time[\"subject\"] = [\"Sub\" + str(j) for j in range(12, 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set up activity column names\n",
    "axis_list = ['accX', 'accY', 'accZ']\n",
    "pos_list = ['LF', 'RF', 'Waist', 'Wrist']\n",
    "sub_list = ['Sub' + str(i) for i in range(1, 21)]\n",
    "column_names = [f\"{y}_{x}\" for x, y in itertools.product(pos_list, axis_list)]\n",
    "\n",
    "# TODO: purposely exclude subject 4 first as missing data -- dont know how to deal with missing data for signal\n",
    "sub_list.remove('Sub4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accX_LF',\n",
       " 'accY_LF',\n",
       " 'accZ_LF',\n",
       " 'accX_RF',\n",
       " 'accY_RF',\n",
       " 'accZ_RF',\n",
       " 'accX_Waist',\n",
       " 'accY_Waist',\n",
       " 'accZ_Waist',\n",
       " 'accX_Wrist',\n",
       " 'accY_Wrist',\n",
       " 'accZ_Wrist']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create master dataframe\n",
    "const_master_df = pd.DataFrame()\n",
    "for sub in sub_list:\n",
    "    df_lf = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'LF.txt'))\n",
    "    df_rf = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'RF.txt'))\n",
    "    df_waist = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'Waist.txt'))\n",
    "    df_wrist = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'Wrist.txt'))\n",
    "    df_sub = pd.concat([df_lf, df_rf, df_waist, df_wrist], axis=1)\n",
    "    df_sub.columns = column_names\n",
    "\n",
    "    df_sub = df_sub.copy()\n",
    "    n = int(sub[3:])\n",
    "    if n > 11:\n",
    "        sub_row = df_outdoor_time[df_outdoor_time['subject'] == sub]\n",
    "        tmp = sub_row.iloc[0]\n",
    "        df_sub.loc[0:tmp['outdoor_walk_end'], 'label'] = 'outdoor_walk'\n",
    "        df_sub.loc[tmp['outdoor_walk_end']: tmp['outdoor_run_end'], 'label'] = 'outdoor_run'\n",
    "    else:\n",
    "        sub_row = df_indoor_time[df_indoor_time['subject'] == sub]\n",
    "        tmp = sub_row.iloc[0]\n",
    "        df_sub.loc[0:tmp['tread_flat_walk_end'], 'label'] = 'tread_flat_walk'\n",
    "        df_sub.loc[tmp['tread_flat_walk_end']: tmp['tread_flat_run_end'], 'label'] = 'tread_flat_run'\n",
    "        df_sub.loc[tmp['tread_flat_run_end']: tmp['tread_slope_walk_start'], 'label'] = 'rest'\n",
    "        df_sub.loc[tmp['tread_slope_walk_start']: tmp['tread_slope_walk_end'], 'label'] = 'tread_slope_walk'\n",
    "        df_sub.loc[tmp['tread_slope_walk_end']: tmp['indoor_flat_walk_start'], 'label'] = 'rest'\n",
    "        df_sub.loc[tmp['indoor_flat_walk_start']: tmp['indoor_flat_walk_end'], 'label'] = 'indoor_flat_walk'\n",
    "        df_sub.loc[tmp['indoor_flat_walk_end']: tmp['indoor_flat_run_end'], 'label'] = 'indoor_flat_run'\n",
    "\n",
    "    df_sub['subject'] = sub\n",
    "    const_master_df = const_master_df.append(df_sub)\n",
    "    # print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accX_LF</th>\n",
       "      <th>accY_LF</th>\n",
       "      <th>accZ_LF</th>\n",
       "      <th>accX_RF</th>\n",
       "      <th>accY_RF</th>\n",
       "      <th>accZ_RF</th>\n",
       "      <th>accX_Waist</th>\n",
       "      <th>accY_Waist</th>\n",
       "      <th>accZ_Waist</th>\n",
       "      <th>accX_Wrist</th>\n",
       "      <th>accY_Wrist</th>\n",
       "      <th>accZ_Wrist</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.196</td>\n",
       "      <td>-11.765</td>\n",
       "      <td>-1.569</td>\n",
       "      <td>4.078</td>\n",
       "      <td>-6.902</td>\n",
       "      <td>2.196</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-8.157</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.941</td>\n",
       "      <td>-11.451</td>\n",
       "      <td>-2.510</td>\n",
       "      <td>tread_flat_walk</td>\n",
       "      <td>Sub1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.510</td>\n",
       "      <td>-10.353</td>\n",
       "      <td>-2.039</td>\n",
       "      <td>4.392</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>1.255</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.627</td>\n",
       "      <td>-10.510</td>\n",
       "      <td>-2.353</td>\n",
       "      <td>tread_flat_walk</td>\n",
       "      <td>Sub1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.353</td>\n",
       "      <td>-10.824</td>\n",
       "      <td>-2.510</td>\n",
       "      <td>4.863</td>\n",
       "      <td>-8.314</td>\n",
       "      <td>0.941</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-7.843</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.784</td>\n",
       "      <td>-10.196</td>\n",
       "      <td>-2.039</td>\n",
       "      <td>tread_flat_walk</td>\n",
       "      <td>Sub1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.882</td>\n",
       "      <td>-10.667</td>\n",
       "      <td>-2.667</td>\n",
       "      <td>4.549</td>\n",
       "      <td>-9.412</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-8.000</td>\n",
       "      <td>1.412</td>\n",
       "      <td>1.255</td>\n",
       "      <td>-10.039</td>\n",
       "      <td>-1.412</td>\n",
       "      <td>tread_flat_walk</td>\n",
       "      <td>Sub1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.412</td>\n",
       "      <td>-10.353</td>\n",
       "      <td>-2.824</td>\n",
       "      <td>3.451</td>\n",
       "      <td>-10.353</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>-0.627</td>\n",
       "      <td>-7.529</td>\n",
       "      <td>1.882</td>\n",
       "      <td>1.569</td>\n",
       "      <td>-10.196</td>\n",
       "      <td>-1.255</td>\n",
       "      <td>tread_flat_walk</td>\n",
       "      <td>Sub1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accX_LF  accY_LF  accZ_LF  accX_RF  accY_RF  accZ_RF  accX_Waist  \\\n",
       "0   -2.196  -11.765   -1.569    4.078   -6.902    2.196      -0.157   \n",
       "1   -2.510  -10.353   -2.039    4.392   -8.000    1.255      -0.157   \n",
       "2   -2.353  -10.824   -2.510    4.863   -8.314    0.941      -0.157   \n",
       "3   -1.882  -10.667   -2.667    4.549   -9.412   -0.157      -0.157   \n",
       "4   -1.412  -10.353   -2.824    3.451  -10.353   -0.784      -0.627   \n",
       "\n",
       "   accY_Waist  accZ_Waist  accX_Wrist  accY_Wrist  accZ_Wrist  \\\n",
       "0      -8.157       0.784       0.941     -11.451      -2.510   \n",
       "1      -8.000       1.255       0.627     -10.510      -2.353   \n",
       "2      -7.843       1.255       0.784     -10.196      -2.039   \n",
       "3      -8.000       1.412       1.255     -10.039      -1.412   \n",
       "4      -7.529       1.882       1.569     -10.196      -1.255   \n",
       "\n",
       "             label subject  \n",
       "0  tread_flat_walk    Sub1  \n",
       "1  tread_flat_walk    Sub1  \n",
       "2  tread_flat_walk    Sub1  \n",
       "3  tread_flat_walk    Sub1  \n",
       "4  tread_flat_walk    Sub1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_master_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def PreprocessingSignal(df, label, subject, feature, window,\n",
    "                        wavelet_args={\"type\": \"Y\",\n",
    "                                      \"threshold\": 2,\n",
    "                                      \"wavedec_options\": {\"wavelet\": \"db4\", \"level\": 2},\n",
    "                                      \"waverec_options\": {\"wavelet\": \"db4\"}},\n",
    "                        window_args={\"type\": \"no_overlap\"}\n",
    "                        ):\n",
    "    df = df.loc[(df['label'] == label) & (df['subject'] == subject)]\n",
    "\n",
    "    ### Do wavelet transform or NOT ###\n",
    "    if wavelet_args[\"type\"] == \"Y\":\n",
    "        # wavelet_args = {\"threshold\":2, \"options\":{\"wavelet\":\"db4\", \"level\":0.8}}\n",
    "        # Do wavelet transform\n",
    "        signal_orig = df[feature].values\n",
    "        args1 = wavelet_args[\"wavedec_options\"]\n",
    "        coeffs_orig = pywt.wavedec(signal_orig, **args1)\n",
    "        coeffs_filter = coeffs_orig.copy()\n",
    "        threshold = wavelet_args[\"threshold\"]\n",
    "        for i in range(1, len(coeffs_orig)):\n",
    "            coeffs_filter[i] = pywt.threshold(coeffs_orig[i], threshold * max(coeffs_orig[i]))\n",
    "        args2 = wavelet_args[\"waverec_options\"]\n",
    "        signal_denoised = pywt.waverec(coeffs_filter, **args2)\n",
    "        to_process_df = pd.DataFrame(signal_denoised)\n",
    "    else:\n",
    "        tmp_df = df[feature].reset_index()\n",
    "        to_process_df = tmp_df.drop(columns=[\"index\"])\n",
    "        to_process_df.columns = [0]\n",
    "    ### Do wavelet transform or NOT ###\n",
    "\n",
    "    min_index = min(to_process_df.index)\n",
    "    max_index = max(to_process_df.index)\n",
    "\n",
    "    ### Define Method to cut signal into windows ###\n",
    "    if window_args[\"type\"] == \"no_overlap\":\n",
    "        # window_args = {\"type\":\"no_overlap\"}\n",
    "        index_list = range(min_index, max_index + 1, int(window))\n",
    "    elif window_args[\"type\"] == \"with_overlap\":\n",
    "        # window_args = {\"type\":\"with_overlap\", \"overlap_perc\":0.5}\n",
    "        overlap_perc = window_args[\"overlap_perc\"]\n",
    "        step = int(window / (1 / overlap_perc))\n",
    "        index_list = range(min_index, max_index + 1, step)\n",
    "    elif window_args[\"type\"] == \"by_peaks\":\n",
    "        index_list = window_args[\"peaks_index\"]\n",
    "    ### Define Method to cut signal into windows ###\n",
    "\n",
    "    ### Cut signal into windows ###\n",
    "    windowed_selected_chunk_array = []\n",
    "    for index in index_list:\n",
    "        windowed_selected_chunk = to_process_df[0].iloc[index:index + window]\n",
    "        if windowed_selected_chunk.shape[0] == window:\n",
    "            windowed_selected_chunk_array.append(windowed_selected_chunk.values)\n",
    "    output_np_arr = np.array(windowed_selected_chunk_array)\n",
    "    ### Cut signal into windows ###\n",
    "\n",
    "    output_np_label = np.asarray([label] * output_np_arr.shape[0])\n",
    "\n",
    "    return output_np_arr, output_np_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# feature_list = column_names TODO: is this line still needed?\n",
    "\n",
    "# Create\n",
    "const_indoor_activity = ['rest',\n",
    "                         'tread_flat_walk',\n",
    "                         'tread_flat_run',\n",
    "                         'tread_slope_walk',\n",
    "                         'indoor_flat_walk',\n",
    "                         'indoor_flat_run'\n",
    "                         ]\n",
    "const_outdoor_activity = ['outdoor_walk', 'outdoor_run']\n",
    "\n",
    "const_indoor_sub = ['Sub' + str(i) for i in range(1, 12)]\n",
    "const_outdoor_sub = ['Sub' + str(i) for i in range(12, 21)]\n",
    "const_indoor_sub.remove('Sub4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def PrepareFeature(df, feature, split_method, window, wavelet_args, window_args, train_size=0.8):\n",
    "    X_train = pd.DataFrame()\n",
    "    X_test = pd.DataFrame()\n",
    "    y_train = pd.DataFrame()\n",
    "    y_test = pd.DataFrame()\n",
    "\n",
    "    # TODO: find_peaks from a given column\n",
    "    # NOTE: code will break when using -> window_args = {\"type\":\"by_peaks\", \"peaks_index\":peaks_index_list}\n",
    "\n",
    "    # method 1: split within each subject\n",
    "    if split_method == 'TrainTestSplitWithinSubject':\n",
    "        # indoor activity\n",
    "        for sub in const_indoor_sub:\n",
    "            for activity in const_indoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "\n",
    "                train_len = int(train_size * len(output_np_arr))\n",
    "                tr_x = output_np_arr[:train_len, :]\n",
    "                ts_x = output_np_arr[train_len:len(output_np_arr), :]\n",
    "\n",
    "                test_len = int(train_size * len(output_label))\n",
    "                tr_y = output_label[:test_len]\n",
    "                ts_y = output_label[test_len:len(output_label)]\n",
    "\n",
    "                X_train = X_train.append(pd.DataFrame(tr_x))\n",
    "                X_test = X_test.append(pd.DataFrame(ts_x))\n",
    "                y_train = y_train.append(pd.DataFrame(tr_y))\n",
    "                y_test = y_test.append(pd.DataFrame(ts_y))\n",
    "\n",
    "                # outdoor activity\n",
    "        for sub in const_outdoor_sub:\n",
    "            for activity in const_outdoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "\n",
    "                train_len = int(train_size * len(output_np_arr))\n",
    "                tr_x = output_np_arr[:train_len, :]\n",
    "                ts_x = output_np_arr[train_len:len(output_np_arr), :]\n",
    "\n",
    "                test_len = int(train_size * len(output_label))\n",
    "                tr_y = output_label[:test_len]\n",
    "                ts_y = output_label[test_len:len(output_label)]\n",
    "\n",
    "                X_train = X_train.append(pd.DataFrame(tr_x))\n",
    "                X_test = X_test.append(pd.DataFrame(ts_x))\n",
    "                y_train = y_train.append(pd.DataFrame(tr_y))\n",
    "                y_test = y_test.append(pd.DataFrame(ts_y))\n",
    "\n",
    "    # method 2: split all samples randomly\n",
    "    if split_method == 'Random':\n",
    "        #             output = np.asarray(list())\n",
    "        #             label = np.asarray(list())\n",
    "        output = np.empty((0, window), float)\n",
    "        label = np.empty((0,), float)\n",
    "\n",
    "        # indoor activity\n",
    "        for sub in const_indoor_sub:\n",
    "            for activity in const_indoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                output = np.append(output, output_np_arr, axis=0)\n",
    "                label = np.append(label, output_label, axis=0)\n",
    "\n",
    "        # outdoor activity:\n",
    "        for sub in const_outdoor_sub:\n",
    "            for activity in const_outdoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                output = np.append(output, output_np_arr, axis=0)\n",
    "                label = np.append(label, output_label, axis=0)\n",
    "\n",
    "        train_len = int(train_size * len(output))\n",
    "        tr_x = output[:train_len, :]\n",
    "        ts_x = output[train_len:len(output), :]\n",
    "\n",
    "        test_len = int(train_size * len(label))\n",
    "        tr_y = label[:test_len]\n",
    "        ts_y = label[test_len:len(label)]\n",
    "\n",
    "        X_train = X_train.append(pd.DataFrame(tr_x))\n",
    "        X_test = X_test.append(pd.DataFrame(ts_x))\n",
    "        y_train = y_train.append(pd.DataFrame(tr_y))\n",
    "        y_test = y_test.append(pd.DataFrame(ts_y))\n",
    "\n",
    "    # method 3: only keep several subjects in the train,\n",
    "    #           put other subjects in the test\n",
    "    if split_method == 'DifferentSubjectsInTrainTest':\n",
    "        train_sub = np.empty((0, window), float)\n",
    "        train_label_sub = np.empty((0,), float)\n",
    "        test_sub = np.empty((0, window), float)\n",
    "        test_label_sub = np.empty((0,), float)\n",
    "\n",
    "        # indoor activity\n",
    "        indoor_sub_train_len = int(train_size * len(const_indoor_sub))\n",
    "        indoor_sub_train = const_indoor_sub[:indoor_sub_train_len]\n",
    "        indoor_sub_test = const_indoor_sub[indoor_sub_train_len:len(const_indoor_sub)]\n",
    "\n",
    "        for sub in indoor_sub_train:\n",
    "            for activity in const_indoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    # print(\"THIS\", activity, peaks_index_list)\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                train_sub = np.append(train_sub, output_np_arr, axis=0)\n",
    "                train_label_sub = np.append(train_label_sub, output_label, axis=0)\n",
    "\n",
    "        for sub in indoor_sub_test:\n",
    "            for activity in const_indoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                test_sub = np.append(test_sub, output_np_arr, axis=0)\n",
    "                test_label_sub = np.append(test_label_sub, output_label, axis=0)\n",
    "\n",
    "        # outdoor activity\n",
    "        outdoor_sub_train_len = int(train_size * len(const_outdoor_sub))\n",
    "        outdoor_sub_train = const_outdoor_sub[:outdoor_sub_train_len]\n",
    "        outdoor_sub_test = const_outdoor_sub[outdoor_sub_train_len:len(const_outdoor_sub)]\n",
    "\n",
    "        for sub in outdoor_sub_train:\n",
    "            for activity in const_outdoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                train_sub = np.append(train_sub, output_np_arr, axis=0)\n",
    "                train_label_sub = np.append(train_label_sub, output_label, axis=0)\n",
    "\n",
    "        for sub in outdoor_sub_test:\n",
    "            for activity in const_outdoor_activity:\n",
    "                if window_args[\"type\"] == \"by_peaks\":\n",
    "                    data_df = df.loc[(df['label'] == activity) & (df['subject'] == sub)]\n",
    "                    args = window_args[\"find_peaks_options\"]\n",
    "                    peak_col = feature\n",
    "                    peaks_index_list = list(find_peaks(data_df[peak_col], **args)[0])\n",
    "                    window_args[\"peaks_index\"] = peaks_index_list\n",
    "                output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n",
    "                                                                  window_args)\n",
    "                test_sub = np.append(test_sub, output_np_arr, axis=0)\n",
    "                test_label_sub = np.append(test_label_sub, output_label, axis=0)\n",
    "\n",
    "        X_train = pd.DataFrame(train_sub)\n",
    "        y_train = pd.DataFrame(train_label_sub)\n",
    "        X_test = pd.DataFrame(test_sub)\n",
    "        y_test = pd.DataFrame(test_label_sub)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SAMPLE #\n",
    "\n",
    "def prepare_single_feature_to_csv(feature='accY_LF', model_name='v1', split_method='TrainTestSplitWithinSubject', \n",
    "                                  window=512, wavelet_args={\"type\": \"N\"},\n",
    "                                  window_args={\"type\": \"by_peaks\", \"find_peaks_col\": \"accX_LF\",\n",
    "                                               \"find_peaks_options\": {\"prominence\": 30, \"height\": 20}}):\n",
    "    # feature = 'accY_LF'\n",
    "    # feature_list = column_names\n",
    "    # for feature in column_names:\n",
    "    # (example) feature = accX_LF (which is stored in the column_names)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = PrepareFeature(const_master_df, feature, split_method, window, wavelet_args,\n",
    "                                                      window_args)\n",
    "\n",
    "    X_train_filename = \"_\".join(\n",
    "        ['X_train', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "    X_test_filename = \"_\".join(\n",
    "        ['X_test', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "    y_train_filename = \"_\".join(\n",
    "        ['y_train', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "    y_test_filename = \"_\".join(\n",
    "        ['y_test', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "\n",
    "    WORK_FOLDER = r'D:/NUS_TERM2_CA3/Models'\n",
    "    MODEL_FOLDER = os.path.join(WORK_FOLDER, model_name) \n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        os.makedirs(MODEL_FOLDER)\n",
    "    \n",
    "    X_train.to_csv(os.path.join(MODEL_FOLDER, X_train_filename), header=None, index=False, sep='\\t')\n",
    "    X_test.to_csv(os.path.join(MODEL_FOLDER, X_test_filename), header=None, index=False, sep='\\t')\n",
    "\n",
    "    pd.DataFrame(y_train).to_csv(os.path.join(MODEL_FOLDER,y_train_filename), header=None, index=False, sep='\\t')\n",
    "    pd.DataFrame(y_test).to_csv(os.path.join(MODEL_FOLDER,y_test_filename), header=None, index=False, sep='\\t')\n",
    "\n",
    "    print(\"Saved X_train to %s\" % (X_train_filename))\n",
    "    print(\"Saved X_test to %s\" % (X_test_filename))\n",
    "    print(\"Saved X_train to %s\" % (y_train_filename))\n",
    "    print(\"Saved X_test to %s\" % (y_test_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_group(filenames):\n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(name)\n",
    "        loaded.append(data)\n",
    "\n",
    "    loaded = np.dstack(loaded)\n",
    "    return loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(feature_list, window, wavelet_args, window_args, split_method):\n",
    "    # X_train, y_train = load_dataset_group('train')  # load_group(X_filenames), load_file(y_filename)\n",
    "    # X_test, y_test = load_dataset_group('test')\n",
    "\n",
    "    X_train_filenames = []\n",
    "    X_test_filenames = []\n",
    "\n",
    "    for feature in [feature_list]:\n",
    "        X_train_filenames.append(\"_\".join(\n",
    "            ['X_train', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "             split_method]) + '.txt')\n",
    "\n",
    "        X_test_filenames.append(\"_\".join(\n",
    "            ['X_test', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "             split_method]) + '.txt')\n",
    "\n",
    "    # X_train_filenames = ['X_train_accY_LF_Window512_WaveletN_by_peaks_TrainTestSplitWithinSubject.txt']\n",
    "    # y_train_filename = 'y_train_accY_LF_Window512_WaveletN_by_peaks_TrainTestSplitWithinSubject.txt'\n",
    "\n",
    "    y_train_filename = \"_\".join(\n",
    "        ['y_train', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "\n",
    "    y_test_filename = \"_\".join(\n",
    "        ['y_test', feature, \"Window\" + str(window), \"Wavelet\" + wavelet_args[\"type\"], window_args[\"type\"],\n",
    "         split_method]) + '.txt'\n",
    "\n",
    "    X_train = load_group(X_train_filenames)\n",
    "    y_train = load_file(y_train_filename)\n",
    "\n",
    "    # X_test_filenames = ['X_test_accY_LF_Window512_WaveletN_by_peaks_TrainTestSplitWithinSubject.txt']\n",
    "    # y_test_filename = 'y_test_accY_LF_Window512_WaveletN_by_peaks_TrainTestSplitWithinSubject.txt'\n",
    "    X_test = load_group(X_test_filenames)\n",
    "    y_test = load_file(y_test_filename)\n",
    "\n",
    "    activity_to_num_mapping = {\n",
    "        \"rest\": 0,\n",
    "        # indoor\n",
    "        \"tread_flat_walk\": 1,\n",
    "        \"tread_flat_run\": 2,\n",
    "        \"tread_slope_walk\": 3,\n",
    "        \"indoor_flat_walk\": 4,\n",
    "        \"indoor_flat_run\": 5,\n",
    "        # outdoor\n",
    "        \"outdoor_walk\": 6,\n",
    "        \"outdoor_run\": 7\n",
    "    }\n",
    "\n",
    "    y_train = np.vectorize(activity_to_num_mapping.get)(y_train)\n",
    "    y_test = np.vectorize(activity_to_num_mapping.get)(y_test)\n",
    "\n",
    "    # convert to binary class matrix\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create LSTM Model\n",
    "def createLSTMModel(n_timesteps, n_features, n_outputs):\n",
    "    ipt = Input(shape=(n_timesteps, n_features))\n",
    "    x = LSTM(100)(ipt)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(n_outputs, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=ipt, outputs=x)\n",
    "\n",
    "    # Sequential API\n",
    "    # model = Sequential()\n",
    "    # model.add(LSTM(100, input_shape=(n_timesteps, n_features)))\n",
    "    # model.add(Dropout(0.5))\n",
    "    # model.add(Dense(100, activation='relu'))\n",
    "    # model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_predict_plot(X_train, y_train, X_test, y_test, model_name='v1', verbose=1, epochs=5, batch_size=128):\n",
    "    model = createLSTMModel(n_timesteps=X_train.shape[1], n_features=X_train.shape[2], n_outputs=y_train.shape[1])\n",
    "    model.summary()\n",
    "\n",
    "    filepath = os.path.join(model_name + \".hdf5\")\n",
    "    checkpoint = ModelCheckpoint(filepath,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='max')\n",
    "\n",
    "    # Log the epoch detail into csv\n",
    "    csv_logger = CSVLogger(os.path.join(model_name + '.csv'))\n",
    "    callbacks_list = [checkpoint, csv_logger]\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        verbose=verbose, callbacks=callbacks_list)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # plot model accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(model_name + '_model_acc', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.savefig(model_name + '_model_loss', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    class_label = np.concatenate((const_indoor_activity, const_outdoor_activity))\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"Best accuracy (on validation dataset): %.2f%%\" % (accuracy_score(y_test, y_pred) * 100))\n",
    "    print(cnf_matrix)\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cnf_matrix,\n",
    "                                    show_absolute=True,\n",
    "                                    show_normed=True,\n",
    "                                    colorbar=True,\n",
    "                                    class_names=class_label,\n",
    "                                    figsize=(10, 10))\n",
    "    plt.savefig(model_name + '_confusion_matrix', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # display report\n",
    "    report_display = classification_report(y_test, y_pred, target_names=class_label, digits=4)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report_display)\n",
    "\n",
    "    # create report and store in csv\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    df_report.to_csv(model_name + '_classification_report')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved X_train to X_train_accX_LF_Window512_WaveletY_by_peaks_DifferentSubjectsInTrainTest.txt\n",
      "Saved X_test to X_test_accX_LF_Window512_WaveletY_by_peaks_DifferentSubjectsInTrainTest.txt\n",
      "Saved X_train to y_train_accX_LF_Window512_WaveletY_by_peaks_DifferentSubjectsInTrainTest.txt\n",
      "Saved X_test to y_test_accX_LF_Window512_WaveletY_by_peaks_DifferentSubjectsInTrainTest.txt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-fc807da9e0de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                         prepare_single_feature_to_csv(feature, model_name, split_method, window,\n\u001b[1;32m---> 41\u001b[1;33m                                                       wavelet_args, window_args)\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                     X_train, y_train, X_test, y_test = load_dataset(feature_list, window, wavelet_args, window_args,\n",
      "\u001b[1;32m<ipython-input-36-a5ccbb41c5a1>\u001b[0m in \u001b[0;36mprepare_single_feature_to_csv\u001b[1;34m(feature, model_name, split_method, window, wavelet_args, window_args)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     X_train, X_test, y_train, y_test = PrepareFeature(const_master_df, feature, split_method, window, wavelet_args,\n\u001b[1;32m---> 13\u001b[1;33m                                                       window_args)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     X_train_filename = \"_\".join(\n",
      "\u001b[1;32m<ipython-input-26-7e5b9261458d>\u001b[0m in \u001b[0;36mPrepareFeature\u001b[1;34m(df, feature, split_method, window, wavelet_args, window_args, train_size)\u001b[0m\n\u001b[0;32m    134\u001b[0m                 output_np_arr, output_label = PreprocessingSignal(df, activity, sub, feature, window, wavelet_args,\n\u001b[0;32m    135\u001b[0m                                                                   window_args)\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0mtrain_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_np_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[0mtrain_label_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   4698\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4699\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4700\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "# Permutation starts here:\n",
    "axis_list = ['accX', 'accY', 'accZ']\n",
    "pos_list = ['LF', 'RF', 'Waist', 'Wrist']\n",
    "\n",
    "split_method_list = [#'TrainTestSplitWithinSubject'\n",
    "                     # , \n",
    "    #'Random'\n",
    "    'DifferentSubjectsInTrainTest'\n",
    "                     ]\n",
    "window_list = [512\n",
    "               # , 256\n",
    "               ]\n",
    "wavelet_args_list = [\n",
    "    {\n",
    "        \"type\": \"Y\",\n",
    "        \"threshold\": 2,\n",
    "        \"wavedec_options\": {\"wavelet\": \"db4\", \"level\": 2},\n",
    "        \"waverec_options\": {\"wavelet\": \"db4\"}\n",
    "    }\n",
    "    # {\"type\": \"N\"}\n",
    "\n",
    "]\n",
    "window_args_list = [{\"type\": \"by_peaks\", \"find_peaks_options\": {\"prominence\": 30, \"height\": 20}}]\n",
    "\n",
    "feature_list_list = [\n",
    "    ['accX_LF', 'accY_LF', 'accZ_LF', 'accX_RF', 'accY_RF', 'accZ_RF',\n",
    "     'accX_Waist', 'accY_Waist', 'accZ_Waist', 'accX_Wrist', 'accY_Wrist', 'accZ_Wrist']\n",
    "]\n",
    "\n",
    "count = 0\n",
    "for split_method in split_method_list:\n",
    "    for window in window_list:\n",
    "        for wavelet_args in wavelet_args_list:\n",
    "            for window_args in window_args_list:\n",
    "                for feature_list in feature_list_list:\n",
    "                    count = count + 1\n",
    "                    model_name = 'model_v' + str(count)\n",
    "                    \n",
    "                    for feature in feature_list:\n",
    "                        prepare_single_feature_to_csv(feature, model_name, split_method, window,\n",
    "                                                      wavelet_args, window_args)\n",
    "\n",
    "                    X_train, y_train, X_test, y_test = load_dataset(feature_list, window, wavelet_args, window_args,\n",
    "                                                                    split_method)\n",
    "\n",
    "                    print(X_train.shape)\n",
    "                    print(y_train.shape)\n",
    "                    print(X_test.shape)\n",
    "                    print(y_test.shape)\n",
    "                    print(X_train.shape[1])\n",
    "                    print(X_train.shape[2])\n",
    "                    print(y_train.shape[1])\n",
    "\n",
    "\n",
    "                    train_predict_plot(X_train, y_train, X_test, y_test, model_name=model_name, verbose=1, epochs=5,\n",
    "                                       batch_size=128)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
