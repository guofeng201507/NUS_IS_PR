{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6bX1Ws4siSUu",
    "outputId": "2990c0ac-a834-4032-fa6c-4c41eba0dc3f",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set data folder path\n",
    "DATA_FOLDER = r'D:/NUS_TERM2_CA3/MAREA_dataset'\n",
    "\n",
    "ACTIVITY_FOLDER = os.path.join(DATA_FOLDER, 'Activity Timings')\n",
    "SUBJECT_FOLDER = os.path.join(DATA_FOLDER, 'Subject_Data_txt_format')\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# treadWalk       = indoor_time['indoorTime'][:,1:2]\n",
    "# treadIncline    = indoor_time['indoorTime'][:,4:5]\n",
    "# treadWalknRun   = indoor_time['indoorTime'][:,1:3]\n",
    "# indoorWalk      = indoor_time['indoorTime'][:,6:7]\n",
    "# indoorRun       = indoor_time['indoorTime'][:,6:8]\n",
    "# outdoorWalk     = outdoor_time['outdoorTime'][:,1:2]\n",
    "# outdoorWalknRun = outdoor_time['outdoorTime'][:,1:3]\n",
    "###########################################################################\n",
    "\n",
    "###########################################################################\n",
    "# Subject numbers 1 to 11 are involved in Indoor Experiments\n",
    "# Subject numbers 12 to 20 are involved in Outdoor Experiments\n",
    "###########################################################################\n",
    "\n",
    "###########################################################################\n",
    "# There are five Indoor Activity labels:\n",
    "# actIndex = 1 -> treadWalk\n",
    "# actIndex = 2 -> treadIncline\n",
    "# actIndex = 3 -> treadWalknRun\n",
    "# actIndex = 4 -> indoorWalk\n",
    "# actIndex = 5 -> indoorWalknRun\n",
    "\n",
    "# There are two Outdoor Activity labels:\n",
    "# actIndex = 1 -> outdoorWalk\n",
    "# actIndex = 2 -> outdoorWalknRun\n",
    "###########################################################################\n",
    "\n",
    "###########################################################################\n",
    "# There are four accelerometer positions (accPos) to choose from:\n",
    "# accPos = 1 -> Left Foot\n",
    "# accPos = 2 -> Right Foot\n",
    "# accPos = 3 -> Waist\n",
    "# accPos = 4 -> Wrist\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "jcAsA04JjP2U",
    "outputId": "3a752545-ad65-427e-a83d-2d5962480c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tread_flat_walk_start  tread_flat_walk_end  tread_flat_run_end  \\\n",
      "0                       1                55931               85681   \n",
      "1                       1                40911               84991   \n",
      "2                       1                62261               83961   \n",
      "3                       1                45781               84551   \n",
      "4                       1                63971               85121   \n",
      "5                       1                69381               84781   \n",
      "6                       1                46331               83761   \n",
      "7                       1                53801               84811   \n",
      "8                       1                69131               84661   \n",
      "9                       1                70041               84941   \n",
      "10                      1                77641               84741   \n",
      "\n",
      "    tread_slope_walk_start  tread_slope_walk_end  indoor_flat_walk_start  \\\n",
      "0                   102181                180281                  223681   \n",
      "1                   181091                262291                  289991   \n",
      "2                    98361                181061                  211061   \n",
      "3                    95851                177951                  193651   \n",
      "4                    97621                177921                  209821   \n",
      "5                    90081                169381                  189181   \n",
      "6                   101561                184861                  210361   \n",
      "7                    98711                183211                  224211   \n",
      "8                    98261                177561                  199861   \n",
      "9                    91941                175641                  195041   \n",
      "10                   98841                186141                  229541   \n",
      "\n",
      "    indoor_flat_walk_end  indoor_flat_run_end subject  \n",
      "0                 246381               271681    Sub1  \n",
      "1                 313491               336391    Sub2  \n",
      "2                 234261               257861    Sub3  \n",
      "3                 216851               239951    Sub4  \n",
      "4                 233021               257021    Sub5  \n",
      "5                 211981               235981    Sub6  \n",
      "6                 232761               255761    Sub7  \n",
      "7                 246811               270311    Sub8  \n",
      "8                 223061               246761    Sub9  \n",
      "9                 217941               240741   Sub10  \n",
      "10                252241               276041   Sub11  \n",
      "   outdoor_walk_start  outdoor_walk_end  outdoor_run_end subject\n",
      "0                   1             23691            49651   Sub12\n",
      "1                   1             47911            71011   Sub13\n",
      "2                   1             23551            43581   Sub14\n",
      "3                   1             22701            33140   Sub15\n",
      "4                   1             23581            46651   Sub16\n",
      "5                   1             23761            47261   Sub17\n",
      "6                   1             23801            48801   Sub18\n",
      "7                   1             23441            47141   Sub19\n",
      "8                   1             23401            32201   Sub20\n"
     ]
    }
   ],
   "source": [
    "# Define Activity Labels\n",
    "indoor_label = ['tread_flat_walk_start', \n",
    "                'tread_flat_walk_end',\n",
    "                'tread_flat_run_end',\n",
    "                'tread_slope_walk_start',\n",
    "                'tread_slope_walk_end',\n",
    "                'indoor_flat_walk_start',\n",
    "                'indoor_flat_walk_end',\n",
    "                'indoor_flat_run_end'\n",
    "               ]\n",
    "\n",
    "outdoor_label = ['outdoor_walk_start',\n",
    "                 'outdoor_walk_end',\n",
    "                 'outdoor_run_end']\n",
    "\n",
    "indoor_time_df = pd.read_csv(os.path.join(ACTIVITY_FOLDER, 'Indoor Experiment Timings.txt')\n",
    "                            , names= indoor_label)\n",
    "\n",
    "outdoor_time_df = pd.read_csv(os.path.join(ACTIVITY_FOLDER, 'Outdoor Experiment Timings.txt')\n",
    "                            , names=outdoor_label)\n",
    "\n",
    "indoor_time_df[\"subject\"] = [\"Sub\" + str(i) for i in range(1, 12)]\n",
    "outdoor_time_df[\"subject\"] = [\"Sub\" + str(j) for j in range(12, 21)]\n",
    "\n",
    "print(indoor_time_df)\n",
    "print(outdoor_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['LF','RF','Waist','Wrist']\n",
    "sub_list = [\"Sub\" + str(i) for i in range(1, 21)]\n",
    "sub_list.remove('Sub4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "def alsbase(y, lam, p, niter=10):\n",
    "    L = len(y)\n",
    "    D = sparse.diags([1,-2,1],[0,-1,-2], shape=(L,L-2))\n",
    "    w = np.ones(L)\n",
    "    for i in range(niter):\n",
    "        W = sparse.spdiags(w, 0, L, L)\n",
    "        Z = W + lam * D.dot(D.transpose())\n",
    "        z = spsolve(Z, w*y)\n",
    "        w = p * (y > z) + (1-p) * (y < z)\n",
    "    return z\n",
    "\n",
    "def denoise(signal_orig):\n",
    "    coeffs_orig = pywt.wavedec(signal_orig, 'db4', level=2)\n",
    "    coeffs_filter = coeffs_orig.copy()\n",
    "\n",
    "    threshold = 0.8\n",
    "\n",
    "    for i in range(1, len(coeffs_orig)):\n",
    "        coeffs_filter[i] = pywt.threshold(coeffs_orig[i], threshold*max(coeffs_orig[i]))\n",
    "\n",
    "    signal_denoised = pywt.waverec(coeffs_filter, 'db4')\n",
    "    \n",
    "    return signal_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   accX_LF  accY_LF  accZ_LF  accX_RF  accY_RF  accZ_RF  accX_Waist  \\\n",
      "0   -2.196  -11.765   -1.569    4.078   -6.902    2.196      -0.157   \n",
      "1   -2.510  -10.353   -2.039    4.392   -8.000    1.255      -0.157   \n",
      "2   -2.353  -10.824   -2.510    4.863   -8.314    0.941      -0.157   \n",
      "3   -1.882  -10.667   -2.667    4.549   -9.412   -0.157      -0.157   \n",
      "4   -1.412  -10.353   -2.824    3.451  -10.353   -0.784      -0.627   \n",
      "\n",
      "   accY_Waist  accZ_Waist  accX_Wrist  accY_Wrist  accZ_Wrist  \n",
      "0      -8.157       0.784       0.941     -11.451      -2.510  \n",
      "1      -8.000       1.255       0.627     -10.510      -2.353  \n",
      "2      -7.843       1.255       0.784     -10.196      -2.039  \n",
      "3      -8.000       1.412       1.255     -10.039      -1.412  \n",
      "4      -7.529       1.882       1.569     -10.196      -1.255  \n",
      "(271681,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a842e5e06a1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_df_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0msub_df_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdenoise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_df_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m#         sub_df_new[column] = sub_df_new[column] - alsbase(sub_df_new[column], 10^5,0.000005,niter=10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3368\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3369\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3370\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3444\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3445\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3446\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3629\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3630\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3631\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3632\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index, copy)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Length of values does not match length of index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "\n",
    "new_names = ['accX_LF', 'accY_LF', 'accZ_LF', \n",
    "            'accX_RF', 'accY_RF', 'accZ_RF', \n",
    "             'accX_Waist', 'accY_Waist', 'accZ_Waist', \n",
    "             'accX_Wrist', 'accY_Wrist', 'accZ_Wrist'            \n",
    "            ]\n",
    "\n",
    "sub_df = None\n",
    "\n",
    "for sub in sub_list:\n",
    "    lf_df = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'LF.txt'))\n",
    "    rf_df = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'RF.txt'))\n",
    "    waist_df = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'Waist.txt'))\n",
    "    wrist_df = pd.read_csv(os.path.join(SUBJECT_FOLDER, sub + '_' + 'Wrist.txt'))\n",
    "    sub_df = pd.concat([lf_df, rf_df, waist_df, wrist_df], axis=1)\n",
    "    sub_df.columns = new_names\n",
    "    print(sub_df.head())\n",
    "\n",
    "    sub_df_new = sub_df.copy()\n",
    "    sub_df_new = denoise(sub_df_new.values)\n",
    "    sub_df_new.columns = new_names\n",
    "    print(sub_df_new.head())\n",
    "\n",
    "    for column in new_names:\n",
    "        sub_df_new[column] = sub_df_new[column] - alsbase(sub_df_new[column], 10 ^ 5, 0.000005, niter=10)\n",
    "\n",
    "    n = int(sub[3:])\n",
    "    if n > 11:\n",
    "        sub_row = outdoor_time_df[outdoor_time_df['subject'] == sub]\n",
    "        tmp = sub_row.iloc[0]\n",
    "        sub_df_new.loc[0:tmp['outdoor_walk_end'], 'label'] = 'outdoor_walk'\n",
    "        sub_df_new.loc[tmp['outdoor_walk_end']: tmp['outdoor_run_end'], 'label'] = 'outdoor_run'\n",
    "    else:\n",
    "        sub_row = indoor_time_df[indoor_time_df['subject'] == sub]\n",
    "        tmp = sub_row.iloc[0]\n",
    "        sub_df_new.loc[0:tmp['tread_flat_walk_end'], 'label'] = 'tread_flat_walk'\n",
    "        sub_df_new.loc[tmp['tread_flat_walk_end']: tmp['tread_flat_run_end'], 'label'] = 'tread_flat_run'\n",
    "        sub_df_new.loc[tmp['tread_flat_run_end']: tmp['tread_slope_walk_start'], 'label'] = 'rest'\n",
    "        sub_df_new.loc[tmp['tread_slope_walk_start']: tmp['tread_slope_walk_end'], 'label'] = 'tread_slope_walk'\n",
    "        sub_df_new.loc[tmp['tread_slope_walk_end']: tmp['indoor_flat_walk_start'], 'label'] = 'rest'\n",
    "        sub_df_new.loc[tmp['indoor_flat_walk_start']: tmp['indoor_flat_walk_end'], 'label'] = 'indoor_flat_walk'\n",
    "        sub_df_new.loc[tmp['indoor_flat_walk_end']: tmp['indoor_flat_run_end'], 'label'] = 'indoor_flat_run'\n",
    "\n",
    "    print(sub_df_new)\n",
    "    sub_df_new.to_csv(sub + '_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271681, 13)\n",
      "Loading Sub2_processed.csv\n",
      "(336391, 13)\n",
      "Loading Sub3_processed.csv\n",
      "(257861, 13)\n",
      "Loading Sub5_processed.csv\n",
      "(257021, 13)\n",
      "Loading Sub6_processed.csv\n",
      "(235981, 13)\n",
      "Loading Sub7_processed.csv\n",
      "(255761, 13)\n",
      "Loading Sub8_processed.csv\n",
      "(270311, 13)\n",
      "Loading Sub9_processed.csv\n",
      "(246761, 13)\n",
      "Loading Sub10_processed.csv\n",
      "(240741, 13)\n",
      "Loading Sub11_processed.csv\n",
      "(276041, 13)\n",
      "Loading Sub12_processed.csv\n",
      "(49651, 13)\n",
      "Loading Sub13_processed.csv\n",
      "(71011, 13)\n",
      "Loading Sub14_processed.csv\n",
      "(43581, 13)\n",
      "Loading Sub15_processed.csv\n",
      "(33140, 13)\n",
      "Loading Sub16_processed.csv\n",
      "(46651, 13)\n",
      "Loading Sub17_processed.csv\n",
      "(47261, 13)\n",
      "Loading Sub18_processed.csv\n",
      "(48801, 13)\n",
      "Loading Sub19_processed.csv\n",
      "(47141, 13)\n",
      "Loading Sub20_processed.csv\n",
      "(32201, 13)\n",
      "(3067988, 13)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "DATA_FOLDER = r'D:/NUS_TERM2_CA3/MAREA_dataset'\n",
    "PROCESSED_FOLDER = os.path.join(DATA_FOLDER, 'Processed_data')\n",
    "\n",
    "sub_list = [\"Sub\" + str(i) for i in range(2, 21)]\n",
    "sub_list.remove('Sub4')\n",
    "\n",
    "\n",
    "full_df = pd.read_csv(os.path.join(PROCESSED_FOLDER, 'Sub1_processed.csv'))\n",
    "full_df = full_df.drop(full_df.columns[[0]], axis=1)\n",
    "\n",
    "print(full_df.shape)\n",
    "\n",
    "for sub in sub_list:\n",
    "    tmp_df = pd.read_csv(os.path.join(PROCESSED_FOLDER, sub + '_processed.csv'))\n",
    "    tmp_df = tmp_df.drop(tmp_df.columns[[0]], axis=1)\n",
    "\n",
    "    print('Loading ' + sub + '_processed.csv')\n",
    "    print(tmp_df.shape)\n",
    "    full_df = full_df.append(tmp_df, ignore_index = True)\n",
    "\n",
    "print(full_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609400, 13)\n",
      "(238050, 13)\n",
      "(819700, 13)\n",
      "(229200, 13)\n",
      "(236600, 13)\n",
      "(515600, 13)\n",
      "(235839, 13)\n",
      "(183599, 13)\n"
     ]
    }
   ],
   "source": [
    "df_tread_flat_walk = full_df[full_df['label'] == 'tread_flat_walk']\n",
    "print(df_tread_flat_walk.shape)\n",
    "\n",
    "df_tread_flat_run = full_df[full_df['label'] == 'tread_flat_run']\n",
    "print(df_tread_flat_run.shape)\n",
    "\n",
    "df_tread_slope_walk = full_df[full_df['label'] == 'tread_slope_walk']\n",
    "print(df_tread_slope_walk.shape)\n",
    "\n",
    "df_indoor_flat_walk = full_df[full_df['label'] == 'indoor_flat_walk']\n",
    "print(df_indoor_flat_walk.shape)\n",
    "\n",
    "df_indoor_flat_run = full_df[full_df['label'] == 'indoor_flat_run']\n",
    "print(df_indoor_flat_run.shape)\n",
    "\n",
    "df_rest = full_df[full_df['label'] == 'rest']\n",
    "print(df_rest.shape)\n",
    "\n",
    "df_outdoor_walk = full_df[full_df['label'] == 'outdoor_walk']\n",
    "print(df_outdoor_walk.shape)\n",
    "\n",
    "df_outdoor_run = full_df[full_df['label'] == 'outdoor_run']\n",
    "print(df_outdoor_run.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 256\n",
    "number_columns = 13\n",
    "\n",
    "activity_to_num_mapping = {\n",
    "    \"rest\":0,\n",
    "    \"tread_flat_walk\":1,\n",
    "    \"tread_flat_run\":2,\n",
    "    \"tread_slope_walk\":3,\n",
    "    \"indoor_flat_walk\":4,\n",
    "    \"indoor_flat_run\":5,\n",
    "    \n",
    "    \"outdoor_walk\":6,\n",
    "    \"outdoor_run\":7\n",
    "}\n",
    "\n",
    "\n",
    "def reshape_df(df, window_size, number_columns):\n",
    "    n_drop = df.shape[0] % window_size\n",
    "    n_samples = df.shape[0] // window_size\n",
    "    df = df[:-n_drop]\n",
    "    \n",
    "    label = activity_to_num_mapping.get(df.iloc[0][12])\n",
    "    label_series = pd.Series([label for _ in range(n_samples)])\n",
    "    \n",
    "    return df.values.reshape(n_samples, window_size, number_columns), label_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2380, 256, 13)\n",
      "(2380,)\n",
      "(929, 256, 13)\n",
      "(929,)\n",
      "(3201, 256, 13)\n",
      "(895, 256, 13)\n",
      "(924, 256, 13)\n",
      "(921, 256, 13)\n",
      "(717, 256, 13)\n",
      "(2014, 256, 13)\n",
      "(11981, 256, 13)\n",
      "(11981,)\n"
     ]
    }
   ],
   "source": [
    "df_tread_flat_walk_3d, ds_tread_flat_walk_label = reshape_df(df_tread_flat_walk, window_size, number_columns)\n",
    "print(df_tread_flat_walk_3d.shape)\n",
    "print(ds_tread_flat_walk_label.shape)\n",
    "\n",
    "df_tread_flat_run_3d, ds_tread_flat_run_label = reshape_df(df_tread_flat_run, window_size, number_columns)\n",
    "print(df_tread_flat_run_3d.shape)\n",
    "print(ds_tread_flat_run_label.shape)\n",
    "\n",
    "df_tread_slope_walk_3d, ds_tread_slope_walk_label = reshape_df(df_tread_slope_walk, window_size, number_columns)\n",
    "print(df_tread_slope_walk_3d.shape)\n",
    "df_indoor_flat_walk_3d, ds_indoor_flat_walk_label = reshape_df(df_indoor_flat_walk, window_size, number_columns)\n",
    "print(df_indoor_flat_walk_3d.shape)\n",
    "df_indoor_flat_run_3d, ds_indoor_flat_run_label = reshape_df(df_indoor_flat_run, window_size, number_columns)\n",
    "print(df_indoor_flat_run_3d.shape)\n",
    "df_outdoor_walk_3d, ds_outdoor_walk_label = reshape_df(df_outdoor_walk, window_size, number_columns)\n",
    "print(df_outdoor_walk_3d.shape)\n",
    "df_outdoor_run_3d, ds_outdoor_run_label = reshape_df(df_outdoor_run, window_size, number_columns)\n",
    "print(df_outdoor_run_3d.shape)\n",
    "df_rest_3d, ds_rest_label = reshape_df(df_rest, window_size, number_columns)\n",
    "print(df_rest_3d.shape)\n",
    "\n",
    "full_df_3d = np.vstack((df_tread_flat_walk_3d, df_tread_flat_run_3d,\n",
    "                        df_tread_slope_walk_3d, df_indoor_flat_walk_3d,\n",
    "                        df_indoor_flat_run_3d, df_outdoor_walk_3d,\n",
    "                        df_outdoor_run_3d, df_rest_3d\n",
    "                       ))\n",
    "\n",
    "print(full_df_3d.shape)\n",
    "\n",
    "full_ds_label = ds_tread_flat_walk_label.append(ds_tread_flat_run_label)\n",
    "full_ds_label= full_ds_label.append(ds_tread_slope_walk_label)\n",
    "full_ds_label= full_ds_label.append(ds_indoor_flat_walk_label)\n",
    "full_ds_label= full_ds_label.append(ds_indoor_flat_run_label)\n",
    "full_ds_label= full_ds_label.append(ds_outdoor_walk_label)\n",
    "full_ds_label= full_ds_label.append(ds_outdoor_run_label)\n",
    "full_ds_label= full_ds_label.append(ds_rest_label)\n",
    "\n",
    "print(full_ds_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11981, 256, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\guofe\\workspace\\nus_is_pr\\venv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "(11981, 8)\n"
     ]
    }
   ],
   "source": [
    "df_array = full_df_3d[:,:,:-1]\n",
    "print(df_array.shape)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_cat = to_categorical(full_ds_label, num_classes=8)\n",
    "print(y_cat)\n",
    "print(y_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 256, 12)\n",
      "(19, 8)\n",
      "(12000, 256, 12)\n",
      "(12000, 8)\n"
     ]
    }
   ],
   "source": [
    "y = y_cat\n",
    "X = df_array\n",
    "\n",
    "padding_number = 19 #12000 - 11981 \n",
    "\n",
    "df_padding_X = df_array[0:19,:,:]\n",
    "df_padding_y = y_cat[0:19,:]\n",
    "\n",
    "print(df_padding_X.shape)\n",
    "print(df_padding_y.shape)\n",
    "\n",
    "X = np.vstack((X, df_padding_X))\n",
    "y = np.vstack((y, df_padding_y))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = full_df['label']\n",
    "# print(y.shape)\n",
    "# print(type(y))\n",
    "# X = full_df.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn\n",
      "  Using cached https://files.pythonhosted.org/packages/d6/9e/6a42486ffa64711fb868e5d4a9167153417e7414c3d8d3e0d627cf391e1e/scikit_learn-0.21.3-cp37-cp37m-win_amd64.whl\n",
      "Collecting joblib>=0.11\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl (294kB)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.17.3)\n",
      "Installing collected packages: joblib, scikit-learn, sklearn\n",
      "    Running setup.py install for sklearn: started\n",
      "    Running setup.py install for sklearn: finished with status 'done'\n",
      "Successfully installed joblib-0.14.0 scikit-learn-0.21.3 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 256, 12)\n",
      "(9000, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "\n",
    "#https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 / 4, random_state=5)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# print(X_train.describe())\n",
    "# print(y_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "Collecting h5py\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/6b/7f62017e3f0b32438dd90bdc1ff0b7b1448b6cb04a1ed84f37b6de95cd7b/h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5MB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras) (1.17.3)\n",
      "Collecting pyyaml\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/3f/4f733cd0b1b675f34beb290d465a65e0f06b492c00b111d1b75125062de1/PyYAML-5.1.2-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras) (1.12.0)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras) (1.3.1)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Installing collected packages: h5py, pyyaml, keras-preprocessing, keras-applications, keras\n",
      "Successfully installed h5py-2.10.0 keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0 pyyaml-5.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading https://files.pythonhosted.org/packages/63/13/ea9ff554aa0043540a2387c28dd7926575eb25cf89e598caecea836d189d/tensorflow_gpu-2.0.0-cp37-cp37m-win_amd64.whl (285.3MB)\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting gast==0.2.2\n",
      "  Using cached https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/bc/60eeb61f97837475dae356afa797c54ea6db986afaf6c6d6320a572ff8aa/grpcio-1.24.3-cp37-cp37m-win_amd64.whl (1.6MB)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-gpu) (1.0.8)\n",
      "Processing c:\\users\\guofe\\appdata\\local\\pip\\cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\\termcolor-1.1.0-cp37-none-any.whl\n",
      "Processing c:\\users\\guofe\\appdata\\local\\pip\\cache\\wheels\\d7\\de\\2e\\efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\\wrapt-1.11.2-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-gpu) (1.12.0)\n",
      "Collecting wheel>=0.26\n",
      "  Using cached https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-gpu) (1.17.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/ae/a11b9b0c8e2410b11887881990b71f54ec39b17c4de2b5d850ef66aade8c/protobuf-3.10.0-cp37-cp37m-win_amd64.whl (1.0MB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/9a/50fadfd53ec909e4399b67c74cc7f4e883488035cfcdb90b685758fa8b34/setuptools-41.4.0-py2.py3-none-any.whl (580kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "Requirement already satisfied: h5py in c:\\users\\guofe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
      "Installing collected packages: astor, gast, setuptools, markdown, absl-py, werkzeug, grpcio, wheel, protobuf, tensorboard, termcolor, wrapt, opt-einsum, google-pasta, tensorflow-estimator, tensorflow-gpu\n",
      "    Running setup.py install for gast: started\n",
      "    Running setup.py install for gast: finished with status 'done'\n",
      "  Found existing installation: setuptools 40.8.0\n",
      "    Uninstalling setuptools-40.8.0:\n",
      "      Successfully uninstalled setuptools-40.8.0\n",
      "    Running setup.py install for absl-py: started\n",
      "    Running setup.py install for absl-py: finished with status 'done'\n",
      "    Running setup.py install for opt-einsum: started\n",
      "    Running setup.py install for opt-einsum: finished with status 'done'\n",
      "Successfully installed absl-py-0.8.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.24.3 markdown-3.1.1 opt-einsum-3.1.0 protobuf-3.10.0 setuptools-41.4.0 tensorboard-2.0.0 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0 termcolor-1.1.0 werkzeug-0.16.0 wheel-0.33.6 wrapt-1.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 1800 samples\n",
      "Epoch 1/200\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 1.9366 - acc: 0.2957 - val_loss: 1.7317 - val_acc: 0.3783\n",
      "Epoch 2/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.6817 - acc: 0.3886 - val_loss: 1.4965 - val_acc: 0.4356\n",
      "Epoch 3/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.5361 - acc: 0.4154 - val_loss: 1.3928 - val_acc: 0.4594\n",
      "Epoch 4/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.4333 - acc: 0.4387 - val_loss: 1.2921 - val_acc: 0.4844\n",
      "Epoch 5/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.3516 - acc: 0.4671 - val_loss: 1.2033 - val_acc: 0.5106\n",
      "Epoch 6/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.2800 - acc: 0.4785 - val_loss: 1.1595 - val_acc: 0.5228\n",
      "Epoch 7/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.2387 - acc: 0.4971 - val_loss: 1.1302 - val_acc: 0.5294\n",
      "Epoch 8/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.1956 - acc: 0.5107 - val_loss: 1.0892 - val_acc: 0.5494\n",
      "Epoch 9/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.1746 - acc: 0.5169 - val_loss: 1.0764 - val_acc: 0.5611\n",
      "Epoch 10/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.1422 - acc: 0.5251 - val_loss: 1.0542 - val_acc: 0.5656\n",
      "Epoch 11/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.1275 - acc: 0.5315 - val_loss: 1.0374 - val_acc: 0.5767\n",
      "Epoch 12/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.1042 - acc: 0.5390 - val_loss: 1.0124 - val_acc: 0.5906\n",
      "Epoch 13/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.0784 - acc: 0.5558 - val_loss: 0.9973 - val_acc: 0.6000\n",
      "Epoch 14/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.0670 - acc: 0.5532 - val_loss: 0.9868 - val_acc: 0.6106\n",
      "Epoch 15/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.0641 - acc: 0.5575 - val_loss: 0.9611 - val_acc: 0.6189\n",
      "Epoch 16/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.0300 - acc: 0.5685 - val_loss: 0.9442 - val_acc: 0.6272\n",
      "Epoch 17/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.0233 - acc: 0.5744 - val_loss: 0.9403 - val_acc: 0.6178\n",
      "Epoch 18/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.0043 - acc: 0.5765 - val_loss: 0.9139 - val_acc: 0.6261\n",
      "Epoch 19/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 1.0007 - acc: 0.5831 - val_loss: 0.8985 - val_acc: 0.6350\n",
      "Epoch 20/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9863 - acc: 0.5929 - val_loss: 0.8922 - val_acc: 0.6428\n",
      "Epoch 21/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9713 - acc: 0.6021 - val_loss: 0.8871 - val_acc: 0.6456\n",
      "Epoch 22/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9616 - acc: 0.6036 - val_loss: 0.8750 - val_acc: 0.6478\n",
      "Epoch 23/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9590 - acc: 0.5976 - val_loss: 0.8788 - val_acc: 0.6400\n",
      "Epoch 24/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9875 - acc: 0.5900 - val_loss: 0.8928 - val_acc: 0.6422\n",
      "Epoch 25/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9464 - acc: 0.6151 - val_loss: 0.8696 - val_acc: 0.6533\n",
      "Epoch 26/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9448 - acc: 0.6122 - val_loss: 0.8559 - val_acc: 0.6611\n",
      "Epoch 27/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9318 - acc: 0.6172 - val_loss: 0.8465 - val_acc: 0.6611\n",
      "Epoch 28/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9181 - acc: 0.6206 - val_loss: 0.8278 - val_acc: 0.6711\n",
      "Epoch 29/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9164 - acc: 0.6167 - val_loss: 0.8390 - val_acc: 0.6644\n",
      "Epoch 30/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9040 - acc: 0.6283 - val_loss: 0.8326 - val_acc: 0.6600\n",
      "Epoch 31/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8977 - acc: 0.6306 - val_loss: 0.8123 - val_acc: 0.6717\n",
      "Epoch 32/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9044 - acc: 0.6228 - val_loss: 0.8055 - val_acc: 0.6700\n",
      "Epoch 33/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9010 - acc: 0.6279 - val_loss: 0.8160 - val_acc: 0.6667\n",
      "Epoch 34/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8818 - acc: 0.6444 - val_loss: 0.8051 - val_acc: 0.6689\n",
      "Epoch 35/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.9014 - acc: 0.6342 - val_loss: 0.8220 - val_acc: 0.6711\n",
      "Epoch 36/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8926 - acc: 0.6326 - val_loss: 0.8058 - val_acc: 0.6672\n",
      "Epoch 37/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8820 - acc: 0.6400 - val_loss: 0.7958 - val_acc: 0.6728\n",
      "Epoch 38/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8889 - acc: 0.6371 - val_loss: 0.7939 - val_acc: 0.6806\n",
      "Epoch 39/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8721 - acc: 0.6465 - val_loss: 0.7983 - val_acc: 0.6878\n",
      "Epoch 40/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8664 - acc: 0.6504 - val_loss: 0.7748 - val_acc: 0.6967\n",
      "Epoch 41/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8754 - acc: 0.6453 - val_loss: 0.7887 - val_acc: 0.6911\n",
      "Epoch 42/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8608 - acc: 0.6547 - val_loss: 0.7907 - val_acc: 0.6778\n",
      "Epoch 43/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8439 - acc: 0.6639 - val_loss: 0.7517 - val_acc: 0.7028\n",
      "Epoch 44/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8469 - acc: 0.6576 - val_loss: 0.7681 - val_acc: 0.6878\n",
      "Epoch 45/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8359 - acc: 0.6601 - val_loss: 0.7454 - val_acc: 0.7194\n",
      "Epoch 46/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8508 - acc: 0.6567 - val_loss: 0.7674 - val_acc: 0.6983\n",
      "Epoch 47/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8359 - acc: 0.6589 - val_loss: 0.7528 - val_acc: 0.7061\n",
      "Epoch 48/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8309 - acc: 0.6646 - val_loss: 0.7566 - val_acc: 0.6906\n",
      "Epoch 49/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8252 - acc: 0.6692 - val_loss: 0.7320 - val_acc: 0.7094\n",
      "Epoch 50/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8262 - acc: 0.6776 - val_loss: 0.7449 - val_acc: 0.7144\n",
      "Epoch 51/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8084 - acc: 0.6757 - val_loss: 0.7385 - val_acc: 0.7167\n",
      "Epoch 52/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8106 - acc: 0.6735 - val_loss: 0.7545 - val_acc: 0.6928\n",
      "Epoch 53/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8046 - acc: 0.6803 - val_loss: 0.7278 - val_acc: 0.7172\n",
      "Epoch 54/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.8097 - acc: 0.6801 - val_loss: 0.7238 - val_acc: 0.7139\n",
      "Epoch 55/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7958 - acc: 0.6833 - val_loss: 0.7221 - val_acc: 0.7222\n",
      "Epoch 56/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7884 - acc: 0.6868 - val_loss: 0.7159 - val_acc: 0.7250\n",
      "Epoch 57/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7843 - acc: 0.6913 - val_loss: 0.7192 - val_acc: 0.7233\n",
      "Epoch 58/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7937 - acc: 0.6847 - val_loss: 0.7093 - val_acc: 0.7233\n",
      "Epoch 59/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7752 - acc: 0.7004 - val_loss: 0.7093 - val_acc: 0.7256\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7900 - acc: 0.6886 - val_loss: 0.7103 - val_acc: 0.7300\n",
      "Epoch 61/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7695 - acc: 0.6929 - val_loss: 0.6936 - val_acc: 0.7339\n",
      "Epoch 62/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7768 - acc: 0.6972 - val_loss: 0.6959 - val_acc: 0.7250\n",
      "Epoch 63/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7832 - acc: 0.6887 - val_loss: 0.7115 - val_acc: 0.7228\n",
      "Epoch 64/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7693 - acc: 0.7022 - val_loss: 0.6986 - val_acc: 0.7233\n",
      "Epoch 65/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7618 - acc: 0.7022 - val_loss: 0.6768 - val_acc: 0.7450\n",
      "Epoch 66/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7705 - acc: 0.7017 - val_loss: 0.6867 - val_acc: 0.7250\n",
      "Epoch 67/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7535 - acc: 0.7047 - val_loss: 0.6653 - val_acc: 0.7522\n",
      "Epoch 68/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7678 - acc: 0.7019 - val_loss: 0.6748 - val_acc: 0.7422\n",
      "Epoch 69/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7385 - acc: 0.7125 - val_loss: 0.6742 - val_acc: 0.7411\n",
      "Epoch 70/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7568 - acc: 0.7051 - val_loss: 0.6802 - val_acc: 0.7461\n",
      "Epoch 71/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7578 - acc: 0.7022 - val_loss: 0.6656 - val_acc: 0.7444\n",
      "Epoch 72/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7476 - acc: 0.7072 - val_loss: 0.6793 - val_acc: 0.7350\n",
      "Epoch 73/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.7441 - acc: 0.7104 - val_loss: 0.6648 - val_acc: 0.7400\n",
      "Epoch 74/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.7423 - acc: 0.7111 - val_loss: 0.6595 - val_acc: 0.7467\n",
      "Epoch 75/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.7316 - acc: 0.7237 - val_loss: 0.6468 - val_acc: 0.7461\n",
      "Epoch 76/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.7268 - acc: 0.7225 - val_loss: 0.6505 - val_acc: 0.7494\n",
      "Epoch 77/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.7263 - acc: 0.7200 - val_loss: 0.6560 - val_acc: 0.7411\n",
      "Epoch 78/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7205 - acc: 0.7293 - val_loss: 0.6470 - val_acc: 0.7444\n",
      "Epoch 79/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7142 - acc: 0.7203 - val_loss: 0.6285 - val_acc: 0.7611\n",
      "Epoch 80/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7095 - acc: 0.7261 - val_loss: 0.6482 - val_acc: 0.7567\n",
      "Epoch 81/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7241 - acc: 0.7281 - val_loss: 0.6514 - val_acc: 0.7478\n",
      "Epoch 82/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7155 - acc: 0.7203 - val_loss: 0.6278 - val_acc: 0.7650\n",
      "Epoch 83/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7093 - acc: 0.7281 - val_loss: 0.6168 - val_acc: 0.7689\n",
      "Epoch 84/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7184 - acc: 0.7265 - val_loss: 0.6233 - val_acc: 0.7744\n",
      "Epoch 85/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7050 - acc: 0.7290 - val_loss: 0.6450 - val_acc: 0.7578\n",
      "Epoch 86/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7156 - acc: 0.7262 - val_loss: 0.6293 - val_acc: 0.7611\n",
      "Epoch 87/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7020 - acc: 0.7269 - val_loss: 0.6284 - val_acc: 0.7633\n",
      "Epoch 88/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6974 - acc: 0.7354 - val_loss: 0.6008 - val_acc: 0.7789\n",
      "Epoch 89/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6857 - acc: 0.7376 - val_loss: 0.6072 - val_acc: 0.7761\n",
      "Epoch 90/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6973 - acc: 0.7390 - val_loss: 0.6164 - val_acc: 0.7672\n",
      "Epoch 91/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7024 - acc: 0.7321 - val_loss: 0.6247 - val_acc: 0.7694\n",
      "Epoch 92/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.7021 - acc: 0.7335 - val_loss: 0.6100 - val_acc: 0.7722\n",
      "Epoch 93/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6848 - acc: 0.7317 - val_loss: 0.6027 - val_acc: 0.7794\n",
      "Epoch 94/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6793 - acc: 0.7413 - val_loss: 0.6184 - val_acc: 0.7667\n",
      "Epoch 95/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6886 - acc: 0.7376 - val_loss: 0.6148 - val_acc: 0.7756\n",
      "Epoch 96/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6770 - acc: 0.7418 - val_loss: 0.6124 - val_acc: 0.7717\n",
      "Epoch 97/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6720 - acc: 0.7447 - val_loss: 0.6049 - val_acc: 0.7783\n",
      "Epoch 98/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6833 - acc: 0.7418 - val_loss: 0.5875 - val_acc: 0.7972\n",
      "Epoch 99/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6684 - acc: 0.7431 - val_loss: 0.6084 - val_acc: 0.7833\n",
      "Epoch 100/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6725 - acc: 0.7437 - val_loss: 0.5938 - val_acc: 0.7817\n",
      "Epoch 101/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6644 - acc: 0.7518 - val_loss: 0.5620 - val_acc: 0.8044\n",
      "Epoch 102/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6736 - acc: 0.7439 - val_loss: 0.5992 - val_acc: 0.7806\n",
      "Epoch 103/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6635 - acc: 0.7504 - val_loss: 0.5887 - val_acc: 0.7822\n",
      "Epoch 104/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6682 - acc: 0.7478 - val_loss: 0.6011 - val_acc: 0.7722\n",
      "Epoch 105/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6668 - acc: 0.7529 - val_loss: 0.5766 - val_acc: 0.7883\n",
      "Epoch 106/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6595 - acc: 0.7528 - val_loss: 0.5883 - val_acc: 0.7878\n",
      "Epoch 107/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6514 - acc: 0.7490 - val_loss: 0.5956 - val_acc: 0.7756\n",
      "Epoch 108/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6529 - acc: 0.7572 - val_loss: 0.5823 - val_acc: 0.7878\n",
      "Epoch 109/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6514 - acc: 0.7547 - val_loss: 0.5759 - val_acc: 0.7967\n",
      "Epoch 110/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6629 - acc: 0.7557 - val_loss: 0.5752 - val_acc: 0.7900\n",
      "Epoch 111/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6320 - acc: 0.7621 - val_loss: 0.5776 - val_acc: 0.7928\n",
      "Epoch 112/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6401 - acc: 0.7521 - val_loss: 0.5938 - val_acc: 0.7894\n",
      "Epoch 113/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6397 - acc: 0.7532 - val_loss: 0.5772 - val_acc: 0.7844\n",
      "Epoch 114/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6573 - acc: 0.7526 - val_loss: 0.5985 - val_acc: 0.7717\n",
      "Epoch 115/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6502 - acc: 0.7526 - val_loss: 0.5880 - val_acc: 0.7889\n",
      "Epoch 116/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6447 - acc: 0.7551 - val_loss: 0.5664 - val_acc: 0.7972\n",
      "Epoch 117/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6496 - acc: 0.7597 - val_loss: 0.5873 - val_acc: 0.7900\n",
      "Epoch 118/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6364 - acc: 0.7550 - val_loss: 0.5606 - val_acc: 0.8089\n",
      "Epoch 119/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6363 - acc: 0.7644 - val_loss: 0.5528 - val_acc: 0.8033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6318 - acc: 0.7601 - val_loss: 0.5510 - val_acc: 0.8072\n",
      "Epoch 121/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6265 - acc: 0.7619 - val_loss: 0.5457 - val_acc: 0.8078\n",
      "Epoch 122/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6271 - acc: 0.7631 - val_loss: 0.5515 - val_acc: 0.8028\n",
      "Epoch 123/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6409 - acc: 0.7667 - val_loss: 0.5488 - val_acc: 0.8094\n",
      "Epoch 124/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6106 - acc: 0.7715 - val_loss: 0.5531 - val_acc: 0.8078\n",
      "Epoch 125/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6201 - acc: 0.7669 - val_loss: 0.5464 - val_acc: 0.8111\n",
      "Epoch 126/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6333 - acc: 0.7654 - val_loss: 0.5629 - val_acc: 0.8028\n",
      "Epoch 127/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6180 - acc: 0.7687 - val_loss: 0.5444 - val_acc: 0.8111\n",
      "Epoch 128/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6278 - acc: 0.7618 - val_loss: 0.5311 - val_acc: 0.8144\n",
      "Epoch 129/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6173 - acc: 0.7667 - val_loss: 0.5377 - val_acc: 0.8089\n",
      "Epoch 130/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6085 - acc: 0.7718 - val_loss: 0.5348 - val_acc: 0.8211\n",
      "Epoch 131/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6140 - acc: 0.7689 - val_loss: 0.5288 - val_acc: 0.8156\n",
      "Epoch 132/200\n",
      "7200/7200 [==============================] - 11s 2ms/step - loss: 0.6157 - acc: 0.7690 - val_loss: 0.5308 - val_acc: 0.8050\n",
      "Epoch 133/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6245 - acc: 0.7688 - val_loss: 0.5341 - val_acc: 0.8150\n",
      "Epoch 134/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6109 - acc: 0.7783 - val_loss: 0.5437 - val_acc: 0.8117\n",
      "Epoch 135/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6143 - acc: 0.7728 - val_loss: 0.5343 - val_acc: 0.8200\n",
      "Epoch 136/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5954 - acc: 0.7740 - val_loss: 0.5187 - val_acc: 0.8222\n",
      "Epoch 137/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5938 - acc: 0.7858 - val_loss: 0.5175 - val_acc: 0.8233\n",
      "Epoch 138/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6145 - acc: 0.7682 - val_loss: 0.5214 - val_acc: 0.8244\n",
      "Epoch 139/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6069 - acc: 0.7775 - val_loss: 0.5413 - val_acc: 0.8139\n",
      "Epoch 140/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6110 - acc: 0.7774 - val_loss: 0.5212 - val_acc: 0.8172\n",
      "Epoch 141/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6026 - acc: 0.7785 - val_loss: 0.5331 - val_acc: 0.8117\n",
      "Epoch 142/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5998 - acc: 0.7817 - val_loss: 0.5176 - val_acc: 0.8256\n",
      "Epoch 143/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5991 - acc: 0.7726 - val_loss: 0.5277 - val_acc: 0.8150\n",
      "Epoch 144/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5920 - acc: 0.7782 - val_loss: 0.5417 - val_acc: 0.8094\n",
      "Epoch 145/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5932 - acc: 0.7774 - val_loss: 0.5210 - val_acc: 0.8239\n",
      "Epoch 146/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5923 - acc: 0.7844 - val_loss: 0.5353 - val_acc: 0.8111\n",
      "Epoch 147/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.6017 - acc: 0.7735 - val_loss: 0.5424 - val_acc: 0.8067\n",
      "Epoch 148/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5947 - acc: 0.7806 - val_loss: 0.5144 - val_acc: 0.8228\n",
      "Epoch 149/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5925 - acc: 0.7787 - val_loss: 0.5077 - val_acc: 0.8239\n",
      "Epoch 150/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5777 - acc: 0.7839 - val_loss: 0.5018 - val_acc: 0.8217\n",
      "Epoch 151/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5961 - acc: 0.7756 - val_loss: 0.5028 - val_acc: 0.8228\n",
      "Epoch 152/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5875 - acc: 0.7869 - val_loss: 0.4876 - val_acc: 0.8289\n",
      "Epoch 153/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5817 - acc: 0.7794 - val_loss: 0.4911 - val_acc: 0.8256\n",
      "Epoch 154/200\n",
      "7200/7200 [==============================] - 14s 2ms/step - loss: 0.5834 - acc: 0.7865 - val_loss: 0.5065 - val_acc: 0.8283\n",
      "Epoch 155/200\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.5974 - acc: 0.7787 - val_loss: 0.5158 - val_acc: 0.8167\n",
      "Epoch 156/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5837 - acc: 0.7867 - val_loss: 0.5082 - val_acc: 0.8244\n",
      "Epoch 157/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5866 - acc: 0.7889 - val_loss: 0.4865 - val_acc: 0.8267\n",
      "Epoch 158/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5665 - acc: 0.7889 - val_loss: 0.4856 - val_acc: 0.8367\n",
      "Epoch 159/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5875 - acc: 0.7850 - val_loss: 0.5027 - val_acc: 0.8278\n",
      "Epoch 160/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5781 - acc: 0.7840 - val_loss: 0.5318 - val_acc: 0.8100\n",
      "Epoch 161/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5694 - acc: 0.7918 - val_loss: 0.5036 - val_acc: 0.8183\n",
      "Epoch 162/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5724 - acc: 0.7919 - val_loss: 0.4920 - val_acc: 0.8383\n",
      "Epoch 163/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5816 - acc: 0.7875 - val_loss: 0.4822 - val_acc: 0.8350\n",
      "Epoch 164/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5873 - acc: 0.7853 - val_loss: 0.4778 - val_acc: 0.8361\n",
      "Epoch 165/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5867 - acc: 0.7876 - val_loss: 0.5034 - val_acc: 0.8261\n",
      "Epoch 166/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5681 - acc: 0.7915 - val_loss: 0.4765 - val_acc: 0.8417\n",
      "Epoch 167/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5842 - acc: 0.7943 - val_loss: 0.4844 - val_acc: 0.8367\n",
      "Epoch 168/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5572 - acc: 0.7954 - val_loss: 0.4881 - val_acc: 0.8339\n",
      "Epoch 169/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5701 - acc: 0.7883 - val_loss: 0.4917 - val_acc: 0.8272\n",
      "Epoch 170/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5546 - acc: 0.7974 - val_loss: 0.4856 - val_acc: 0.8317\n",
      "Epoch 171/200\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.5742 - acc: 0.7913 - val_loss: 0.4828 - val_acc: 0.8378\n",
      "Epoch 172/200\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.5607 - acc: 0.7931 - val_loss: 0.4795 - val_acc: 0.8306\n",
      "Epoch 173/200\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.5592 - acc: 0.7947 - val_loss: 0.4943 - val_acc: 0.8289\n",
      "Epoch 174/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5429 - acc: 0.8004 - val_loss: 0.4657 - val_acc: 0.8383\n",
      "Epoch 175/200\n",
      "7200/7200 [==============================] - 13s 2ms/step - loss: 0.5606 - acc: 0.7937 - val_loss: 0.4763 - val_acc: 0.8411\n",
      "Epoch 176/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5547 - acc: 0.7915 - val_loss: 0.4824 - val_acc: 0.8294\n",
      "Epoch 177/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5548 - acc: 0.7997 - val_loss: 0.4825 - val_acc: 0.8250\n",
      "Epoch 178/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5565 - acc: 0.7982 - val_loss: 0.4799 - val_acc: 0.8378\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5433 - acc: 0.7994 - val_loss: 0.4867 - val_acc: 0.8317\n",
      "Epoch 180/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5434 - acc: 0.8032 - val_loss: 0.4908 - val_acc: 0.8322\n",
      "Epoch 181/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5335 - acc: 0.8001 - val_loss: 0.4666 - val_acc: 0.8450\n",
      "Epoch 182/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5434 - acc: 0.8017 - val_loss: 0.4649 - val_acc: 0.8411\n",
      "Epoch 183/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5399 - acc: 0.8017 - val_loss: 0.4637 - val_acc: 0.8428\n",
      "Epoch 184/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5366 - acc: 0.8053 - val_loss: 0.4991 - val_acc: 0.8261\n",
      "Epoch 185/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5346 - acc: 0.8012 - val_loss: 0.4773 - val_acc: 0.8322\n",
      "Epoch 186/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5629 - acc: 0.7986 - val_loss: 0.5106 - val_acc: 0.8172\n",
      "Epoch 187/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5544 - acc: 0.7990 - val_loss: 0.4881 - val_acc: 0.8311\n",
      "Epoch 188/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5477 - acc: 0.7999 - val_loss: 0.4744 - val_acc: 0.8344\n",
      "Epoch 189/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5383 - acc: 0.8047 - val_loss: 0.4555 - val_acc: 0.8422\n",
      "Epoch 190/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5266 - acc: 0.8122 - val_loss: 0.4507 - val_acc: 0.8372\n",
      "Epoch 191/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5242 - acc: 0.8097 - val_loss: 0.4829 - val_acc: 0.8378\n",
      "Epoch 192/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5324 - acc: 0.8054 - val_loss: 0.4699 - val_acc: 0.8433\n",
      "Epoch 193/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5290 - acc: 0.8111 - val_loss: 0.4880 - val_acc: 0.8400\n",
      "Epoch 194/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5284 - acc: 0.8144 - val_loss: 0.4829 - val_acc: 0.8306\n",
      "Epoch 195/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5248 - acc: 0.8108 - val_loss: 0.4708 - val_acc: 0.8417\n",
      "Epoch 196/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5432 - acc: 0.8007 - val_loss: 0.4725 - val_acc: 0.8383\n",
      "Epoch 197/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5475 - acc: 0.7961 - val_loss: 0.4633 - val_acc: 0.8378\n",
      "Epoch 198/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5208 - acc: 0.8099 - val_loss: 0.4753 - val_acc: 0.8328\n",
      "Epoch 199/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5320 - acc: 0.8101 - val_loss: 0.4635 - val_acc: 0.8383\n",
      "Epoch 200/200\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5191 - acc: 0.8094 - val_loss: 0.4755 - val_acc: 0.8356\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "batch_size = 300\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64, batch_input_shape=(batch_size, 256, 12), return_sequences=False, dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "STEPS = X_train.shape[0] // 20\n",
    "# VALID_STEPS = validation_generator.n // 20\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=batch_size, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (300, 64)                 19712     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (300, 64)                 4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (300, 64)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (300, 8)                  520       \n",
      "=================================================================\n",
      "Total params: 24,392\n",
      "Trainable params: 24,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(3000, 256, 12)\n",
      "(3000, 8)\n",
      "3000/3000 [==============================] - 2s 828us/step\n",
      "Test Loss: 0.4809246093034744\n",
      "Test accuracy: 0.8366666615009308\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "GUOFENG_CA3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
