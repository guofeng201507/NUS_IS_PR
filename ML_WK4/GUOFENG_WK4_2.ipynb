{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "GUOFENG_WK4_2.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "-DPHjHQ2mg9h",
    "colab_type": "code",
    "outputId": "9bd2d593-e57f-425f-f400-9797168e8186",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 16 13:26:00 2019\n",
    "\n",
    "@author: GUOFENG\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def implt(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Set up 'ggplot' style\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')  # if want to use the default style, set 'classic'\n",
    "plt.rcParams['ytick.right'] = True\n",
    "plt.rcParams['ytick.labelright'] = True\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.labelleft'] = False\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "# .............................................................................\n",
    "\n",
    "\n",
    "data = cifar10.load_data()\n",
    "(trDat, trLbl) = data[0]\n",
    "(tsDat, tsLbl) = data[1]\n",
    "\n",
    "# Convert the data into 'float32'\n",
    "# Rescale the values from 0~255 to 0~1\n",
    "trDat = trDat.astype('float32') / 255\n",
    "tsDat = tsDat.astype('float32') / 255\n",
    "\n",
    "# Retrieve the row size of each image\n",
    "# Retrieve the column size of each image\n",
    "imgrows = trDat.shape[1]\n",
    "imgclms = trDat.shape[2]\n",
    "channel = trDat.shape[3]\n",
    "\n",
    "# Perform one hot encoding on the labels\n",
    "# Retrieve the number of classes in this problem\n",
    "trLbl = to_categorical(trLbl)\n",
    "tsLbl = to_categorical(tsLbl)\n",
    "num_classes = tsLbl.shape[1]\n",
    "\n",
    "# .............................................................................\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I1en5sOgoo1F",
    "colab_type": "code",
    "outputId": "e5576b4a-a59a-460b-9547-29cf33b32a9d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 29\n",
    "np.random.seed(seed)\n",
    "\n",
    "optmz = optimizers.Adam(lr=0.001)\n",
    "modelname = 'cifar10ResV1Cfg5'\n",
    "\n",
    "\n",
    "# define the deep learning model\n",
    "\n",
    "\n",
    "def resLyr(inputs,\n",
    "           numFilters=16,\n",
    "           kernelSz=3,\n",
    "           strides=1,\n",
    "           activation='relu',\n",
    "           batchNorm=True,\n",
    "           convFirst=True,\n",
    "           lyrName=None):\n",
    "  \n",
    "    convLyr = Conv2D(numFilters,\n",
    "                     kernel_size=kernelSz,\n",
    "                     strides=strides,\n",
    "                     padding='same',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4),\n",
    "                     name=lyrName + '_conv' if lyrName else None)\n",
    "    x = inputs\n",
    "    if convFirst:\n",
    "        x = convLyr(x)\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "    else:\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization(name=lyrName + '_bn' if lyrName else None)(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation, name=lyrName + '_' + activation if lyrName else None)(x)\n",
    "        x = convLyr(x)\n",
    "  \n",
    "    return x\n",
    "\n",
    "\n",
    "def resBlkV1(inputs,\n",
    "             numFilters=16,\n",
    "             numBlocks=3,\n",
    "             downsampleOnFirst=True,\n",
    "             names=None):\n",
    "  \n",
    "    x = inputs\n",
    "    for run in range(0, numBlocks):\n",
    "        strides = 1\n",
    "        blkStr = str(run + 1)\n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            strides = 2\n",
    "            \n",
    "        y = resLyr(inputs=x, numFilters=numFilters, strides=strides, lyrName=names+'_Blk'+blkStr+'_Res1' if names else None)\n",
    "        y = resLyr(inputs=y, numFilters=numFilters, activation=None, lyrName=names+'_Blk'+blkStr+'_Res2' if names else None) \n",
    "        \n",
    "        if downsampleOnFirst and run == 0:\n",
    "            x = resLyr(inputs=x, numFilters=numFilters, kernelSz=1, \n",
    "                       strides=strides, activation=None, batchNorm=False, \n",
    "                       lyrName=names+'_Blk'+blkStr+'_lin' if names else None)\n",
    "\n",
    "        x = add([x, y], name=names+'_Blk'+blkStr+'_add' if names else None) \n",
    "\n",
    "        x = Activation('relu',  name=names+'_Blk'+blkStr+'_relu' if names else None)(x)   \n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def createResNetV1(inputShape=(32, 32, 3),\n",
    "                   numClasses=10):\n",
    "  \n",
    "    inputs = Input(shape=inputShape)\n",
    "    v = resLyr(inputs,\n",
    "               lyrName='Inpt')\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=16,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=False,\n",
    "                 names='Stg1')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=32,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg2')\n",
    "    v = Dropout(0.3)(v)\n",
    "    v = resBlkV1(inputs=v,\n",
    "                 numFilters=64,\n",
    "                 numBlocks=3,\n",
    "                 downsampleOnFirst=True,\n",
    "                 names='Stg3')\n",
    "    v = AveragePooling2D(pool_size=8,\n",
    "                         name='AvgPool')(v)\n",
    "    v = Flatten()(v)\n",
    "    outputs = Dense(numClasses,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(v)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optmz,\n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "    return model\n",
    "\n",
    "    # Setup the models\n",
    "\n",
    "\n",
    "model = createResNetV1()  # This is meant for training\n",
    "modelGo = createResNetV1()  # This is used for final testing\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# .............................................................................\n",
    "\n",
    "def lrSchedule(epoch):\n",
    "    lr = 1e-3\n",
    "\n",
    "    if epoch > 160:\n",
    "        lr *= 0.5e-3\n",
    "\n",
    "    elif epoch > 140:\n",
    "        lr *= 1e-3\n",
    "\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "\n",
    "    print('Learning rate: ', lr)\n",
    "\n",
    "    return lr\n",
    "\n",
    "\n",
    "LRScheduler = LearningRateScheduler(lrSchedule)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n__________________________________________________________________________________________________\nInpt_conv (Conv2D)              (None, 32, 32, 16)   448         input_1[0][0]                    \n__________________________________________________________________________________________________\nInpt_bn (BatchNormalization)    (None, 32, 32, 16)   64          Inpt_conv[0][0]                  \n__________________________________________________________________________________________________\nInpt_relu (Activation)          (None, 32, 32, 16)   0           Inpt_bn[0][0]                    \n__________________________________________________________________________________________________\nStg1_Blk1_Res1_conv (Conv2D)    (None, 32, 32, 16)   2320        Inpt_relu[0][0]                  \n__________________________________________________________________________________________________\nStg1_Blk1_Res1_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk1_Res1_conv[0][0]        \n__________________________________________________________________________________________________\nStg1_Blk1_Res1_relu (Activation (None, 32, 32, 16)   0           Stg1_Blk1_Res1_bn[0][0]          \n__________________________________________________________________________________________________\nStg1_Blk1_Res2_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk1_Res1_relu[0][0]        \n__________________________________________________________________________________________________\nStg1_Blk1_Res2_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk1_Res2_conv[0][0]        \n__________________________________________________________________________________________________\nStg1_Blk1_add (Add)             (None, 32, 32, 16)   0           Inpt_relu[0][0]                  \n                                                                 Stg1_Blk1_Res2_bn[0][0]          \n__________________________________________________________________________________________________\nStg1_Blk1_relu (Activation)     (None, 32, 32, 16)   0           Stg1_Blk1_add[0][0]              \n__________________________________________________________________________________________________\nStg1_Blk2_Res1_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk1_relu[0][0]             \n__________________________________________________________________________________________________\nStg1_Blk2_Res1_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk2_Res1_conv[0][0]        \n__________________________________________________________________________________________________\nStg1_Blk2_Res1_relu (Activation (None, 32, 32, 16)   0           Stg1_Blk2_Res1_bn[0][0]          \n__________________________________________________________________________________________________\nStg1_Blk2_Res2_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk2_Res1_relu[0][0]        \n__________________________________________________________________________________________________\nStg1_Blk2_Res2_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk2_Res2_conv[0][0]        \n__________________________________________________________________________________________________\nStg1_Blk2_add (Add)             (None, 32, 32, 16)   0           Stg1_Blk1_relu[0][0]             \n                                                                 Stg1_Blk2_Res2_bn[0][0]          \n__________________________________________________________________________________________________\nStg1_Blk2_relu (Activation)     (None, 32, 32, 16)   0           Stg1_Blk2_add[0][0]              \n__________________________________________________________________________________________________\nStg1_Blk3_Res1_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk2_relu[0][0]             \n__________________________________________________________________________________________________\nStg1_Blk3_Res1_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk3_Res1_conv[0][0]        \n__________________________________________________________________________________________________\nStg1_Blk3_Res1_relu (Activation (None, 32, 32, 16)   0           Stg1_Blk3_Res1_bn[0][0]          \n__________________________________________________________________________________________________\nStg1_Blk3_Res2_conv (Conv2D)    (None, 32, 32, 16)   2320        Stg1_Blk3_Res1_relu[0][0]        \n__________________________________________________________________________________________________\nStg1_Blk3_Res2_bn (BatchNormali (None, 32, 32, 16)   64          Stg1_Blk3_Res2_conv[0][0]        \n__________________________________________________________________________________________________\nStg1_Blk3_add (Add)             (None, 32, 32, 16)   0           Stg1_Blk2_relu[0][0]             \n                                                                 Stg1_Blk3_Res2_bn[0][0]          \n__________________________________________________________________________________________________\nStg1_Blk3_relu (Activation)     (None, 32, 32, 16)   0           Stg1_Blk3_add[0][0]              \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 32, 32, 16)   0           Stg1_Blk3_relu[0][0]             \n__________________________________________________________________________________________________\nStg2_Blk1_Res1_conv (Conv2D)    (None, 16, 16, 32)   4640        dropout[0][0]                    \n__________________________________________________________________________________________________\nStg2_Blk1_Res1_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk1_Res1_conv[0][0]        \n__________________________________________________________________________________________________\nStg2_Blk1_Res1_relu (Activation (None, 16, 16, 32)   0           Stg2_Blk1_Res1_bn[0][0]          \n__________________________________________________________________________________________________\nStg2_Blk1_Res2_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk1_Res1_relu[0][0]        \n__________________________________________________________________________________________________\nStg2_Blk1_lin_conv (Conv2D)     (None, 16, 16, 32)   544         dropout[0][0]                    \n__________________________________________________________________________________________________\nStg2_Blk1_Res2_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk1_Res2_conv[0][0]        \n__________________________________________________________________________________________________\nStg2_Blk1_add (Add)             (None, 16, 16, 32)   0           Stg2_Blk1_lin_conv[0][0]         \n                                                                 Stg2_Blk1_Res2_bn[0][0]          \n__________________________________________________________________________________________________\nStg2_Blk1_relu (Activation)     (None, 16, 16, 32)   0           Stg2_Blk1_add[0][0]              \n__________________________________________________________________________________________________\nStg2_Blk2_Res1_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk1_relu[0][0]             \n__________________________________________________________________________________________________\nStg2_Blk2_Res1_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk2_Res1_conv[0][0]        \n__________________________________________________________________________________________________\nStg2_Blk2_Res1_relu (Activation (None, 16, 16, 32)   0           Stg2_Blk2_Res1_bn[0][0]          \n__________________________________________________________________________________________________\nStg2_Blk2_Res2_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk2_Res1_relu[0][0]        \n__________________________________________________________________________________________________\nStg2_Blk2_Res2_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk2_Res2_conv[0][0]        \n__________________________________________________________________________________________________\nStg2_Blk2_add (Add)             (None, 16, 16, 32)   0           Stg2_Blk1_relu[0][0]             \n                                                                 Stg2_Blk2_Res2_bn[0][0]          \n__________________________________________________________________________________________________\nStg2_Blk2_relu (Activation)     (None, 16, 16, 32)   0           Stg2_Blk2_add[0][0]              \n__________________________________________________________________________________________________\nStg2_Blk3_Res1_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk2_relu[0][0]             \n__________________________________________________________________________________________________\nStg2_Blk3_Res1_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk3_Res1_conv[0][0]        \n__________________________________________________________________________________________________\nStg2_Blk3_Res1_relu (Activation (None, 16, 16, 32)   0           Stg2_Blk3_Res1_bn[0][0]          \n__________________________________________________________________________________________________\nStg2_Blk3_Res2_conv (Conv2D)    (None, 16, 16, 32)   9248        Stg2_Blk3_Res1_relu[0][0]        \n__________________________________________________________________________________________________\nStg2_Blk3_Res2_bn (BatchNormali (None, 16, 16, 32)   128         Stg2_Blk3_Res2_conv[0][0]        \n__________________________________________________________________________________________________\nStg2_Blk3_add (Add)             (None, 16, 16, 32)   0           Stg2_Blk2_relu[0][0]             \n                                                                 Stg2_Blk3_Res2_bn[0][0]          \n__________________________________________________________________________________________________\nStg2_Blk3_relu (Activation)     (None, 16, 16, 32)   0           Stg2_Blk3_add[0][0]              \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 16, 16, 32)   0           Stg2_Blk3_relu[0][0]             \n__________________________________________________________________________________________________\nStg3_Blk1_Res1_conv (Conv2D)    (None, 8, 8, 64)     18496       dropout_1[0][0]                  \n__________________________________________________________________________________________________\nStg3_Blk1_Res1_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk1_Res1_conv[0][0]        \n__________________________________________________________________________________________________\nStg3_Blk1_Res1_relu (Activation (None, 8, 8, 64)     0           Stg3_Blk1_Res1_bn[0][0]          \n__________________________________________________________________________________________________\nStg3_Blk1_Res2_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk1_Res1_relu[0][0]        \n__________________________________________________________________________________________________\nStg3_Blk1_lin_conv (Conv2D)     (None, 8, 8, 64)     2112        dropout_1[0][0]                  \n__________________________________________________________________________________________________\nStg3_Blk1_Res2_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk1_Res2_conv[0][0]        \n__________________________________________________________________________________________________\nStg3_Blk1_add (Add)             (None, 8, 8, 64)     0           Stg3_Blk1_lin_conv[0][0]         \n                                                                 Stg3_Blk1_Res2_bn[0][0]          \n__________________________________________________________________________________________________\nStg3_Blk1_relu (Activation)     (None, 8, 8, 64)     0           Stg3_Blk1_add[0][0]              \n__________________________________________________________________________________________________\nStg3_Blk2_Res1_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk1_relu[0][0]             \n__________________________________________________________________________________________________\nStg3_Blk2_Res1_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk2_Res1_conv[0][0]        \n__________________________________________________________________________________________________\nStg3_Blk2_Res1_relu (Activation (None, 8, 8, 64)     0           Stg3_Blk2_Res1_bn[0][0]          \n__________________________________________________________________________________________________\nStg3_Blk2_Res2_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk2_Res1_relu[0][0]        \n__________________________________________________________________________________________________\nStg3_Blk2_Res2_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk2_Res2_conv[0][0]        \n__________________________________________________________________________________________________\nStg3_Blk2_add (Add)             (None, 8, 8, 64)     0           Stg3_Blk1_relu[0][0]             \n                                                                 Stg3_Blk2_Res2_bn[0][0]          \n__________________________________________________________________________________________________\nStg3_Blk2_relu (Activation)     (None, 8, 8, 64)     0           Stg3_Blk2_add[0][0]              \n__________________________________________________________________________________________________\nStg3_Blk3_Res1_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk2_relu[0][0]             \n__________________________________________________________________________________________________\nStg3_Blk3_Res1_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk3_Res1_conv[0][0]        \n__________________________________________________________________________________________________\nStg3_Blk3_Res1_relu (Activation (None, 8, 8, 64)     0           Stg3_Blk3_Res1_bn[0][0]          \n__________________________________________________________________________________________________\nStg3_Blk3_Res2_conv (Conv2D)    (None, 8, 8, 64)     36928       Stg3_Blk3_Res1_relu[0][0]        \n__________________________________________________________________________________________________\nStg3_Blk3_Res2_bn (BatchNormali (None, 8, 8, 64)     256         Stg3_Blk3_Res2_conv[0][0]        \n__________________________________________________________________________________________________\nStg3_Blk3_add (Add)             (None, 8, 8, 64)     0           Stg3_Blk2_relu[0][0]             \n                                                                 Stg3_Blk3_Res2_bn[0][0]          \n__________________________________________________________________________________________________\nStg3_Blk3_relu (Activation)     (None, 8, 8, 64)     0           Stg3_Blk3_add[0][0]              \n__________________________________________________________________________________________________\nAvgPool (AveragePooling2D)      (None, 1, 1, 64)     0           Stg3_Blk3_relu[0][0]             \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 64)           0           AvgPool[0][0]                    \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 10)           650         flatten[0][0]                    \n==================================================================================================\nTotal params: 274,442\nTrainable params: 273,066\nNon-trainable params: 1,376\n__________________________________________________________________________________________________\nNone\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GXb2lOY0opB8",
    "colab_type": "code",
    "outputId": "0191dde2-dc5e-4bfb-e089-7d8b61b2888b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Create checkpoint for the training\n",
    "# This checkpoint performs model saving when\n",
    "# an epoch gives highest testing accuracy\n",
    "filepath = modelname + \".hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "# Log the epoch detail into csv\n",
    "csv_logger = CSVLogger(modelname + '.csv')\n",
    "callbacks_list = [checkpoint, csv_logger, LRScheduler]\n",
    "\n",
    "# .............................................................................\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             rotation_range=20,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=False)\n",
    "\n",
    "model.fit_generator(datagen.flow(trDat, trLbl, batch_size=32),\n",
    "                    validation_data=(tsDat, tsLbl),\n",
    "                    epochs=200,\n",
    "                    verbose=2,\n",
    "                    steps_per_epoch=len(trDat) / 32,\n",
    "                    callbacks=callbacks_list)\n",
    "\n",
    "# ......................................................................\n",
    "\n",
    "\n",
    "# Now the training is complete, we get\n",
    "# another object to load the weights\n",
    "# compile it, so that we can do \n",
    "# final evaluation on it\n",
    "modelGo.load_weights(filepath)\n",
    "modelGo.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optmz,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# .......................................................................\n",
    "\n",
    "\n",
    "# Make classification on the test dataset\n",
    "predicts = modelGo.predict(tsDat)\n",
    "\n",
    "# Prepare the classification output\n",
    "# for the classification report\n",
    "predout = np.argmax(predicts, axis=1)\n",
    "testout = np.argmax(tsLbl, axis=1)\n",
    "labelname = ['airplane',\n",
    "             'automobile',\n",
    "             'bird',\n",
    "             'cat',\n",
    "             'deer',\n",
    "             'dog',\n",
    "             'frog',\n",
    "             'horse',\n",
    "             'ship',\n",
    "             'truck']\n",
    "# the labels for the classfication report\n",
    "\n",
    "\n",
    "testScores = metrics.accuracy_score(testout, predout)\n",
    "confusion = metrics.confusion_matrix(testout, predout)\n",
    "\n",
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores * 100))\n",
    "print(metrics.classification_report(testout, predout, target_names=labelname, digits=4))\n",
    "print(confusion)\n",
    "\n",
    "# ..................................................................\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "records = pd.read_csv(modelname + '.csv')\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(records['val_loss'])\n",
    "plt.plot(records['loss'])\n",
    "plt.yticks([0, 0.20, 0.40, 0.60, 0.80, 1.00])\n",
    "plt.title('Loss value', fontsize=12)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(records['val_acc'])\n",
    "plt.plot(records['acc'])\n",
    "plt.yticks([0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "plt.title('Accuracy', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model,\n",
    "           to_file=modelname + '_model.pdf',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=False,\n",
    "           rankdir='TB')"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Learning rate:  0.001\nEpoch 1/200\n",
      "1563/1562 - 40s - loss: 1.7023 - acc: 0.4394 - val_loss: 1.7859 - val_acc: 0.4756\n",
      "Learning rate:  0.001\nEpoch 2/200\n",
      "1563/1562 - 30s - loss: 1.3640 - acc: 0.5649 - val_loss: 1.5747 - val_acc: 0.5439\n",
      "Learning rate:  0.001\nEpoch 3/200\n",
      "1563/1562 - 31s - loss: 1.2130 - acc: 0.6215 - val_loss: 1.1003 - val_acc: 0.6683\n",
      "Learning rate:  0.001\nEpoch 4/200\n",
      "1563/1562 - 30s - loss: 1.1263 - acc: 0.6543 - val_loss: 1.3295 - val_acc: 0.6145\n",
      "Learning rate:  0.001\nEpoch 5/200\n",
      "1563/1562 - 30s - loss: 1.0527 - acc: 0.6818 - val_loss: 1.2148 - val_acc: 0.6476\n",
      "Learning rate:  0.001\nEpoch 6/200\n",
      "1563/1562 - 29s - loss: 0.9983 - acc: 0.7052 - val_loss: 1.1612 - val_acc: 0.6735\n",
      "Learning rate:  0.001\nEpoch 7/200\n",
      "1563/1562 - 30s - loss: 0.9550 - acc: 0.7192 - val_loss: 1.0108 - val_acc: 0.7082\n",
      "Learning rate:  0.001\nEpoch 8/200\n",
      "1563/1562 - 30s - loss: 0.9243 - acc: 0.7334 - val_loss: 1.1985 - val_acc: 0.6692\n",
      "Learning rate:  0.001\nEpoch 9/200\n",
      "1563/1562 - 30s - loss: 0.8959 - acc: 0.7444 - val_loss: 0.9482 - val_acc: 0.7297\n",
      "Learning rate:  0.001\nEpoch 10/200\n",
      "1563/1562 - 30s - loss: 0.8715 - acc: 0.7535 - val_loss: 0.9851 - val_acc: 0.7306\n",
      "Learning rate:  0.001\nEpoch 11/200\n",
      "1563/1562 - 30s - loss: 0.8474 - acc: 0.7633 - val_loss: 1.0811 - val_acc: 0.7158\n",
      "Learning rate:  0.001\nEpoch 12/200\n",
      "1563/1562 - 31s - loss: 0.8315 - acc: 0.7709 - val_loss: 0.7899 - val_acc: 0.7894\n",
      "Learning rate:  0.001\nEpoch 13/200\n",
      "1563/1562 - 29s - loss: 0.8135 - acc: 0.7757 - val_loss: 0.9644 - val_acc: 0.7390\n",
      "Learning rate:  0.001\nEpoch 14/200\n",
      "1563/1562 - 30s - loss: 0.8020 - acc: 0.7818 - val_loss: 0.7287 - val_acc: 0.8102\n",
      "Learning rate:  0.001\nEpoch 15/200\n",
      "1563/1562 - 30s - loss: 0.7965 - acc: 0.7850 - val_loss: 0.9605 - val_acc: 0.7415\n",
      "Learning rate:  0.001\nEpoch 16/200\n",
      "1563/1562 - 30s - loss: 0.7823 - acc: 0.7902 - val_loss: 0.8290 - val_acc: 0.7827\n",
      "Learning rate:  0.001\nEpoch 17/200\n",
      "1563/1562 - 30s - loss: 0.7714 - acc: 0.7961 - val_loss: 0.9445 - val_acc: 0.7559\n",
      "Learning rate:  0.001\nEpoch 18/200\n",
      "1563/1562 - 30s - loss: 0.7629 - acc: 0.7976 - val_loss: 0.7801 - val_acc: 0.7926\n",
      "Learning rate:  0.001\nEpoch 19/200\n",
      "1563/1562 - 30s - loss: 0.7560 - acc: 0.7994 - val_loss: 0.8838 - val_acc: 0.7809\n",
      "Learning rate:  0.001\nEpoch 20/200\n",
      "1563/1562 - 30s - loss: 0.7478 - acc: 0.8050 - val_loss: 0.8518 - val_acc: 0.7796\n",
      "Learning rate:  0.001\nEpoch 21/200\n",
      "1563/1562 - 31s - loss: 0.7414 - acc: 0.8060 - val_loss: 0.6961 - val_acc: 0.8235\n",
      "Learning rate:  0.001\nEpoch 22/200\n",
      "1563/1562 - 30s - loss: 0.7348 - acc: 0.8113 - val_loss: 0.8136 - val_acc: 0.7863\n",
      "Learning rate:  0.001\nEpoch 23/200\n",
      "1563/1562 - 29s - loss: 0.7319 - acc: 0.8097 - val_loss: 0.8099 - val_acc: 0.7930\n",
      "Learning rate:  0.001\nEpoch 24/200\n",
      "1563/1562 - 30s - loss: 0.7282 - acc: 0.8135 - val_loss: 0.7070 - val_acc: 0.8243\n",
      "Learning rate:  0.001\nEpoch 25/200\n",
      "1563/1562 - 30s - loss: 0.7207 - acc: 0.8168 - val_loss: 0.7459 - val_acc: 0.8119\n",
      "Learning rate:  0.001\nEpoch 26/200\n",
      "1563/1562 - 30s - loss: 0.7173 - acc: 0.8185 - val_loss: 0.7521 - val_acc: 0.8146\n",
      "Learning rate:  0.001\nEpoch 27/200\n",
      "1563/1562 - 30s - loss: 0.7097 - acc: 0.8212 - val_loss: 0.7385 - val_acc: 0.8141\n",
      "Learning rate:  0.001\nEpoch 28/200\n",
      "1563/1562 - 33s - loss: 0.7103 - acc: 0.8204 - val_loss: 0.7109 - val_acc: 0.8295\n",
      "Learning rate:  0.001\nEpoch 29/200\n",
      "1563/1562 - 37s - loss: 0.7084 - acc: 0.8203 - val_loss: 0.7108 - val_acc: 0.8257\n",
      "Learning rate:  0.001\nEpoch 30/200\n",
      "1563/1562 - 30s - loss: 0.6972 - acc: 0.8237 - val_loss: 0.7960 - val_acc: 0.8062\n",
      "Learning rate:  0.001\nEpoch 31/200\n",
      "1563/1562 - 31s - loss: 0.6987 - acc: 0.8251 - val_loss: 0.8342 - val_acc: 0.7907\n",
      "Learning rate:  0.001\nEpoch 32/200\n",
      "1563/1562 - 30s - loss: 0.6932 - acc: 0.8256 - val_loss: 0.8316 - val_acc: 0.7975\n",
      "Learning rate:  0.001\nEpoch 33/200\n",
      "1563/1562 - 30s - loss: 0.6912 - acc: 0.8273 - val_loss: 0.7528 - val_acc: 0.8169\n",
      "Learning rate:  0.001\nEpoch 34/200\n",
      "1563/1562 - 30s - loss: 0.6899 - acc: 0.8277 - val_loss: 0.9444 - val_acc: 0.7728\n",
      "Learning rate:  0.001\nEpoch 35/200\n",
      "1563/1562 - 30s - loss: 0.6861 - acc: 0.8311 - val_loss: 0.6713 - val_acc: 0.8375\n",
      "Learning rate:  0.001\nEpoch 36/200\n",
      "1563/1562 - 30s - loss: 0.6830 - acc: 0.8321 - val_loss: 0.6816 - val_acc: 0.8386\n",
      "Learning rate:  0.001\nEpoch 37/200\n",
      "1563/1562 - 30s - loss: 0.6822 - acc: 0.8314 - val_loss: 0.7147 - val_acc: 0.8244\n",
      "Learning rate:  0.001\nEpoch 38/200\n",
      "1563/1562 - 29s - loss: 0.6812 - acc: 0.8330 - val_loss: 0.7383 - val_acc: 0.8196\n",
      "Learning rate:  0.001\nEpoch 39/200\n",
      "1563/1562 - 30s - loss: 0.6786 - acc: 0.8332 - val_loss: 0.7082 - val_acc: 0.8289\n",
      "Learning rate:  0.001\nEpoch 40/200\n",
      "1563/1562 - 30s - loss: 0.6771 - acc: 0.8336 - val_loss: 0.8505 - val_acc: 0.7921\n",
      "Learning rate:  0.001\nEpoch 41/200\n",
      "1563/1562 - 30s - loss: 0.6699 - acc: 0.8364 - val_loss: 0.7473 - val_acc: 0.8208\n",
      "Learning rate:  0.001\nEpoch 42/200\n",
      "1563/1562 - 35s - loss: 0.6683 - acc: 0.8370 - val_loss: 0.7673 - val_acc: 0.8087\n",
      "Learning rate:  0.001\nEpoch 43/200\n",
      "1563/1562 - 35s - loss: 0.6708 - acc: 0.8368 - val_loss: 0.6573 - val_acc: 0.8475\n",
      "Learning rate:  0.001\nEpoch 44/200\n",
      "1563/1562 - 33s - loss: 0.6668 - acc: 0.8373 - val_loss: 0.6364 - val_acc: 0.8492\n",
      "Learning rate:  0.001\nEpoch 45/200\n",
      "1563/1562 - 30s - loss: 0.6672 - acc: 0.8379 - val_loss: 0.8376 - val_acc: 0.7944\n",
      "Learning rate:  0.001\nEpoch 46/200\n",
      "1563/1562 - 33s - loss: 0.6652 - acc: 0.8392 - val_loss: 0.7070 - val_acc: 0.8258\n",
      "Learning rate:  0.001\nEpoch 47/200\n",
      "1563/1562 - 30s - loss: 0.6592 - acc: 0.8399 - val_loss: 0.7314 - val_acc: 0.8226\n",
      "Learning rate:  0.001\nEpoch 48/200\n",
      "1563/1562 - 30s - loss: 0.6604 - acc: 0.8384 - val_loss: 0.6854 - val_acc: 0.8339\n",
      "Learning rate:  0.001\nEpoch 49/200\n",
      "1563/1562 - 31s - loss: 0.6596 - acc: 0.8399 - val_loss: 0.7171 - val_acc: 0.8223\n",
      "Learning rate:  0.001\nEpoch 50/200\n",
      "1563/1562 - 29s - loss: 0.6563 - acc: 0.8399 - val_loss: 0.6696 - val_acc: 0.8401\n",
      "Learning rate:  0.001\nEpoch 51/200\n",
      "1563/1562 - 29s - loss: 0.6497 - acc: 0.8439 - val_loss: 0.7505 - val_acc: 0.8211\n",
      "Learning rate:  0.001\nEpoch 52/200\n",
      "1563/1562 - 29s - loss: 0.6553 - acc: 0.8405 - val_loss: 0.7476 - val_acc: 0.8238\n",
      "Learning rate:  0.001\nEpoch 53/200\n",
      "1563/1562 - 30s - loss: 0.6523 - acc: 0.8425 - val_loss: 0.6625 - val_acc: 0.8464\n",
      "Learning rate:  0.001\nEpoch 54/200\n",
      "1563/1562 - 29s - loss: 0.6552 - acc: 0.8424 - val_loss: 0.8090 - val_acc: 0.8106\n",
      "Learning rate:  0.001\nEpoch 55/200\n",
      "1563/1562 - 30s - loss: 0.6481 - acc: 0.8442 - val_loss: 0.6819 - val_acc: 0.8431\n",
      "Learning rate:  0.001\nEpoch 56/200\n",
      "1563/1562 - 29s - loss: 0.6444 - acc: 0.8458 - val_loss: 0.6711 - val_acc: 0.8416\n",
      "Learning rate:  0.001\nEpoch 57/200\n",
      "1563/1562 - 30s - loss: 0.6438 - acc: 0.8464 - val_loss: 0.6582 - val_acc: 0.8497\n",
      "Learning rate:  0.001\nEpoch 58/200\n",
      "1563/1562 - 30s - loss: 0.6425 - acc: 0.8473 - val_loss: 0.6461 - val_acc: 0.8507\n",
      "Learning rate:  0.001\nEpoch 59/200\n",
      "1563/1562 - 29s - loss: 0.6442 - acc: 0.8449 - val_loss: 0.6834 - val_acc: 0.8399\n",
      "Learning rate:  0.001\nEpoch 60/200\n",
      "1563/1562 - 30s - loss: 0.6446 - acc: 0.8452 - val_loss: 0.9315 - val_acc: 0.7756\n",
      "Learning rate:  0.001\nEpoch 61/200\n",
      "1563/1562 - 31s - loss: 0.6367 - acc: 0.8488 - val_loss: 0.7518 - val_acc: 0.8226\n",
      "Learning rate:  0.001\nEpoch 62/200\n",
      "1563/1562 - 30s - loss: 0.6409 - acc: 0.8469 - val_loss: 0.6507 - val_acc: 0.8495\n",
      "Learning rate:  0.001\nEpoch 63/200\n",
      "1563/1562 - 30s - loss: 0.6346 - acc: 0.8475 - val_loss: 0.7422 - val_acc: 0.8208\n",
      "Learning rate:  0.001\nEpoch 64/200\n",
      "1563/1562 - 30s - loss: 0.6375 - acc: 0.8458 - val_loss: 0.8176 - val_acc: 0.8060\n",
      "Learning rate:  0.001\nEpoch 65/200\n",
      "1563/1562 - 30s - loss: 0.6359 - acc: 0.8479 - val_loss: 0.6073 - val_acc: 0.8624\n",
      "Learning rate:  0.001\nEpoch 66/200\n",
      "1563/1562 - 30s - loss: 0.6303 - acc: 0.8490 - val_loss: 0.6831 - val_acc: 0.8446\n",
      "Learning rate:  0.001\nEpoch 67/200\n",
      "1563/1562 - 30s - loss: 0.6326 - acc: 0.8491 - val_loss: 0.6521 - val_acc: 0.8487\n",
      "Learning rate:  0.001\nEpoch 68/200\n",
      "1563/1562 - 30s - loss: 0.6329 - acc: 0.8499 - val_loss: 0.7521 - val_acc: 0.8178\n",
      "Learning rate:  0.001\nEpoch 69/200\n",
      "1563/1562 - 30s - loss: 0.6403 - acc: 0.8484 - val_loss: 0.7928 - val_acc: 0.8118\n",
      "Learning rate:  0.001\nEpoch 70/200\n",
      "1563/1562 - 30s - loss: 0.6287 - acc: 0.8516 - val_loss: 0.8188 - val_acc: 0.7988\n",
      "Learning rate:  0.001\nEpoch 71/200\n",
      "1563/1562 - 30s - loss: 0.6276 - acc: 0.8508 - val_loss: 0.6651 - val_acc: 0.8430\n",
      "Learning rate:  0.001\nEpoch 72/200\n",
      "1563/1562 - 33s - loss: 0.6246 - acc: 0.8513 - val_loss: 0.6596 - val_acc: 0.8533\n",
      "Learning rate:  0.001\nEpoch 73/200\n",
      "1563/1562 - 31s - loss: 0.6275 - acc: 0.8505 - val_loss: 0.8504 - val_acc: 0.8011\n",
      "Learning rate:  0.001\nEpoch 74/200\n",
      "1563/1562 - 30s - loss: 0.6266 - acc: 0.8526 - val_loss: 0.7900 - val_acc: 0.8164\n",
      "Learning rate:  0.001\nEpoch 75/200\n",
      "1563/1562 - 30s - loss: 0.6238 - acc: 0.8529 - val_loss: 0.6697 - val_acc: 0.8436\n",
      "Learning rate:  0.001\nEpoch 76/200\n",
      "1563/1562 - 30s - loss: 0.6228 - acc: 0.8527 - val_loss: 0.7071 - val_acc: 0.8360\n",
      "Learning rate:  0.001\nEpoch 77/200\n",
      "1563/1562 - 30s - loss: 0.6184 - acc: 0.8540 - val_loss: 0.6476 - val_acc: 0.8445\n",
      "Learning rate:  0.001\nEpoch 78/200\n",
      "1563/1562 - 31s - loss: 0.6230 - acc: 0.8532 - val_loss: 0.6924 - val_acc: 0.8330\n",
      "Learning rate:  0.001\nEpoch 79/200\n",
      "1563/1562 - 32s - loss: 0.6196 - acc: 0.8545 - val_loss: 0.6631 - val_acc: 0.8416\n",
      "Learning rate:  0.001\nEpoch 80/200\n",
      "1563/1562 - 29s - loss: 0.6190 - acc: 0.8541 - val_loss: 0.6833 - val_acc: 0.8416\n",
      "Learning rate:  0.001\nEpoch 81/200\n",
      "1563/1562 - 33s - loss: 0.6210 - acc: 0.8540 - val_loss: 0.6277 - val_acc: 0.8578\n",
      "Learning rate:  0.0001\nEpoch 82/200\n",
      "1563/1562 - 31s - loss: 0.5432 - acc: 0.8819 - val_loss: 0.5316 - val_acc: 0.8867\n",
      "Learning rate:  0.0001\nEpoch 83/200\n",
      "1563/1562 - 30s - loss: 0.5036 - acc: 0.8928 - val_loss: 0.5072 - val_acc: 0.8918\n",
      "Learning rate:  0.0001\nEpoch 84/200\n",
      "1563/1562 - 29s - loss: 0.4905 - acc: 0.8951 - val_loss: 0.5038 - val_acc: 0.8948\n",
      "Learning rate:  0.0001\nEpoch 85/200\n",
      "1563/1562 - 29s - loss: 0.4846 - acc: 0.8970 - val_loss: 0.4880 - val_acc: 0.8966\n",
      "Learning rate:  0.0001\nEpoch 86/200\n",
      "1563/1562 - 30s - loss: 0.4699 - acc: 0.9009 - val_loss: 0.4781 - val_acc: 0.9024\n",
      "Learning rate:  0.0001\nEpoch 87/200\n",
      "1563/1562 - 29s - loss: 0.4606 - acc: 0.9023 - val_loss: 0.4896 - val_acc: 0.8961\n",
      "Learning rate:  0.0001\nEpoch 88/200\n",
      "1563/1562 - 29s - loss: 0.4563 - acc: 0.9032 - val_loss: 0.4820 - val_acc: 0.8999\n",
      "Learning rate:  0.0001\nEpoch 89/200\n",
      "1563/1562 - 30s - loss: 0.4479 - acc: 0.9038 - val_loss: 0.4566 - val_acc: 0.9066\n",
      "Learning rate:  0.0001\nEpoch 90/200\n",
      "1563/1562 - 29s - loss: 0.4456 - acc: 0.9054 - val_loss: 0.4776 - val_acc: 0.9006\n",
      "Learning rate:  0.0001\nEpoch 91/200\n",
      "1563/1562 - 30s - loss: 0.4419 - acc: 0.9056 - val_loss: 0.4732 - val_acc: 0.9005\n",
      "Learning rate:  0.0001\nEpoch 92/200\n",
      "1563/1562 - 29s - loss: 0.4348 - acc: 0.9079 - val_loss: 0.4632 - val_acc: 0.9037\n",
      "Learning rate:  0.0001\nEpoch 93/200\n",
      "1563/1562 - 30s - loss: 0.4290 - acc: 0.9078 - val_loss: 0.4553 - val_acc: 0.9059\n",
      "Learning rate:  0.0001\nEpoch 94/200\n",
      "1563/1562 - 30s - loss: 0.4253 - acc: 0.9093 - val_loss: 0.4481 - val_acc: 0.9074\n",
      "Learning rate:  0.0001\nEpoch 95/200\n",
      "1563/1562 - 30s - loss: 0.4232 - acc: 0.9096 - val_loss: 0.4419 - val_acc: 0.9077\n",
      "Learning rate:  0.0001\nEpoch 96/200\n",
      "1563/1562 - 29s - loss: 0.4191 - acc: 0.9102 - val_loss: 0.4516 - val_acc: 0.9041\n",
      "Learning rate:  0.0001\nEpoch 97/200\n",
      "1563/1562 - 30s - loss: 0.4125 - acc: 0.9112 - val_loss: 0.4511 - val_acc: 0.9036\n",
      "Learning rate:  0.0001\nEpoch 98/200\n",
      "1563/1562 - 30s - loss: 0.4128 - acc: 0.9110 - val_loss: 0.4481 - val_acc: 0.9054\n",
      "Learning rate:  0.0001\nEpoch 99/200\n",
      "1563/1562 - 30s - loss: 0.4071 - acc: 0.9116 - val_loss: 0.4544 - val_acc: 0.9039\n",
      "Learning rate:  0.0001\nEpoch 100/200\n",
      "1563/1562 - 29s - loss: 0.4035 - acc: 0.9126 - val_loss: 0.4396 - val_acc: 0.9049\n",
      "Learning rate:  0.0001\nEpoch 101/200\n",
      "1563/1562 - 30s - loss: 0.3953 - acc: 0.9145 - val_loss: 0.4356 - val_acc: 0.9068\n",
      "Learning rate:  0.0001\nEpoch 102/200\n",
      "1563/1562 - 29s - loss: 0.3957 - acc: 0.9135 - val_loss: 0.4699 - val_acc: 0.8979\n",
      "Learning rate:  0.0001\nEpoch 103/200\n",
      "1563/1562 - 30s - loss: 0.3955 - acc: 0.9131 - val_loss: 0.4333 - val_acc: 0.9065\n",
      "Learning rate:  0.0001\nEpoch 104/200\n",
      "1563/1562 - 29s - loss: 0.3904 - acc: 0.9142 - val_loss: 0.4314 - val_acc: 0.9071\n",
      "Learning rate:  0.0001\nEpoch 105/200\n",
      "1563/1562 - 30s - loss: 0.3875 - acc: 0.9151 - val_loss: 0.4288 - val_acc: 0.9058\n",
      "Learning rate:  0.0001\nEpoch 106/200\n",
      "1563/1562 - 29s - loss: 0.3849 - acc: 0.9157 - val_loss: 0.4467 - val_acc: 0.9034\n",
      "Learning rate:  0.0001\nEpoch 107/200\n",
      "1563/1562 - 30s - loss: 0.3820 - acc: 0.9161 - val_loss: 0.4330 - val_acc: 0.9059\n",
      "Learning rate:  0.0001\nEpoch 108/200\n",
      "1563/1562 - 31s - loss: 0.3787 - acc: 0.9178 - val_loss: 0.4248 - val_acc: 0.9082\n",
      "Learning rate:  0.0001\nEpoch 109/200\n",
      "1563/1562 - 31s - loss: 0.3776 - acc: 0.9169 - val_loss: 0.4123 - val_acc: 0.9107\n",
      "Learning rate:  0.0001\nEpoch 110/200\n",
      "1563/1562 - 30s - loss: 0.3746 - acc: 0.9171 - val_loss: 0.4176 - val_acc: 0.9093\n",
      "Learning rate:  0.0001\nEpoch 111/200\n",
      "1563/1562 - 30s - loss: 0.3687 - acc: 0.9203 - val_loss: 0.4219 - val_acc: 0.9064\n",
      "Learning rate:  0.0001\nEpoch 112/200\n",
      "1563/1562 - 30s - loss: 0.3664 - acc: 0.9194 - val_loss: 0.4256 - val_acc: 0.9067\n",
      "Learning rate:  0.0001\nEpoch 113/200\n",
      "1563/1562 - 30s - loss: 0.3650 - acc: 0.9206 - val_loss: 0.4169 - val_acc: 0.9087\n",
      "Learning rate:  0.0001\nEpoch 114/200\n",
      "1563/1562 - 30s - loss: 0.3654 - acc: 0.9200 - val_loss: 0.4191 - val_acc: 0.9063\n",
      "Learning rate:  0.0001\nEpoch 115/200\n",
      "1563/1562 - 30s - loss: 0.3623 - acc: 0.9197 - val_loss: 0.4053 - val_acc: 0.9091\n",
      "Learning rate:  0.0001\nEpoch 116/200\n",
      "1563/1562 - 30s - loss: 0.3614 - acc: 0.9200 - val_loss: 0.4139 - val_acc: 0.9074\n",
      "Learning rate:  0.0001\nEpoch 117/200\n",
      "1563/1562 - 30s - loss: 0.3609 - acc: 0.9193 - val_loss: 0.4225 - val_acc: 0.9051\n",
      "Learning rate:  0.0001\nEpoch 118/200\n",
      "1563/1562 - 30s - loss: 0.3568 - acc: 0.9200 - val_loss: 0.4219 - val_acc: 0.9052\n",
      "Learning rate:  0.0001\nEpoch 119/200\n",
      "1563/1562 - 30s - loss: 0.3562 - acc: 0.9210 - val_loss: 0.4074 - val_acc: 0.9089\n",
      "Learning rate:  0.0001\nEpoch 120/200\n",
      "1563/1562 - 30s - loss: 0.3517 - acc: 0.9211 - val_loss: 0.4047 - val_acc: 0.9094\n",
      "Learning rate:  0.0001\nEpoch 121/200\n",
      "1563/1562 - 30s - loss: 0.3484 - acc: 0.9226 - val_loss: 0.4092 - val_acc: 0.9092\n",
      "Learning rate:  1e-05\nEpoch 122/200\n",
      "1563/1562 - 30s - loss: 0.3389 - acc: 0.9266 - val_loss: 0.4009 - val_acc: 0.9121\n",
      "Learning rate:  1e-05\nEpoch 123/200\n",
      "1563/1562 - 30s - loss: 0.3352 - acc: 0.9275 - val_loss: 0.3990 - val_acc: 0.9128\n",
      "Learning rate:  1e-05\nEpoch 124/200\n",
      "1563/1562 - 30s - loss: 0.3358 - acc: 0.9281 - val_loss: 0.4003 - val_acc: 0.9132\n",
      "Learning rate:  1e-05\nEpoch 125/200\n",
      "1563/1562 - 30s - loss: 0.3339 - acc: 0.9268 - val_loss: 0.3975 - val_acc: 0.9135\n",
      "Learning rate:  1e-05\nEpoch 126/200\n",
      "1563/1562 - 30s - loss: 0.3291 - acc: 0.9287 - val_loss: 0.3955 - val_acc: 0.9150\n",
      "Learning rate:  1e-05\nEpoch 127/200\n",
      "1563/1562 - 30s - loss: 0.3297 - acc: 0.9288 - val_loss: 0.3970 - val_acc: 0.9134\n",
      "Learning rate:  1e-05\nEpoch 128/200\n",
      "1563/1562 - 30s - loss: 0.3290 - acc: 0.9290 - val_loss: 0.3950 - val_acc: 0.9148\n",
      "Learning rate:  1e-05\nEpoch 129/200\n",
      "1563/1562 - 30s - loss: 0.3295 - acc: 0.9293 - val_loss: 0.4004 - val_acc: 0.9128\n",
      "Learning rate:  1e-05\nEpoch 130/200\n",
      "1563/1562 - 30s - loss: 0.3297 - acc: 0.9294 - val_loss: 0.3932 - val_acc: 0.9138\n",
      "Learning rate:  1e-05\nEpoch 131/200\n",
      "1563/1562 - 30s - loss: 0.3295 - acc: 0.9286 - val_loss: 0.3923 - val_acc: 0.9149\n",
      "Learning rate:  1e-05\nEpoch 132/200\n",
      "1563/1562 - 30s - loss: 0.3283 - acc: 0.9292 - val_loss: 0.3880 - val_acc: 0.9157\n",
      "Learning rate:  1e-05\nEpoch 133/200\n",
      "1563/1562 - 30s - loss: 0.3251 - acc: 0.9311 - val_loss: 0.3916 - val_acc: 0.9149\n",
      "Learning rate:  1e-05\nEpoch 134/200\n",
      "1563/1562 - 31s - loss: 0.3295 - acc: 0.9285 - val_loss: 0.3947 - val_acc: 0.9138\n",
      "Learning rate:  1e-05\nEpoch 135/200\n",
      "1563/1562 - 31s - loss: 0.3244 - acc: 0.9305 - val_loss: 0.3931 - val_acc: 0.9137\n",
      "Learning rate:  1e-05\nEpoch 136/200\n",
      "1563/1562 - 32s - loss: 0.3240 - acc: 0.9298 - val_loss: 0.3926 - val_acc: 0.9135\n",
      "Learning rate:  1e-05\nEpoch 137/200\n",
      "1563/1562 - 32s - loss: 0.3277 - acc: 0.9292 - val_loss: 0.3958 - val_acc: 0.9136\n",
      "Learning rate:  1e-05\nEpoch 138/200\n",
      "1563/1562 - 33s - loss: 0.3277 - acc: 0.9290 - val_loss: 0.3959 - val_acc: 0.9128\n",
      "Learning rate:  1e-05\nEpoch 139/200\n",
      "1563/1562 - 33s - loss: 0.3260 - acc: 0.9307 - val_loss: 0.3948 - val_acc: 0.9138\n",
      "Learning rate:  1e-05\nEpoch 140/200\n",
      "1563/1562 - 32s - loss: 0.3251 - acc: 0.9307 - val_loss: 0.3988 - val_acc: 0.9133\n",
      "Learning rate:  1e-05\nEpoch 141/200\n",
      "1563/1562 - 31s - loss: 0.3254 - acc: 0.9291 - val_loss: 0.3951 - val_acc: 0.9118\n",
      "Learning rate:  1e-06\nEpoch 142/200\n",
      "1563/1562 - 30s - loss: 0.3235 - acc: 0.9304 - val_loss: 0.3961 - val_acc: 0.9119\n",
      "Learning rate:  1e-06\nEpoch 143/200\n",
      "1563/1562 - 30s - loss: 0.3217 - acc: 0.9303 - val_loss: 0.3941 - val_acc: 0.9125\n",
      "Learning rate:  1e-06\nEpoch 144/200\n",
      "1563/1562 - 30s - loss: 0.3239 - acc: 0.9310 - val_loss: 0.3928 - val_acc: 0.9128\n",
      "Learning rate:  1e-06\nEpoch 145/200\n",
      "1563/1562 - 29s - loss: 0.3248 - acc: 0.9300 - val_loss: 0.3933 - val_acc: 0.9126\n",
      "Learning rate:  1e-06\nEpoch 146/200\n",
      "1563/1562 - 30s - loss: 0.3197 - acc: 0.9321 - val_loss: 0.3933 - val_acc: 0.9127\n",
      "Learning rate:  1e-06\nEpoch 147/200\n",
      "1563/1562 - 30s - loss: 0.3242 - acc: 0.9309 - val_loss: 0.3935 - val_acc: 0.9125\n",
      "Learning rate:  1e-06\nEpoch 148/200\n",
      "1563/1562 - 30s - loss: 0.3256 - acc: 0.9298 - val_loss: 0.3933 - val_acc: 0.9134\n",
      "Learning rate:  1e-06\nEpoch 149/200\n",
      "1563/1562 - 30s - loss: 0.3224 - acc: 0.9301 - val_loss: 0.3925 - val_acc: 0.9133\n",
      "Learning rate:  1e-06\nEpoch 150/200\n",
      "1563/1562 - 30s - loss: 0.3225 - acc: 0.9316 - val_loss: 0.3920 - val_acc: 0.9135\n",
      "Learning rate:  1e-06\nEpoch 151/200\n",
      "1563/1562 - 30s - loss: 0.3204 - acc: 0.9314 - val_loss: 0.3954 - val_acc: 0.9127\n",
      "Learning rate:  1e-06\nEpoch 152/200\n",
      "1563/1562 - 29s - loss: 0.3261 - acc: 0.9301 - val_loss: 0.3935 - val_acc: 0.9129\n",
      "Learning rate:  1e-06\nEpoch 153/200\n",
      "1563/1562 - 29s - loss: 0.3217 - acc: 0.9312 - val_loss: 0.3926 - val_acc: 0.9136\n",
      "Learning rate:  1e-06\nEpoch 154/200\n",
      "1563/1562 - 29s - loss: 0.3266 - acc: 0.9288 - val_loss: 0.3918 - val_acc: 0.9137\n",
      "Learning rate:  1e-06\nEpoch 155/200\n",
      "1563/1562 - 29s - loss: 0.3233 - acc: 0.9306 - val_loss: 0.3935 - val_acc: 0.9130\n",
      "Learning rate:  1e-06\nEpoch 156/200\n",
      "1563/1562 - 29s - loss: 0.3222 - acc: 0.9316 - val_loss: 0.3940 - val_acc: 0.9124\n",
      "Learning rate:  1e-06\nEpoch 157/200\n",
      "1563/1562 - 29s - loss: 0.3204 - acc: 0.9321 - val_loss: 0.3936 - val_acc: 0.9132\n",
      "Learning rate:  1e-06\nEpoch 158/200\n",
      "1563/1562 - 29s - loss: 0.3204 - acc: 0.9309 - val_loss: 0.3932 - val_acc: 0.9133\n",
      "Learning rate:  1e-06\nEpoch 159/200\n",
      "1563/1562 - 29s - loss: 0.3222 - acc: 0.9307 - val_loss: 0.3926 - val_acc: 0.9131\n",
      "Learning rate:  1e-06\nEpoch 160/200\n",
      "1563/1562 - 29s - loss: 0.3213 - acc: 0.9306 - val_loss: 0.3936 - val_acc: 0.9132\n",
      "Learning rate:  1e-06\nEpoch 161/200\n",
      "1563/1562 - 29s - loss: 0.3193 - acc: 0.9317 - val_loss: 0.3917 - val_acc: 0.9138\n",
      "Learning rate:  5e-07\nEpoch 162/200\n",
      "1563/1562 - 29s - loss: 0.3235 - acc: 0.9310 - val_loss: 0.3927 - val_acc: 0.9138\n",
      "Learning rate:  5e-07\nEpoch 163/200\n",
      "1563/1562 - 30s - loss: 0.3191 - acc: 0.9318 - val_loss: 0.3921 - val_acc: 0.9133\n",
      "Learning rate:  5e-07\nEpoch 164/200\n",
      "1563/1562 - 29s - loss: 0.3218 - acc: 0.9318 - val_loss: 0.3936 - val_acc: 0.9133\n",
      "Learning rate:  5e-07\nEpoch 165/200\n",
      "1563/1562 - 29s - loss: 0.3222 - acc: 0.9311 - val_loss: 0.3945 - val_acc: 0.9129\n",
      "Learning rate:  5e-07\nEpoch 166/200\n",
      "1563/1562 - 29s - loss: 0.3224 - acc: 0.9304 - val_loss: 0.3932 - val_acc: 0.9131\n",
      "Learning rate:  5e-07\nEpoch 167/200\n",
      "1563/1562 - 30s - loss: 0.3179 - acc: 0.9334 - val_loss: 0.3924 - val_acc: 0.9129\n",
      "Learning rate:  5e-07\nEpoch 168/200\n",
      "1563/1562 - 30s - loss: 0.3233 - acc: 0.9296 - val_loss: 0.3905 - val_acc: 0.9140\n",
      "Learning rate:  5e-07\nEpoch 169/200\n",
      "1563/1562 - 30s - loss: 0.3223 - acc: 0.9311 - val_loss: 0.3920 - val_acc: 0.9137\n",
      "Learning rate:  5e-07\nEpoch 170/200\n",
      "1563/1562 - 30s - loss: 0.3184 - acc: 0.9318 - val_loss: 0.3933 - val_acc: 0.9130\n",
      "Learning rate:  5e-07\nEpoch 171/200\n",
      "1563/1562 - 30s - loss: 0.3204 - acc: 0.9309 - val_loss: 0.3947 - val_acc: 0.9131\n",
      "Learning rate:  5e-07\nEpoch 172/200\n",
      "1563/1562 - 31s - loss: 0.3175 - acc: 0.9323 - val_loss: 0.3933 - val_acc: 0.9132\n",
      "Learning rate:  5e-07\nEpoch 173/200\n",
      "1563/1562 - 30s - loss: 0.3182 - acc: 0.9322 - val_loss: 0.3937 - val_acc: 0.9129\n",
      "Learning rate:  5e-07\nEpoch 174/200\n",
      "1563/1562 - 30s - loss: 0.3194 - acc: 0.9324 - val_loss: 0.3933 - val_acc: 0.9134\n",
      "Learning rate:  5e-07\nEpoch 175/200\n",
      "1563/1562 - 31s - loss: 0.3188 - acc: 0.9312 - val_loss: 0.3962 - val_acc: 0.9133\n",
      "Learning rate:  5e-07\nEpoch 176/200\n",
      "1563/1562 - 31s - loss: 0.3213 - acc: 0.9322 - val_loss: 0.3933 - val_acc: 0.9133\n",
      "Learning rate:  5e-07\nEpoch 177/200\n",
      "1563/1562 - 30s - loss: 0.3242 - acc: 0.9298 - val_loss: 0.3934 - val_acc: 0.9135\n",
      "Learning rate:  5e-07\nEpoch 178/200\n",
      "1563/1562 - 30s - loss: 0.3209 - acc: 0.9319 - val_loss: 0.3929 - val_acc: 0.9131\n",
      "Learning rate:  5e-07\nEpoch 179/200\n",
      "1563/1562 - 30s - loss: 0.3239 - acc: 0.9295 - val_loss: 0.3925 - val_acc: 0.9130\n",
      "Learning rate:  5e-07\nEpoch 180/200\n",
      "1563/1562 - 32s - loss: 0.3180 - acc: 0.9318 - val_loss: 0.3952 - val_acc: 0.9131\n",
      "Learning rate:  5e-07\nEpoch 181/200\n",
      "1563/1562 - 31s - loss: 0.3185 - acc: 0.9338 - val_loss: 0.3927 - val_acc: 0.9137\n",
      "Learning rate:  5e-07\nEpoch 182/200\n",
      "1563/1562 - 31s - loss: 0.3201 - acc: 0.9313 - val_loss: 0.3939 - val_acc: 0.9130\n",
      "Learning rate:  5e-07\nEpoch 183/200\n",
      "1563/1562 - 30s - loss: 0.3209 - acc: 0.9303 - val_loss: 0.3924 - val_acc: 0.9137\n",
      "Learning rate:  5e-07\nEpoch 184/200\n",
      "1563/1562 - 31s - loss: 0.3197 - acc: 0.9320 - val_loss: 0.3933 - val_acc: 0.9137\n",
      "Learning rate:  5e-07\nEpoch 185/200\n",
      "1563/1562 - 30s - loss: 0.3252 - acc: 0.9299 - val_loss: 0.3938 - val_acc: 0.9136\n",
      "Learning rate:  5e-07\nEpoch 186/200\n",
      "1563/1562 - 29s - loss: 0.3222 - acc: 0.9312 - val_loss: 0.3945 - val_acc: 0.9125\n",
      "Learning rate:  5e-07\nEpoch 187/200\n",
      "1563/1562 - 29s - loss: 0.3228 - acc: 0.9297 - val_loss: 0.3917 - val_acc: 0.9146\n",
      "Learning rate:  5e-07\nEpoch 188/200\n",
      "1563/1562 - 29s - loss: 0.3191 - acc: 0.9327 - val_loss: 0.3923 - val_acc: 0.9134\n",
      "Learning rate:  5e-07\nEpoch 189/200\n",
      "1563/1562 - 29s - loss: 0.3238 - acc: 0.9300 - val_loss: 0.3920 - val_acc: 0.9140\n",
      "Learning rate:  5e-07\nEpoch 190/200\n",
      "1563/1562 - 29s - loss: 0.3232 - acc: 0.9306 - val_loss: 0.3935 - val_acc: 0.9129\n",
      "Learning rate:  5e-07\nEpoch 191/200\n",
      "1563/1562 - 29s - loss: 0.3245 - acc: 0.9303 - val_loss: 0.3940 - val_acc: 0.9129\n",
      "Learning rate:  5e-07\nEpoch 192/200\n",
      "1563/1562 - 29s - loss: 0.3211 - acc: 0.9310 - val_loss: 0.3927 - val_acc: 0.9136\n",
      "Learning rate:  5e-07\nEpoch 193/200\n",
      "1563/1562 - 29s - loss: 0.3224 - acc: 0.9308 - val_loss: 0.3957 - val_acc: 0.9132\n",
      "Learning rate:  5e-07\nEpoch 194/200\n",
      "1563/1562 - 29s - loss: 0.3224 - acc: 0.9312 - val_loss: 0.3929 - val_acc: 0.9135\n",
      "Learning rate:  5e-07\nEpoch 195/200\n",
      "1563/1562 - 29s - loss: 0.3198 - acc: 0.9318 - val_loss: 0.3921 - val_acc: 0.9139\n",
      "Learning rate:  5e-07\nEpoch 196/200\n",
      "1563/1562 - 29s - loss: 0.3211 - acc: 0.9312 - val_loss: 0.3916 - val_acc: 0.9136\n",
      "Learning rate:  5e-07\nEpoch 197/200\n",
      "1563/1562 - 29s - loss: 0.3221 - acc: 0.9309 - val_loss: 0.3925 - val_acc: 0.9132\n",
      "Learning rate:  5e-07\nEpoch 198/200\n",
      "1563/1562 - 29s - loss: 0.3208 - acc: 0.9312 - val_loss: 0.3932 - val_acc: 0.9136\n",
      "Learning rate:  5e-07\nEpoch 199/200\n",
      "1563/1562 - 29s - loss: 0.3165 - acc: 0.9339 - val_loss: 0.3924 - val_acc: 0.9126\n",
      "Learning rate:  5e-07\nEpoch 200/200\n",
      "1563/1562 - 29s - loss: 0.3219 - acc: 0.9303 - val_loss: 0.3899 - val_acc: 0.9137\n",
      "Best accuracy (on testing dataset): 91.57%\n              precision    recall  f1-score   support\n\n    airplane     0.9198    0.9180    0.9189      1000\n  automobile     0.9400    0.9710    0.9552      1000\n        bird     0.8855    0.8970    0.8912      1000\n         cat     0.8684    0.7920    0.8285      1000\n        deer     0.9167    0.9350    0.9257      1000\n         dog     0.8898    0.8320    0.8599      1000\n        frog     0.9011    0.9570    0.9282      1000\n       horse     0.9510    0.9510    0.9510      1000\n        ship     0.9500    0.9510    0.9505      1000\n       truck     0.9288    0.9530    0.9408      1000\n\n    accuracy                         0.9157     10000\n   macro avg     0.9151    0.9157    0.9150     10000\nweighted avg     0.9151    0.9157    0.9150     10000\n\n[[918   9  23   6   1   0   2   1  23  17]\n [  2 971   1   1   0   0   0   0   2  23]\n [ 16   0 897  16  14   8  34   6   7   2]\n [ 16   3  28 792  23  70  36  13   8  11]\n [  6   1  13   9 935   8  15   9   2   2]\n [  5   2  29  72  24 832  14  18   1   3]\n [  4   1  15  14   4   3 957   1   1   0]\n [  4   1   4   2  19  14   2 951   2   1]\n [ 21  10   2   0   0   0   2   0 951  14]\n [  6  35   1   0   0   0   0   1   4 953]]\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  }
 ]
}